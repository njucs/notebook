{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQ94+RUmWeSeh7heOt5Pz5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/HuggingfaceTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaJkFGh3V48"
      },
      "source": [
        "### **简介** ###\n",
        "目前各种Pretraining的Transformer模型层出不穷，虽然这些模型都有开源代码，但是它们的实现各不相同，我们在对比不同模型时也会很麻烦。Huggingface Transformer能够帮我们跟踪流行的新模型，并且提供统一的代码风格来使用BERT、XLNet和GPT等等各种不同的模型。而且它有一个模型仓库，所有常见的预训练模型和不同任务上fine-tuning的模型都可以在这里方便的下载。到目前为止，transformers 提供了超过100种语言的，32种预训练语言模型，简单，强大，高性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6Cim5P3mk8"
      },
      "source": [
        "### **安装** ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIek8LNY3KjV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGaNwJi-Tlb"
      },
      "source": [
        "###**主要概念**###\n",
        "1. 只有configuration，models和tokenizer三个主要类。\n",
        "   - 诸如BertModel的模型(Model)类，包括30+PyTorch模型(torch.nn.Module)和对应的TensorFlow模型(tf.keras.Model)。模型列表可以参考https://huggingface.co/models\n",
        "   - 诸如BertConfig的配置(Config)类，它保存了模型的相关(超)参数。我们通常不需要自己来构造它。如果我们不需要进行模型的修改，那么创建模型时会自动使用对于的配置\n",
        "   - 诸如BertTokenizer的Tokenizer类，它保存了词典等信息并且实现了把字符串变成ID序列的功能。\n",
        "   - 所有这三类对象都可以使用from_pretrained()函数自动通过名字或者目录进行构造，也可以使用save_pretrained()函数保存。\n",
        "2. 所有的模型都可以通过统一的from_pretrained()函数来实现加载，transformers会处理下载、缓存和其它所有加载模型相关的细节。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1C7XIttcqQ"
      },
      "source": [
        "###**模型输入**\n",
        "虽然基于transformer的模型各不相同，但是可以把输入抽象成统一的格式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3wVAhm3tfpl"
      },
      "source": [
        "# 输入 ID\n",
        "# 虽然不同的 tokenizer 实现差异很大，但是它们的作用是相同的，即把一个句子变成 Token 的序列，不同的 Token 有不同的整数 ID\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
        "\n",
        "# 把句子变成 Token 序列\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)\n",
        "\n",
        "# 把句子变成 ID 序列\n",
        "inputs = tokenizer(sequence)\n",
        "print(inputs)\n",
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "print(encoded_sequence)\n",
        "\n",
        "# ID 的序列比 Token 要多两个元素，这是 Tokenizer 会自动增加一些特殊的 Token，比如 CLS 和 SEP\n",
        "# 用 decode 来把 ID 解码成 Token\n",
        "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
        "print(decoded_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Squ3f8Pq3P5h"
      },
      "source": [
        "###**关于 attention_mask**\n",
        "如果输入是一个batch，那么会返回Attention Mask，它可以告诉模型哪些部分是padding的，从而要mask掉。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuiQzhQ03Vv1"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"This is a short sequence.\"\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
        "\n",
        "# 可以看到第一个 ID 序列后面补了很多零。但这带来一个问题：模型并不知道哪些是 padding 的。\n",
        "# 我们可以约定 0 就代表 padding，但是用起来会比较麻烦，所以通过一个 attention_mask 明确的标出哪个是 padding 会更加方便。\n",
        "padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
        "print(padded_sequences[\"input_ids\"])\n",
        "print(padded_sequences[\"attention_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_MFlRf41ZD"
      },
      "source": [
        "###**关于 token_type_ids**\n",
        "如果输入的是两个句子，需要明确地告诉模型某个 Token 到底属于哪个句子。就是 token 对应的句子 id，值为 0 或 1（0 表示对应的 token 属于第一句，1 表示属于第二句）。**只能同时输入两个句子作为参数（待确认）。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6zptl2U48bR",
        "outputId": "346328a8-766d-4a53-ac2d-d474054be4b3"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"HuggingFace is based in NYC\"\n",
        "sequence_b = \"Where is HuggingFace based?\"\n",
        "sequence_c = \"I don't know !\"\n",
        "\n",
        "# 会自动帮我们加上 [SEP]\n",
        "encoded_dict = tokenizer(sequence_b, sequence_c)\n",
        "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
        "\n",
        "print(encoded_dict)\n",
        "print(decoded)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2777, 1110, 20164, 10932, 2271, 7954, 1359, 136, 102, 146, 1274, 112, 189, 1221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[CLS] Where is HuggingFace based? [SEP] I don't know! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6psYaun5-eJD"
      },
      "source": [
        "###**自定义模型（调整超参数，不是定义全新模型）**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTQp9_r-bMs"
      },
      "source": [
        "# 需要构造配置类\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# 如果修改了核心的超参数，那么就不能使用 from_pretrained 加载预训练的模型了，必须从头开始训练模型\n",
        "# Tokenizer 一般还是可以复用的\n",
        "\n",
        "# Case 1: 修改核心超参数，构造 Tokenizer 和模型对象\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512) # 修改超参数\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # Tokenizer 还是可以复用\n",
        "model = DistilBertForSequenceClassification(config) # model 不能用 from_pretrained 加载了，需要重新训练\n",
        "\n",
        "# Case 2: 只改变最后一层，比如把一个两分类的模型改成 10 分类的模型\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10) # 通过设置 num_labels 参数来实现对最后一层的修改\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "'''\n",
        "class DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3a_JHDX-6xk"
      },
      "source": [
        "###**使用**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8goWHm--T7"
      },
      "source": [
        "# 使用 pipeline\n",
        "\n",
        "'''\n",
        "使用预训练模型最简单的方法就是使用pipeline函数，它支持如下的任务：\n",
        "- 情感分析(Sentiment analysis)：一段文本是正面还是负面的情感倾向\n",
        "- 文本生成(Text generation)：给定一段文本，让模型补充后面的内容\n",
        "- 命名实体识别(Name entity recognition)：识别文字中出现的人名地名的命名实体\n",
        "- 问答(Question answering)：给定一段文本以及针对它的一个问题，从文本中抽取答案\n",
        "- 填词(Filling masked text)：把一段文字的某些部分mask住，然后让模型填空\n",
        "- 摘要(Summarization)：根据一段长文本中生成简短的摘要\n",
        "- 翻译(Translation)：把一种语言的文字翻译成另一种语言\n",
        "- 特征提取(Feature extraction)：把一段文字用一个向量来表示\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 除了通过名字来制定 model 参数，我们也可以传给 model 一个包含模型的目录的路径，也可以传递一个模型对象。\n",
        "# 如果我们想传递模型对象，那么也需要传入 tokenizer。\n",
        "# 我们需要两个类，一个是 AutoTokenizer，我们将使用它来下载和加载与模型匹配的 Tokenizer。\n",
        "# 另一个是 AutoModelForSequenceClassification。\n",
        "# 这两个 AutoXXX 类会根据加载的模型自动选择 Tokenizer 和 Model，如果我们提前知道了，也可以直接用对应的模型和 Tokenizer 进行 from_pretrained 调用\n",
        "# 注意：模型类是与任务相关的，我们这里是情感分类的分类任务，所以是AutoModelForSequenceClassification。\n",
        "\n",
        "# classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvpz-zHQ5KTb"
      },
      "source": [
        "# 关于 Tokenizer 和 Model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizer 的作用大致就是分词，然后把词变成的整数 ID，最终的目的是把一段文本变成 ID 的序列。\n",
        "inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
        "print(inputs)\n",
        "\n",
        "# 也可以输入一个 batch\n",
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# pt_batch 仍然是一个 dict，input_ids 是一个 batch 的 ID 序列，\n",
        "# 我们可以看到第二个字符串较短，所以它被 padding 成和第一个一样长。\n",
        "# 如果某个句子的长度超过 max_length，也会被切掉多余的部分。\n",
        "for key, value in pt_batch.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "# Tokenizer 的处理结果可以输入给模型，对于 PyTorch 需要使用 ** 来展开参数\n",
        "# Transformers 的所有输出都是 tuple， 默认返回 logits，如果需要概率，可以自己加 softmax\n",
        "pt_outputs = pt_model(**pt_batch)\n",
        "\n",
        "# 如果有输出分类对应的标签，那么也可以传入，这样它除了会计算 logits 还会计算 loss\n",
        "# pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))\n",
        "\n",
        "# 也可以返回所有的隐状态和 attention\n",
        "# pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states, all_attentions = pt_outputs[-2:]\n",
        "\n",
        "pt_predictions = F.softmax(pt_outputs[0], dim=-1)\n",
        "print(pt_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZPqS7w-KN-"
      },
      "source": [
        "# 存储和加载使用\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# 还可以轻松的在 PyTorch 和 TensorFlow 之间切换\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
        "\n",
        "# 如果用 PyTorch 加载 TensorFlow 模型，则需要设置 from_tf = True：\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
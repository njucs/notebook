{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0AB++xmHLQ8ceiSqhxxk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/HuggingfaceTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaJkFGh3V48"
      },
      "source": [
        "### **ç®€ä»‹** ###\n",
        "ç›®å‰å„ç§Pretrainingçš„Transformeræ¨¡å‹å±‚å‡ºä¸ç©·ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹éƒ½æœ‰å¼€æºä»£ç ï¼Œä½†æ˜¯å®ƒä»¬çš„å®ç°å„ä¸ç›¸åŒï¼Œæˆ‘ä»¬åœ¨å¯¹æ¯”ä¸åŒæ¨¡å‹æ—¶ä¹Ÿä¼šå¾ˆéº»çƒ¦ã€‚Huggingface Transformerèƒ½å¤Ÿå¸®æˆ‘ä»¬è·Ÿè¸ªæµè¡Œçš„æ–°æ¨¡å‹ï¼Œå¹¶ä¸”æä¾›ç»Ÿä¸€çš„ä»£ç é£æ ¼æ¥ä½¿ç”¨BERTã€XLNetå’ŒGPTç­‰ç­‰å„ç§ä¸åŒçš„æ¨¡å‹ã€‚è€Œä¸”å®ƒæœ‰ä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œæ‰€æœ‰å¸¸è§çš„é¢„è®­ç»ƒæ¨¡å‹å’Œä¸åŒä»»åŠ¡ä¸Šfine-tuningçš„æ¨¡å‹éƒ½å¯ä»¥åœ¨è¿™é‡Œæ–¹ä¾¿çš„ä¸‹è½½ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œtransformers æä¾›äº†è¶…è¿‡100ç§è¯­è¨€çš„ï¼Œ32ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç®€å•ï¼Œå¼ºå¤§ï¼Œé«˜æ€§èƒ½ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6Cim5P3mk8"
      },
      "source": [
        "### **å®‰è£…** ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIek8LNY3KjV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGaNwJi-Tlb"
      },
      "source": [
        "###**ä¸»è¦æ¦‚å¿µ**###\n",
        "1. åªæœ‰configurationï¼Œmodelså’Œtokenizerä¸‰ä¸ªä¸»è¦ç±»ã€‚\n",
        "   - è¯¸å¦‚BertModelçš„æ¨¡å‹(Model)ç±»ï¼ŒåŒ…æ‹¬30+PyTorchæ¨¡å‹(torch.nn.Module)å’Œå¯¹åº”çš„TensorFlowæ¨¡å‹(tf.keras.Model)ã€‚æ¨¡å‹åˆ—è¡¨å¯ä»¥å‚è€ƒhttps://huggingface.co/models\n",
        "   - è¯¸å¦‚BertConfigçš„é…ç½®(Config)ç±»ï¼Œå®ƒä¿å­˜äº†æ¨¡å‹çš„ç›¸å…³(è¶…)å‚æ•°ã€‚æˆ‘ä»¬é€šå¸¸ä¸éœ€è¦è‡ªå·±æ¥æ„é€ å®ƒã€‚å¦‚æœæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œæ¨¡å‹çš„ä¿®æ”¹ï¼Œé‚£ä¹ˆåˆ›å»ºæ¨¡å‹æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨å¯¹äºçš„é…ç½®\n",
        "   - è¯¸å¦‚BertTokenizerçš„Tokenizerç±»ï¼Œå®ƒä¿å­˜äº†è¯å…¸ç­‰ä¿¡æ¯å¹¶ä¸”å®ç°äº†æŠŠå­—ç¬¦ä¸²å˜æˆIDåºåˆ—çš„åŠŸèƒ½ã€‚\n",
        "   - æ‰€æœ‰è¿™ä¸‰ç±»å¯¹è±¡éƒ½å¯ä»¥ä½¿ç”¨from_pretrained()å‡½æ•°è‡ªåŠ¨é€šè¿‡åå­—æˆ–è€…ç›®å½•è¿›è¡Œæ„é€ ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨save_pretrained()å‡½æ•°ä¿å­˜ã€‚\n",
        "2. æ‰€æœ‰çš„æ¨¡å‹éƒ½å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„from_pretrained()å‡½æ•°æ¥å®ç°åŠ è½½ï¼Œtransformersä¼šå¤„ç†ä¸‹è½½ã€ç¼“å­˜å’Œå…¶å®ƒæ‰€æœ‰åŠ è½½æ¨¡å‹ç›¸å…³çš„ç»†èŠ‚ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1C7XIttcqQ"
      },
      "source": [
        "##**æ¨¡å‹è¾“å…¥**\n",
        "è™½ç„¶åŸºäºtransformerçš„æ¨¡å‹å„ä¸ç›¸åŒï¼Œä½†æ˜¯å¯ä»¥æŠŠè¾“å…¥æŠ½è±¡æˆç»Ÿä¸€çš„æ ¼å¼ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3wVAhm3tfpl"
      },
      "source": [
        "# è¾“å…¥ ID\n",
        "# è™½ç„¶ä¸åŒçš„ tokenizer å®ç°å·®å¼‚å¾ˆå¤§ï¼Œä½†æ˜¯å®ƒä»¬çš„ä½œç”¨æ˜¯ç›¸åŒçš„ï¼Œå³æŠŠä¸€ä¸ªå¥å­å˜æˆ Token çš„åºåˆ—ï¼Œä¸åŒçš„ Token æœ‰ä¸åŒçš„æ•´æ•° ID\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
        "\n",
        "# æŠŠå¥å­å˜æˆ Token åºåˆ—\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)\n",
        "\n",
        "# æŠŠå¥å­å˜æˆ ID åºåˆ—\n",
        "inputs = tokenizer(sequence)\n",
        "print(inputs)\n",
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "print(encoded_sequence)\n",
        "\n",
        "# ID çš„åºåˆ—æ¯” Token è¦å¤šä¸¤ä¸ªå…ƒç´ ï¼Œè¿™æ˜¯ Tokenizer ä¼šè‡ªåŠ¨å¢åŠ ä¸€äº›ç‰¹æ®Šçš„ Tokenï¼Œæ¯”å¦‚ CLS å’Œ SEP\n",
        "# ç”¨ decode æ¥æŠŠ ID è§£ç æˆ Token\n",
        "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
        "print(decoded_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Squ3f8Pq3P5h"
      },
      "source": [
        "###**å…³äº attention_mask**\n",
        "å¦‚æœè¾“å…¥æ˜¯ä¸€ä¸ªbatchï¼Œé‚£ä¹ˆä¼šè¿”å›Attention Maskï¼Œå®ƒå¯ä»¥å‘Šè¯‰æ¨¡å‹å“ªäº›éƒ¨åˆ†æ˜¯paddingçš„ï¼Œä»è€Œè¦maskæ‰ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuiQzhQ03Vv1"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"This is a short sequence.\"\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
        "\n",
        "# å¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ª ID åºåˆ—åé¢è¡¥äº†å¾ˆå¤šé›¶ã€‚ä½†è¿™å¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼šæ¨¡å‹å¹¶ä¸çŸ¥é“å“ªäº›æ˜¯ padding çš„ã€‚\n",
        "# æˆ‘ä»¬å¯ä»¥çº¦å®š 0 å°±ä»£è¡¨ paddingï¼Œä½†æ˜¯ç”¨èµ·æ¥ä¼šæ¯”è¾ƒéº»çƒ¦ï¼Œæ‰€ä»¥é€šè¿‡ä¸€ä¸ª attention_mask æ˜ç¡®çš„æ ‡å‡ºå“ªä¸ªæ˜¯ padding ä¼šæ›´åŠ æ–¹ä¾¿ã€‚\n",
        "padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
        "print(padded_sequences[\"input_ids\"])\n",
        "print(padded_sequences[\"attention_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_MFlRf41ZD"
      },
      "source": [
        "###**å…³äº token_type_ids**\n",
        "å¦‚æœè¾“å…¥çš„æ˜¯ä¸¤ä¸ªå¥å­ï¼Œéœ€è¦æ˜ç¡®åœ°å‘Šè¯‰æ¨¡å‹æŸä¸ª Token åˆ°åº•å±äºå“ªä¸ªå¥å­ã€‚å°±æ˜¯ token å¯¹åº”çš„å¥å­ idï¼Œå€¼ä¸º 0 æˆ– 1ï¼ˆ0 è¡¨ç¤ºå¯¹åº”çš„ token å±äºç¬¬ä¸€å¥ï¼Œ1 è¡¨ç¤ºå±äºç¬¬äºŒå¥ï¼‰ã€‚**åªèƒ½åŒæ—¶è¾“å…¥ä¸¤ä¸ªå¥å­ä½œä¸ºå‚æ•°ï¼ˆå¾…ç¡®è®¤ï¼‰ã€‚**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6zptl2U48bR",
        "outputId": "346328a8-766d-4a53-ac2d-d474054be4b3"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"HuggingFace is based in NYC\"\n",
        "sequence_b = \"Where is HuggingFace based?\"\n",
        "sequence_c = \"I don't know !\"\n",
        "\n",
        "# ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬åŠ ä¸Š [SEP]\n",
        "encoded_dict = tokenizer(sequence_b, sequence_c)\n",
        "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
        "\n",
        "print(encoded_dict)\n",
        "print(decoded)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2777, 1110, 20164, 10932, 2271, 7954, 1359, 136, 102, 146, 1274, 112, 189, 1221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[CLS] Where is HuggingFace based? [SEP] I don't know! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6psYaun5-eJD"
      },
      "source": [
        "##**è‡ªå®šä¹‰æ¨¡å‹ï¼ˆè°ƒè¶…å‚ï¼Œéå…¨æ–°æ¨¡å‹å®šä¹‰ï¼‰**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTQp9_r-bMs"
      },
      "source": [
        "# éœ€è¦æ„é€ é…ç½®ç±»\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# å¦‚æœä¿®æ”¹äº†æ ¸å¿ƒçš„è¶…å‚æ•°ï¼Œé‚£ä¹ˆå°±ä¸èƒ½ä½¿ç”¨ from_pretrained åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹äº†ï¼Œå¿…é¡»ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
        "# Tokenizer ä¸€èˆ¬è¿˜æ˜¯å¯ä»¥å¤ç”¨çš„\n",
        "\n",
        "# Case 1: ä¿®æ”¹æ ¸å¿ƒè¶…å‚æ•°ï¼Œæ„é€  Tokenizer å’Œæ¨¡å‹å¯¹è±¡\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512) # ä¿®æ”¹è¶…å‚æ•°\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # Tokenizer è¿˜æ˜¯å¯ä»¥å¤ç”¨\n",
        "model = DistilBertForSequenceClassification(config) # model ä¸èƒ½ç”¨ from_pretrained åŠ è½½äº†ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ\n",
        "\n",
        "# Case 2: åªæ”¹å˜æœ€åä¸€å±‚ï¼Œæ¯”å¦‚æŠŠä¸€ä¸ªä¸¤åˆ†ç±»çš„æ¨¡å‹æ”¹æˆ 10 åˆ†ç±»çš„æ¨¡å‹\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10) # é€šè¿‡è®¾ç½® num_labels å‚æ•°æ¥å®ç°å¯¹æœ€åä¸€å±‚çš„ä¿®æ”¹\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "'''\n",
        "class DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3a_JHDX-6xk"
      },
      "source": [
        "##**ä½¿ç”¨**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8goWHm--T7"
      },
      "source": [
        "# ä½¿ç”¨ pipeline\n",
        "\n",
        "'''\n",
        "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨pipelineå‡½æ•°ï¼Œå®ƒæ”¯æŒå¦‚ä¸‹çš„ä»»åŠ¡ï¼š\n",
        "- æƒ…æ„Ÿåˆ†æ(Sentiment analysis)ï¼šä¸€æ®µæ–‡æœ¬æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„æƒ…æ„Ÿå€¾å‘\n",
        "- æ–‡æœ¬ç”Ÿæˆ(Text generation)ï¼šç»™å®šä¸€æ®µæ–‡æœ¬ï¼Œè®©æ¨¡å‹è¡¥å……åé¢çš„å†…å®¹\n",
        "- å‘½åå®ä½“è¯†åˆ«(Name entity recognition)ï¼šè¯†åˆ«æ–‡å­—ä¸­å‡ºç°çš„äººååœ°åçš„å‘½åå®ä½“\n",
        "- é—®ç­”(Question answering)ï¼šç»™å®šä¸€æ®µæ–‡æœ¬ä»¥åŠé’ˆå¯¹å®ƒçš„ä¸€ä¸ªé—®é¢˜ï¼Œä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆ\n",
        "- å¡«è¯(Filling masked text)ï¼šæŠŠä¸€æ®µæ–‡å­—çš„æŸäº›éƒ¨åˆ†maskä½ï¼Œç„¶åè®©æ¨¡å‹å¡«ç©º\n",
        "- æ‘˜è¦(Summarization)ï¼šæ ¹æ®ä¸€æ®µé•¿æ–‡æœ¬ä¸­ç”Ÿæˆç®€çŸ­çš„æ‘˜è¦\n",
        "- ç¿»è¯‘(Translation)ï¼šæŠŠä¸€ç§è¯­è¨€çš„æ–‡å­—ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€\n",
        "- ç‰¹å¾æå–(Feature extraction)ï¼šæŠŠä¸€æ®µæ–‡å­—ç”¨ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤º\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# é™¤äº†é€šè¿‡åå­—æ¥åˆ¶å®š model å‚æ•°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ ç»™ model ä¸€ä¸ªåŒ…å«æ¨¡å‹çš„ç›®å½•çš„è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ä¼ é€’ä¸€ä¸ªæ¨¡å‹å¯¹è±¡ã€‚\n",
        "# å¦‚æœæˆ‘ä»¬æƒ³ä¼ é€’æ¨¡å‹å¯¹è±¡ï¼Œé‚£ä¹ˆä¹Ÿéœ€è¦ä¼ å…¥ tokenizerã€‚\n",
        "# æˆ‘ä»¬éœ€è¦ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯ AutoTokenizerï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä¸‹è½½å’ŒåŠ è½½ä¸æ¨¡å‹åŒ¹é…çš„ Tokenizerã€‚\n",
        "# å¦ä¸€ä¸ªæ˜¯ AutoModelForSequenceClassificationã€‚\n",
        "# è¿™ä¸¤ä¸ª AutoXXX ç±»ä¼šæ ¹æ®åŠ è½½çš„æ¨¡å‹è‡ªåŠ¨é€‰æ‹© Tokenizer å’Œ Modelï¼Œå¦‚æœæˆ‘ä»¬æå‰çŸ¥é“äº†ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨å¯¹åº”çš„æ¨¡å‹å’Œ Tokenizer è¿›è¡Œ from_pretrained è°ƒç”¨\n",
        "# æ³¨æ„ï¼šæ¨¡å‹ç±»æ˜¯ä¸ä»»åŠ¡ç›¸å…³çš„ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯æƒ…æ„Ÿåˆ†ç±»çš„åˆ†ç±»ä»»åŠ¡ï¼Œæ‰€ä»¥æ˜¯AutoModelForSequenceClassificationã€‚\n",
        "\n",
        "# classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy to show you the ğŸ¤— Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvpz-zHQ5KTb"
      },
      "source": [
        "# å…³äº Tokenizer å’Œ Model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizer çš„ä½œç”¨å¤§è‡´å°±æ˜¯åˆ†è¯ï¼Œç„¶åæŠŠè¯å˜æˆçš„æ•´æ•° IDï¼Œæœ€ç»ˆçš„ç›®çš„æ˜¯æŠŠä¸€æ®µæ–‡æœ¬å˜æˆ ID çš„åºåˆ—ã€‚\n",
        "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
        "print(inputs)\n",
        "\n",
        "# ä¹Ÿå¯ä»¥è¾“å…¥ä¸€ä¸ª batch\n",
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# pt_batch ä»ç„¶æ˜¯ä¸€ä¸ª dictï¼Œinput_ids æ˜¯ä¸€ä¸ª batch çš„ ID åºåˆ—ï¼Œ\n",
        "# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬äºŒä¸ªå­—ç¬¦ä¸²è¾ƒçŸ­ï¼Œæ‰€ä»¥å®ƒè¢« padding æˆå’Œç¬¬ä¸€ä¸ªä¸€æ ·é•¿ã€‚\n",
        "# å¦‚æœæŸä¸ªå¥å­çš„é•¿åº¦è¶…è¿‡ max_lengthï¼Œä¹Ÿä¼šè¢«åˆ‡æ‰å¤šä½™çš„éƒ¨åˆ†ã€‚\n",
        "for key, value in pt_batch.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "# Tokenizer çš„å¤„ç†ç»“æœå¯ä»¥è¾“å…¥ç»™æ¨¡å‹ï¼Œå¯¹äº PyTorch éœ€è¦ä½¿ç”¨ ** æ¥å±•å¼€å‚æ•°\n",
        "# Transformers çš„æ‰€æœ‰è¾“å‡ºéƒ½æ˜¯ tupleï¼Œ é»˜è®¤è¿”å› logitsï¼Œå¦‚æœéœ€è¦æ¦‚ç‡ï¼Œå¯ä»¥è‡ªå·±åŠ  softmax\n",
        "pt_outputs = pt_model(**pt_batch)\n",
        "\n",
        "# å¦‚æœæœ‰è¾“å‡ºåˆ†ç±»å¯¹åº”çš„æ ‡ç­¾ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥ä¼ å…¥ï¼Œè¿™æ ·å®ƒé™¤äº†ä¼šè®¡ç®— logits è¿˜ä¼šè®¡ç®— loss\n",
        "# pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))\n",
        "\n",
        "# ä¹Ÿå¯ä»¥è¿”å›æ‰€æœ‰çš„éšçŠ¶æ€å’Œ attention\n",
        "# pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states, all_attentions = pt_outputs[-2:]\n",
        "\n",
        "pt_predictions = F.softmax(pt_outputs[0], dim=-1)\n",
        "print(pt_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZPqS7w-KN-"
      },
      "source": [
        "# å­˜å‚¨å’ŒåŠ è½½ä½¿ç”¨\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# è¿˜å¯ä»¥è½»æ¾çš„åœ¨ PyTorch å’Œ TensorFlow ä¹‹é—´åˆ‡æ¢\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
        "\n",
        "# å¦‚æœç”¨ PyTorch åŠ è½½ TensorFlow æ¨¡å‹ï¼Œåˆ™éœ€è¦è®¾ç½® from_tf = Trueï¼š\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsYtLIZ4-Inv"
      },
      "source": [
        "##**å¸¸è§ä»»åŠ¡**\n",
        "- æ­¤å¤„æ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯ä½¿ç”¨è‡ªåŠ¨æ„é€ çš„æ¨¡å‹(Auto Models)ï¼Œå®ƒä¼šä»æŸä¸ªcheckpointæ¢å¤æ¨¡å‹å‚æ•°ï¼Œå¹¶ä¸”è‡ªåŠ¨æ„é€ ç½‘ç»œ\n",
        "- ä¸ºäº†è·å¾—å¥½çš„æ•ˆæœï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°é€‚åˆè¿™ä¸ªä»»åŠ¡çš„checkpointã€‚è¿™äº›checkpointé€šå¸¸æ˜¯åœ¨å¤§é‡æ— æ ‡æ³¨æ•°æ®ä¸Šè¿›pretrainingå¹¶ä¸”åœ¨æŸä¸ªç‰¹å®šä»»åŠ¡ä¸Šfine-tuningåçš„ç»“æœ\n",
        "- å¹¶ä¸æ˜¯æ‰€æœ‰ä»»åŠ¡éƒ½æœ‰fine-tuningçš„æ¨¡å‹\n",
        "- fine-tuningçš„æ•°æ®é›†ä¸è§å¾—å’Œæˆ‘ä»¬çš„å®é™…ä»»åŠ¡å®Œå…¨åŒ¹é…ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è‡ªå·±fine-tuning\n",
        "- ä¸ºäº†è¿›è¡Œé¢„æµ‹ï¼ŒTransformersæä¾›ä¸¤ç§æ–¹æ³•ï¼špipelineå’Œè‡ªå·±æ„é€ æ¨¡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_B42sV-oqI"
      },
      "source": [
        "###**åˆ†ç±»**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRxeOqpy-t7r"
      },
      "source": [
        "###**æŠ½å–å¼é—®ç­”**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cV511NT-3Ey"
      },
      "source": [
        "###**æ–‡æœ¬ç”Ÿæˆ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7M2-coN-5DF"
      },
      "source": [
        "###**å‘½åå®ä½“è¯†åˆ«**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUE_jjS--nW"
      },
      "source": [
        "###**æ‘˜è¦**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2IOoQC_Aod"
      },
      "source": [
        "###**ç¿»è¯‘**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPSnQVR_J_2"
      },
      "source": [
        "###**è¯­è¨€æ¨¡å‹**"
      ]
    }
  ]
}
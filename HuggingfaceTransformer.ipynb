{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgrkN/DKap8EfBkhc5jtUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/HuggingfaceTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaJkFGh3V48"
      },
      "source": [
        "### **简介** ###\n",
        "目前各种Pretraining的Transformer模型层出不穷，虽然这些模型都有开源代码，但是它们的实现各不相同，我们在对比不同模型时也会很麻烦。Huggingface Transformer能够帮我们跟踪流行的新模型，并且提供统一的代码风格来使用BERT、XLNet和GPT等等各种不同的模型。而且它有一个模型仓库，所有常见的预训练模型和不同任务上fine-tuning的模型都可以在这里方便的下载。到目前为止，transformers 提供了超过100种语言的，32种预训练语言模型，简单，强大，高性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6Cim5P3mk8"
      },
      "source": [
        "### **安装** ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIek8LNY3KjV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGaNwJi-Tlb"
      },
      "source": [
        "###**主要概念**###\n",
        "1. 只有configuration，models和tokenizer三个主要类。\n",
        "   - 诸如BertModel的模型(Model)类，包括30+PyTorch模型(torch.nn.Module)和对应的TensorFlow模型(tf.keras.Model)。模型列表可以参考https://huggingface.co/models\n",
        "   - 诸如BertConfig的配置(Config)类，它保存了模型的相关(超)参数。我们通常不需要自己来构造它。如果我们不需要进行模型的修改，那么创建模型时会自动使用对于的配置\n",
        "   - 诸如BertTokenizer的Tokenizer类，它保存了词典等信息并且实现了把字符串变成ID序列的功能。\n",
        "   - 所有这三类对象都可以使用from_pretrained()函数自动通过名字或者目录进行构造，也可以使用save_pretrained()函数保存。\n",
        "2. 所有的模型都可以通过统一的from_pretrained()函数来实现加载，transformers会处理下载、缓存和其它所有加载模型相关的细节。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3a_JHDX-6xk"
      },
      "source": [
        "###**使用**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8goWHm--T7"
      },
      "source": [
        "# 使用 pipeline\n",
        "\n",
        "'''\n",
        "使用预训练模型最简单的方法就是使用pipeline函数，它支持如下的任务：\n",
        "- 情感分析(Sentiment analysis)：一段文本是正面还是负面的情感倾向\n",
        "- 文本生成(Text generation)：给定一段文本，让模型补充后面的内容\n",
        "- 命名实体识别(Name entity recognition)：识别文字中出现的人名地名的命名实体\n",
        "- 问答(Question answering)：给定一段文本以及针对它的一个问题，从文本中抽取答案\n",
        "- 填词(Filling masked text)：把一段文字的某些部分mask住，然后让模型填空\n",
        "- 摘要(Summarization)：根据一段长文本中生成简短的摘要\n",
        "- 翻译(Translation)：把一种语言的文字翻译成另一种语言\n",
        "- 特征提取(Feature extraction)：把一段文字用一个向量来表示\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 除了通过名字来制定 model 参数，我们也可以传给 model 一个包含模型的目录的路径，也可以传递一个模型对象。\n",
        "# 如果我们想传递模型对象，那么也需要传入 tokenizer。\n",
        "# 我们需要两个类，一个是 AutoTokenizer，我们将使用它来下载和加载与模型匹配的 Tokenizer。\n",
        "# 另一个是 AutoModelForSequenceClassification。\n",
        "# 这两个 AutoXXX 类会根据加载的模型自动选择 Tokenizer 和 Model，如果我们提前知道了，也可以直接用对应的模型和 Tokenizer 进行 from_pretrained 调用\n",
        "# 注意：模型类是与任务相关的，我们这里是情感分类的分类任务，所以是AutoModelForSequenceClassification。\n",
        "\n",
        "# classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvpz-zHQ5KTb"
      },
      "source": [
        "# 关于 Tokenizer 和 Model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizer 的作用大致就是分词，然后把词变成的整数 ID，最终的目的是把一段文本变成 ID 的序列。\n",
        "inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
        "print(inputs)\n",
        "\n",
        "# 也可以输入一个 batch\n",
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# pt_batch 仍然是一个 dict，input_ids 是一个 batch 的 ID 序列，\n",
        "# 我们可以看到第二个字符串较短，所以它被 padding 成和第一个一样长。\n",
        "# 如果某个句子的长度超过 max_length，也会被切掉多余的部分。\n",
        "for key, value in pt_batch.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "# Tokenizer 的处理结果可以输入给模型，对于 PyTorch 需要使用 ** 来展开参数\n",
        "# Transformers 的所有输出都是 tuple， 默认返回 logits，如果需要概率，可以自己加 softmax\n",
        "pt_outputs = pt_model(**pt_batch)\n",
        "\n",
        "# 如果有输出分类对应的标签，那么也可以传入，这样它除了会计算 logits 还会计算 loss\n",
        "# pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))\n",
        "\n",
        "# 也可以返回所有的隐状态和 attention\n",
        "# pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states, all_attentions = pt_outputs[-2:]\n",
        "\n",
        "pt_predictions = F.softmax(pt_outputs[0], dim=-1)\n",
        "print(pt_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZPqS7w-KN-"
      },
      "source": [
        "# 存储和加载使用\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# 还可以轻松的在 PyTorch 和 TensorFlow 之间切换\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
        "\n",
        "# 如果用 PyTorch 加载 TensorFlow 模型，则需要设置 from_tf = True：\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6psYaun5-eJD"
      },
      "source": [
        "###**自定义模型（调整超参数，不是定义全新模型）**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTQp9_r-bMs"
      },
      "source": [
        "# 需要构造配置类\n",
        "\n",
        "# 如果修改了核心的超参数，那么就不能使用 from_pretrained 加载预训练的模型了，必须从头开始训练模型\n",
        "# Tokenizer 一般还是可以复用的"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
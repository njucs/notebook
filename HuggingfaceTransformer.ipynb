{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNIg4EbGnfXHIE89H2LMNUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/HuggingfaceTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaJkFGh3V48"
      },
      "source": [
        "### **简介** ###\n",
        "目前各种Pretraining的Transformer模型层出不穷，虽然这些模型都有开源代码，但是它们的实现各不相同，我们在对比不同模型时也会很麻烦。Huggingface Transformer能够帮我们跟踪流行的新模型，并且提供统一的代码风格来使用BERT、XLNet和GPT等等各种不同的模型。而且它有一个模型仓库，所有常见的预训练模型和不同任务上fine-tuning的模型都可以在这里方便的下载。到目前为止，transformers 提供了超过100种语言的，32种预训练语言模型，简单，强大，高性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6Cim5P3mk8"
      },
      "source": [
        "### **安装** ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIek8LNY3KjV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGaNwJi-Tlb"
      },
      "source": [
        "###**主要概念**###\n",
        "1. 只有configuration，models和tokenizer三个主要类。\n",
        "   - 诸如BertModel的模型(Model)类，包括30+PyTorch模型(torch.nn.Module)和对应的TensorFlow模型(tf.keras.Model)。模型列表可以参考https://huggingface.co/models\n",
        "   - 诸如BertConfig的配置(Config)类，它保存了模型的相关(超)参数。我们通常不需要自己来构造它。如果我们不需要进行模型的修改，那么创建模型时会自动使用对于的配置\n",
        "   - 诸如BertTokenizer的Tokenizer类，它保存了词典等信息并且实现了把字符串变成ID序列的功能。\n",
        "   - 所有这三类对象都可以使用from_pretrained()函数自动通过名字或者目录进行构造，也可以使用save_pretrained()函数保存。\n",
        "2. 所有的模型都可以通过统一的from_pretrained()函数来实现加载，transformers会处理下载、缓存和其它所有加载模型相关的细节。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1C7XIttcqQ"
      },
      "source": [
        "##**模型输入**\n",
        "虽然基于transformer的模型各不相同，但是可以把输入抽象成统一的格式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3wVAhm3tfpl"
      },
      "source": [
        "# 输入 ID\n",
        "# 虽然不同的 tokenizer 实现差异很大，但是它们的作用是相同的，即把一个句子变成 Token 的序列，不同的 Token 有不同的整数 ID\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
        "\n",
        "# 把句子变成 Token 序列\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)\n",
        "\n",
        "# 把句子变成 ID 序列\n",
        "inputs = tokenizer(sequence)\n",
        "print(inputs)\n",
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "print(encoded_sequence)\n",
        "\n",
        "# ID 的序列比 Token 要多两个元素，这是 Tokenizer 会自动增加一些特殊的 Token，比如 CLS 和 SEP\n",
        "# 用 decode 来把 ID 解码成 Token\n",
        "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
        "print(decoded_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Squ3f8Pq3P5h"
      },
      "source": [
        "###**关于 attention_mask**\n",
        "如果输入是一个batch，那么会返回Attention Mask，它可以告诉模型哪些部分是padding的，从而要mask掉。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuiQzhQ03Vv1"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"This is a short sequence.\"\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
        "\n",
        "# 可以看到第一个 ID 序列后面补了很多零。但这带来一个问题：模型并不知道哪些是 padding 的。\n",
        "# 我们可以约定 0 就代表 padding，但是用起来会比较麻烦，所以通过一个 attention_mask 明确的标出哪个是 padding 会更加方便。\n",
        "padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
        "print(padded_sequences[\"input_ids\"])\n",
        "print(padded_sequences[\"attention_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_MFlRf41ZD"
      },
      "source": [
        "###**关于 token_type_ids**\n",
        "如果输入的是两个句子，需要明确地告诉模型某个 Token 到底属于哪个句子。就是 token 对应的句子 id，值为 0 或 1（0 表示对应的 token 属于第一句，1 表示属于第二句）。**只能同时输入两个句子作为参数（待确认）。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6zptl2U48bR"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"HuggingFace is based in NYC\"\n",
        "sequence_b = \"Where is HuggingFace based?\"\n",
        "sequence_c = \"I don't know !\"\n",
        "\n",
        "# 会自动帮我们加上 [SEP]\n",
        "encoded_dict = tokenizer(sequence_b, sequence_c)\n",
        "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
        "\n",
        "print(encoded_dict)\n",
        "print(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6psYaun5-eJD"
      },
      "source": [
        "##**自定义模型（调超参，非全新模型定义）**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTQp9_r-bMs"
      },
      "source": [
        "# 需要构造配置类\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# 如果修改了核心的超参数，那么就不能使用 from_pretrained 加载预训练的模型了，必须从头开始训练模型\n",
        "# Tokenizer 一般还是可以复用的\n",
        "\n",
        "# Case 1: 修改核心超参数，构造 Tokenizer 和模型对象\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512) # 修改超参数\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # Tokenizer 还是可以复用\n",
        "model = DistilBertForSequenceClassification(config) # model 不能用 from_pretrained 加载了，需要重新训练\n",
        "\n",
        "# Case 2: 只改变最后一层，比如把一个两分类的模型改成 10 分类的模型\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10) # 通过设置 num_labels 参数来实现对最后一层的修改\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "'''\n",
        "class DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3a_JHDX-6xk"
      },
      "source": [
        "##**使用**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8goWHm--T7"
      },
      "source": [
        "# 使用 pipeline\n",
        "\n",
        "'''\n",
        "使用预训练模型最简单的方法就是使用pipeline函数，它支持如下的任务：\n",
        "- 情感分析(Sentiment analysis)：一段文本是正面还是负面的情感倾向\n",
        "- 文本生成(Text generation)：给定一段文本，让模型补充后面的内容\n",
        "- 命名实体识别(Name entity recognition)：识别文字中出现的人名地名的命名实体\n",
        "- 问答(Question answering)：给定一段文本以及针对它的一个问题，从文本中抽取答案\n",
        "- 填词(Filling masked text)：把一段文字的某些部分mask住，然后让模型填空\n",
        "- 摘要(Summarization)：根据一段长文本中生成简短的摘要\n",
        "- 翻译(Translation)：把一种语言的文字翻译成另一种语言\n",
        "- 特征提取(Feature extraction)：把一段文字用一个向量来表示\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 除了通过名字来制定 model 参数，我们也可以传给 model 一个包含模型的目录的路径，也可以传递一个模型对象。\n",
        "# 如果我们想传递模型对象，那么也需要传入 tokenizer。\n",
        "# 我们需要两个类，一个是 AutoTokenizer，我们将使用它来下载和加载与模型匹配的 Tokenizer。\n",
        "# 另一个是 AutoModelForSequenceClassification。\n",
        "# 这两个 AutoXXX 类会根据加载的模型自动选择 Tokenizer 和 Model，如果我们提前知道了，也可以直接用对应的模型和 Tokenizer 进行 from_pretrained 调用\n",
        "# 注意：模型类是与任务相关的，我们这里是情感分类的分类任务，所以是AutoModelForSequenceClassification。\n",
        "\n",
        "# classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvpz-zHQ5KTb"
      },
      "source": [
        "# 关于 Tokenizer 和 Model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizer 的作用大致就是分词，然后把词变成的整数 ID，最终的目的是把一段文本变成 ID 的序列。\n",
        "inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
        "print(inputs)\n",
        "\n",
        "# 也可以输入一个 batch\n",
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# pt_batch 仍然是一个 dict，input_ids 是一个 batch 的 ID 序列，\n",
        "# 我们可以看到第二个字符串较短，所以它被 padding 成和第一个一样长。\n",
        "# 如果某个句子的长度超过 max_length，也会被切掉多余的部分。\n",
        "for key, value in pt_batch.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "# Tokenizer 的处理结果可以输入给模型，对于 PyTorch 需要使用 ** 来展开参数\n",
        "# Transformers 的所有输出都是 tuple， 默认返回 logits，如果需要概率，可以自己加 softmax\n",
        "pt_outputs = pt_model(**pt_batch)\n",
        "\n",
        "# 如果有输出分类对应的标签，那么也可以传入，这样它除了会计算 logits 还会计算 loss\n",
        "# pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))\n",
        "\n",
        "# 也可以返回所有的隐状态和 attention\n",
        "# pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states, all_attentions = pt_outputs[-2:]\n",
        "\n",
        "pt_predictions = F.softmax(pt_outputs[0], dim=-1)\n",
        "print(pt_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZPqS7w-KN-"
      },
      "source": [
        "# 存储和加载使用\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# 还可以轻松的在 PyTorch 和 TensorFlow 之间切换\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
        "\n",
        "# 如果用 PyTorch 加载 TensorFlow 模型，则需要设置 from_tf = True：\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsYtLIZ4-Inv"
      },
      "source": [
        "##**常见任务**\n",
        "- 此处所有任务都是使用自动构造的模型(Auto Models)，它会从某个checkpoint恢复模型参数，并且自动构造网络\n",
        "- 为了获得好的效果，我们需要找到适合这个任务的checkpoint。这些checkpoint通常是在大量无标注数据上进pretraining并且在某个特定任务上fine-tuning后的结果\n",
        "- 并不是所有任务都有fine-tuning的模型\n",
        "- fine-tuning的数据集不见得和我们的实际任务完全匹配，我们可能需要自己fine-tuning\n",
        "- 为了进行预测，Transformers提供两种方法：pipeline和自己构造模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_B42sV-oqI"
      },
      "source": [
        "###**分类**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNA7FH5E4YIl"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# 使用 pipeline 进行情感分类\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "result = nlp(\"I hate you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "result = nlp(\"I love you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "\n",
        "# 自己构造模型判断两个句子是否相同含义 paraphrase\n",
        "'''\n",
        "1. 从 checkpoint 构造一个 Tokenizer 和模型\n",
        "2. 给定两个输入句子，通过 tokenizer 的__call__方法正确地构造输入，包括 token 类型和 attention mask\n",
        "3. 把输入传给模型进行预测，输出 logits\n",
        "4. 计算 softmax 变成概率\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
        "\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "\n",
        "paraphrase_classification_logits = model(**paraphrase).logits\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
        "\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "# Should be paraphrase\n",
        "for i in range(len(classes)):\n",
        "  print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
        "# Should not be paraphrase\n",
        "for i in range(len(classes)):\n",
        "  print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRxeOqpy-t7r"
      },
      "source": [
        "###**抽取式问答**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoI2S6hP4ZFr"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 使用 pipeline\n",
        "nlp = pipeline(\"question-answering\")\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
        "\"\"\"\n",
        "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "\n",
        "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "\n",
        "# 自己构造模型\n",
        "'''\n",
        "1. 构造 Tokenizer 和模型\n",
        "2. 定义文本和一些问题\n",
        "3. 对每一个问题构造输入，Tokenizer 会帮我们插入合适的特殊符号和 attention mask\n",
        "4. 输入模型进行预测，得到是开始和结束下标的 logits\n",
        "5. 计算 softmax 并且选择概率最大的 start 和 end\n",
        "6. 最终根据 start 和 end 截取答案文本\n",
        "'''\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
        "    \"What does 🤗 Transformers provide?\",\n",
        "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    # 拿到 token 的开始和结束下标后，需要用 tokenizer.convert_ids_to_tokens 先把 id 变成 token\n",
        "    # 然后用 convert_tokens_to_string 把 token 变成字符串\n",
        "    # 前面的 pipeline 把这些工作都直接帮我们做好了\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cV511NT-3Ey"
      },
      "source": [
        "###**文本生成**\n",
        "我们可以用语言模型 fine-tuning 采样的方式一个接一个的生成更多的文本，但是 Transformers 帮我们实现了这些逻辑。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oyxPz3k4Zv0"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "# 提供一段 context 的文本，指定最多生成 50 个 Token\n",
        "# do_sample 为 False 指定选择概率最大的而不是采样，从而每次运行的结果都是固定的\n",
        "# 默认会使用 gpt-2 的模型来生成文本\n",
        "# GPT-2、OpenAi-GPT、CTRL、XLNet、Transfo-XL 和 Reformer 等模型都可以用于生成文本\n",
        "# XLNet 通常需要 padding 一下才会达到比较好的效果，而 GPT-2 则不需要\n",
        "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))\n",
        "print(text_generator(\"How to success?\", max_length=100, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7M2-coN-5DF"
      },
      "source": [
        "###**命名实体识别**\n",
        "把命名实体识别当成一个序列标注任务"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd6pIqXg4aPl"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
        "           \"close to the Manhattan Bridge which is visible from the window.\"\n",
        "print(nlp(sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUE_jjS--nW"
      },
      "source": [
        "###**摘要**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL8LaH1B4a9S"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "# pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\"\n",
        "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n",
        "\n",
        "# 自己构造\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
        "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n",
        "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "#print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(outputs[1])))\n",
        "print(outputs[-1]) # 还需要从 ID 转化为 Token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2IOoQC_Aod"
      },
      "source": [
        "###**翻译**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqQ_CrZCZsuV"
      },
      "source": [
        "# for using Helsinki-NLP/opus-mt-en-zh\n",
        "# have to restart the runtime\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mORdTU-e4bcd"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "from transformers import pipeline, AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "# pipeline\n",
        "translator = pipeline(\"translation_en_to_de\")\n",
        "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))\n",
        "\n",
        "# 自定义模型\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
        "print(outputs) # 还需要从 ID 转化为 Token\n",
        "\n",
        "# 中文翻译\n",
        "model_cn = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
        "tokenizer_cn = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
        "translation = pipeline(\"translation_en_to_zh\", model=model_cn, tokenizer=tokenizer_cn)\n",
        "\n",
        "text = \"I like to study Data Science and Machine Learning\"\n",
        "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
        "print(translated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPSnQVR_J_2"
      },
      "source": [
        "###**语言模型**\n",
        "1. 通常是用来预训练基础的模型，然后也可以使用领域的未标注数据来fine-tuning语言模型\n",
        "2. 比如我们的任务是一个文本分类任务，我们可以基于基础的BERT模型在我们的分类数据上fine-tuning模型。可以用领域的未标注数据对基础的BERT用语言模型这个任务进行再次进行pretraining，然后再用标注的数据fine-tuning分类任务。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXMqNtSP4b-5"
      },
      "source": [
        "# Case 1: fine-tuning MaskedLM\n",
        "# pipeline\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "from pprint import pprint\n",
        "\n",
        "nlp = pipeline(\"fill-mask\")\n",
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))\n",
        "\n",
        "# 自己构造 Tokenizer 和模型\n",
        "'''\n",
        "1. 构造Tokenizer和模型。比如可以使用DistilBERT从checkpoint加载预训练的模型\n",
        "2. 构造输入序列，把需要mask的词替换成tokenizer.mask_token\n",
        "3. 用tokenizer把输入变成ID list\n",
        "4. 获取预测的结果，它的size是词典大小，表示预测某个词的概率\n",
        "5. 获取topk个概率最大的词\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "\n",
        "token_logits = model(input).logits\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "for token in top_5_tokens:\n",
        "  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgsuOmAaRKaX"
      },
      "source": [
        "# Case 2: 预测下一个词\n",
        "# 根据概率采样下一个词，然后不断的重复这个过程来生成更多的文本\n",
        "\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "\n",
        "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
        "\n",
        "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "\n",
        "# filter\n",
        "# top_k_top_p_filtering 的作用是把非 top-k 的 logits 变成负无穷大，这样 softmax 时这些项就是 0\n",
        "# 也可以传入参数 top_p，它的含义是滤掉概率小于它的项目\n",
        "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "\n",
        "# sample 采样\n",
        "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vf_-mFeaxpR"
      },
      "source": [
        "##**Huggingface Transformer 使用总结**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOqOC9iEa48L"
      },
      "source": [
        "###**预处理数据**\n",
        "1. Transformers 处理数据的主要工具是 tokenizer\n",
        "2. 我们可以使用与某个模型匹配的特定 tokenizer，也可以通过 AutoTokenizer 类自动帮我们选择合适的 tokenizer\n",
        "3. Tokenizer 的作用是把输入文本切分成 Token，然后把 Token 变成整数 ID，除此之外它也会增加一些额外的特殊 Token 以处理特定的任务\n",
        "4. 如果我们要使用预训练的模型，那么一定要使用它的 Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bk_MEGZcDxG"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 使用 AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# 用法1: __call__\n",
        "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
        "# 返回一个 dict，包含 input_ids、token_type_ids 和 attention_mask\n",
        "print(encoded_input)\n",
        "# 可以用 decode 方法把 ID 恢复成字符串\n",
        "# 会增加一些特殊的 Token，比如 [CLS] 和 [SEP]\n",
        "# 并不是所有的模型都需要增加特殊 Token，我们可以使用参数 add_special_tokens=False 来禁用这个特性\n",
        "print(tokenizer.decode(encoded_input[\"input_ids\"]))\n",
        "\n",
        "# 用法2: 处理一个 batch\n",
        "batch_sentences = [\"Hello I'm a single sentence\",\n",
        "                   \"And another sentence\",\n",
        "                   \"And the very very last one\"]\n",
        "encoded_inputs = tokenizer(batch_sentences)\n",
        "print(encoded_inputs)\n",
        "# 对于大部分应用，batch 的处理通常会需要补齐或者截断，使得 batch 内每个句子都一样长，并返回 tensor\n",
        "# 如果没有指定最大长度限制，则 truncation 不起作用\n",
        "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(batch)\n",
        "\n",
        "# 用法3: 处理两个输入\n",
        "# 可以给 __call__ 方法传入两个参数(不是一个 list 的参数)\n",
        "encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")\n",
        "print(encoded_input)\n",
        "print(tokenizer.decode(encoded_input[\"input_ids\"]))\n",
        "\n",
        "# 用法4: 传入两个 list，从而进行 batch 处理\n",
        "batch_sentences = [\"Hello I'm a single sentence\",\n",
        "                   \"And another sentence\",\n",
        "                   \"And the very very last one\"]\n",
        "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
        "                             \"And I should be encoded with the second sentence\",\n",
        "                             \"And I go with the very last one\"]\n",
        "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)\n",
        "print(encoded_inputs)\n",
        "for ids in encoded_inputs[\"input_ids\"]:\n",
        "  print(tokenizer.decode(ids))\n",
        "# 也可以 padding 和 truncate 以及返回 tensor\n",
        "# 此处是确保 batch 内两个句子拼接后长度一致，不是对两个拼接的句子进行 padding 或者 truncation\n",
        "batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(batch)\n",
        "for ids in batch[\"input_ids\"]:\n",
        "  print(tokenizer.decode(ids))\n",
        "\n",
        "# 用法5: Pre-tokenized\n",
        "# Pre-tokenized 指的是提前进行了分词，但是并没有进行 subword 的处理\n",
        "# 如果输入是 Pre-tokenized，则可以给定参数 is_split_into_words=True\n",
        "encoded_input = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"], is_split_into_words=True)\n",
        "print(encoded_input)\n",
        "# 如果处理一个 batch，那么可以传字符串 list\n",
        "batch_sentences = [[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],\n",
        "                   [\"And\", \"another\", \"sentence\"],\n",
        "                   [\"And\", \"the\", \"very\", \"very\", \"last\", \"one\"]]\n",
        "encoded_inputs = tokenizer(batch_sentences, is_split_into_words=True)\n",
        "print(encoded_inputs)\n",
        "# 如果每个输入都是两个句子，可以传入两个这样的字符串 list\n",
        "batch_of_second_sentences = [[\"I'm\", \"a\", \"sentence\", \"that\", \"goes\", \"with\", \"the\", \"first\", \"sentence\"],\n",
        "                             [\"And\", \"I\", \"should\", \"be\", \"encoded\", \"with\", \"the\", \"second\", \"sentence\"],\n",
        "                             [\"And\", \"I\", \"go\", \"with\", \"the\", \"very\", \"last\", \"one\"]]\n",
        "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=True)\n",
        "print(encoded_inputs)\n",
        "# 也可以 padding/truncating 并且返回 tensor：\n",
        "batch = tokenizer(batch_sentences,\n",
        "                  batch_of_second_sentences,\n",
        "                  is_split_into_words=True,\n",
        "                  padding=True,\n",
        "                  truncation=True,\n",
        "                  return_tensors=\"pt\")\n",
        "print(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEwSEm1bbH-8"
      },
      "source": [
        "###**训练和 fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l-TTLxAwQvU"
      },
      "source": [
        "# 使用 Trainer\n",
        "\n",
        "'''\n",
        "TrainingArguments参数指定了训练的设置：\n",
        "1. 输出目录\n",
        "2. 总的 epochs\n",
        "3. 训练的 batch_size\n",
        "4. 预测的 batch_size\n",
        "5. warmup 的 step 数\n",
        "6. weight_decay 和 log 目录。\n",
        "\n",
        "然后使用 trainer.train() 和 trainer.evaluate() 函数就可以进行训练和验证。\n",
        "我们也可以自己实现模型，但是要求它的 forward 返回的第一个参数是 loss。\n",
        "'''\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset            # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# 如果我们想计算除了 loss 之外的指标，需要给 Trainer 传入 compute_metrics 函数\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgrkN/DKap8EfBkhc5jtUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/HuggingfaceTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaJkFGh3V48"
      },
      "source": [
        "### **ç®€ä»‹** ###\n",
        "ç›®å‰å„ç§Pretrainingçš„Transformeræ¨¡å‹å±‚å‡ºä¸ç©·ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹éƒ½æœ‰å¼€æºä»£ç ï¼Œä½†æ˜¯å®ƒä»¬çš„å®ç°å„ä¸ç›¸åŒï¼Œæˆ‘ä»¬åœ¨å¯¹æ¯”ä¸åŒæ¨¡å‹æ—¶ä¹Ÿä¼šå¾ˆéº»çƒ¦ã€‚Huggingface Transformerèƒ½å¤Ÿå¸®æˆ‘ä»¬è·Ÿè¸ªæµè¡Œçš„æ–°æ¨¡å‹ï¼Œå¹¶ä¸”æä¾›ç»Ÿä¸€çš„ä»£ç é£æ ¼æ¥ä½¿ç”¨BERTã€XLNetå’ŒGPTç­‰ç­‰å„ç§ä¸åŒçš„æ¨¡å‹ã€‚è€Œä¸”å®ƒæœ‰ä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œæ‰€æœ‰å¸¸è§çš„é¢„è®­ç»ƒæ¨¡å‹å’Œä¸åŒä»»åŠ¡ä¸Šfine-tuningçš„æ¨¡å‹éƒ½å¯ä»¥åœ¨è¿™é‡Œæ–¹ä¾¿çš„ä¸‹è½½ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œtransformers æä¾›äº†è¶…è¿‡100ç§è¯­è¨€çš„ï¼Œ32ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç®€å•ï¼Œå¼ºå¤§ï¼Œé«˜æ€§èƒ½ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6Cim5P3mk8"
      },
      "source": [
        "### **å®‰è£…** ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIek8LNY3KjV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGaNwJi-Tlb"
      },
      "source": [
        "###**ä¸»è¦æ¦‚å¿µ**###\n",
        "1. åªæœ‰configurationï¼Œmodelså’Œtokenizerä¸‰ä¸ªä¸»è¦ç±»ã€‚\n",
        "   - è¯¸å¦‚BertModelçš„æ¨¡å‹(Model)ç±»ï¼ŒåŒ…æ‹¬30+PyTorchæ¨¡å‹(torch.nn.Module)å’Œå¯¹åº”çš„TensorFlowæ¨¡å‹(tf.keras.Model)ã€‚æ¨¡å‹åˆ—è¡¨å¯ä»¥å‚è€ƒhttps://huggingface.co/models\n",
        "   - è¯¸å¦‚BertConfigçš„é…ç½®(Config)ç±»ï¼Œå®ƒä¿å­˜äº†æ¨¡å‹çš„ç›¸å…³(è¶…)å‚æ•°ã€‚æˆ‘ä»¬é€šå¸¸ä¸éœ€è¦è‡ªå·±æ¥æ„é€ å®ƒã€‚å¦‚æœæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œæ¨¡å‹çš„ä¿®æ”¹ï¼Œé‚£ä¹ˆåˆ›å»ºæ¨¡å‹æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨å¯¹äºçš„é…ç½®\n",
        "   - è¯¸å¦‚BertTokenizerçš„Tokenizerç±»ï¼Œå®ƒä¿å­˜äº†è¯å…¸ç­‰ä¿¡æ¯å¹¶ä¸”å®ç°äº†æŠŠå­—ç¬¦ä¸²å˜æˆIDåºåˆ—çš„åŠŸèƒ½ã€‚\n",
        "   - æ‰€æœ‰è¿™ä¸‰ç±»å¯¹è±¡éƒ½å¯ä»¥ä½¿ç”¨from_pretrained()å‡½æ•°è‡ªåŠ¨é€šè¿‡åå­—æˆ–è€…ç›®å½•è¿›è¡Œæ„é€ ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨save_pretrained()å‡½æ•°ä¿å­˜ã€‚\n",
        "2. æ‰€æœ‰çš„æ¨¡å‹éƒ½å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„from_pretrained()å‡½æ•°æ¥å®ç°åŠ è½½ï¼Œtransformersä¼šå¤„ç†ä¸‹è½½ã€ç¼“å­˜å’Œå…¶å®ƒæ‰€æœ‰åŠ è½½æ¨¡å‹ç›¸å…³çš„ç»†èŠ‚ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3a_JHDX-6xk"
      },
      "source": [
        "###**ä½¿ç”¨**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8goWHm--T7"
      },
      "source": [
        "# ä½¿ç”¨ pipeline\n",
        "\n",
        "'''\n",
        "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨pipelineå‡½æ•°ï¼Œå®ƒæ”¯æŒå¦‚ä¸‹çš„ä»»åŠ¡ï¼š\n",
        "- æƒ…æ„Ÿåˆ†æ(Sentiment analysis)ï¼šä¸€æ®µæ–‡æœ¬æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„æƒ…æ„Ÿå€¾å‘\n",
        "- æ–‡æœ¬ç”Ÿæˆ(Text generation)ï¼šç»™å®šä¸€æ®µæ–‡æœ¬ï¼Œè®©æ¨¡å‹è¡¥å……åé¢çš„å†…å®¹\n",
        "- å‘½åå®ä½“è¯†åˆ«(Name entity recognition)ï¼šè¯†åˆ«æ–‡å­—ä¸­å‡ºç°çš„äººååœ°åçš„å‘½åå®ä½“\n",
        "- é—®ç­”(Question answering)ï¼šç»™å®šä¸€æ®µæ–‡æœ¬ä»¥åŠé’ˆå¯¹å®ƒçš„ä¸€ä¸ªé—®é¢˜ï¼Œä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆ\n",
        "- å¡«è¯(Filling masked text)ï¼šæŠŠä¸€æ®µæ–‡å­—çš„æŸäº›éƒ¨åˆ†maskä½ï¼Œç„¶åè®©æ¨¡å‹å¡«ç©º\n",
        "- æ‘˜è¦(Summarization)ï¼šæ ¹æ®ä¸€æ®µé•¿æ–‡æœ¬ä¸­ç”Ÿæˆç®€çŸ­çš„æ‘˜è¦\n",
        "- ç¿»è¯‘(Translation)ï¼šæŠŠä¸€ç§è¯­è¨€çš„æ–‡å­—ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€\n",
        "- ç‰¹å¾æå–(Feature extraction)ï¼šæŠŠä¸€æ®µæ–‡å­—ç”¨ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤º\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# é™¤äº†é€šè¿‡åå­—æ¥åˆ¶å®š model å‚æ•°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ ç»™ model ä¸€ä¸ªåŒ…å«æ¨¡å‹çš„ç›®å½•çš„è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ä¼ é€’ä¸€ä¸ªæ¨¡å‹å¯¹è±¡ã€‚\n",
        "# å¦‚æœæˆ‘ä»¬æƒ³ä¼ é€’æ¨¡å‹å¯¹è±¡ï¼Œé‚£ä¹ˆä¹Ÿéœ€è¦ä¼ å…¥ tokenizerã€‚\n",
        "# æˆ‘ä»¬éœ€è¦ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯ AutoTokenizerï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä¸‹è½½å’ŒåŠ è½½ä¸æ¨¡å‹åŒ¹é…çš„ Tokenizerã€‚\n",
        "# å¦ä¸€ä¸ªæ˜¯ AutoModelForSequenceClassificationã€‚\n",
        "# è¿™ä¸¤ä¸ª AutoXXX ç±»ä¼šæ ¹æ®åŠ è½½çš„æ¨¡å‹è‡ªåŠ¨é€‰æ‹© Tokenizer å’Œ Modelï¼Œå¦‚æœæˆ‘ä»¬æå‰çŸ¥é“äº†ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨å¯¹åº”çš„æ¨¡å‹å’Œ Tokenizer è¿›è¡Œ from_pretrained è°ƒç”¨\n",
        "# æ³¨æ„ï¼šæ¨¡å‹ç±»æ˜¯ä¸ä»»åŠ¡ç›¸å…³çš„ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯æƒ…æ„Ÿåˆ†ç±»çš„åˆ†ç±»ä»»åŠ¡ï¼Œæ‰€ä»¥æ˜¯AutoModelForSequenceClassificationã€‚\n",
        "\n",
        "# classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy to show you the ğŸ¤— Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvpz-zHQ5KTb"
      },
      "source": [
        "# å…³äº Tokenizer å’Œ Model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizer çš„ä½œç”¨å¤§è‡´å°±æ˜¯åˆ†è¯ï¼Œç„¶åæŠŠè¯å˜æˆçš„æ•´æ•° IDï¼Œæœ€ç»ˆçš„ç›®çš„æ˜¯æŠŠä¸€æ®µæ–‡æœ¬å˜æˆ ID çš„åºåˆ—ã€‚\n",
        "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
        "print(inputs)\n",
        "\n",
        "# ä¹Ÿå¯ä»¥è¾“å…¥ä¸€ä¸ª batch\n",
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# pt_batch ä»ç„¶æ˜¯ä¸€ä¸ª dictï¼Œinput_ids æ˜¯ä¸€ä¸ª batch çš„ ID åºåˆ—ï¼Œ\n",
        "# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬äºŒä¸ªå­—ç¬¦ä¸²è¾ƒçŸ­ï¼Œæ‰€ä»¥å®ƒè¢« padding æˆå’Œç¬¬ä¸€ä¸ªä¸€æ ·é•¿ã€‚\n",
        "# å¦‚æœæŸä¸ªå¥å­çš„é•¿åº¦è¶…è¿‡ max_lengthï¼Œä¹Ÿä¼šè¢«åˆ‡æ‰å¤šä½™çš„éƒ¨åˆ†ã€‚\n",
        "for key, value in pt_batch.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "# Tokenizer çš„å¤„ç†ç»“æœå¯ä»¥è¾“å…¥ç»™æ¨¡å‹ï¼Œå¯¹äº PyTorch éœ€è¦ä½¿ç”¨ ** æ¥å±•å¼€å‚æ•°\n",
        "# Transformers çš„æ‰€æœ‰è¾“å‡ºéƒ½æ˜¯ tupleï¼Œ é»˜è®¤è¿”å› logitsï¼Œå¦‚æœéœ€è¦æ¦‚ç‡ï¼Œå¯ä»¥è‡ªå·±åŠ  softmax\n",
        "pt_outputs = pt_model(**pt_batch)\n",
        "\n",
        "# å¦‚æœæœ‰è¾“å‡ºåˆ†ç±»å¯¹åº”çš„æ ‡ç­¾ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥ä¼ å…¥ï¼Œè¿™æ ·å®ƒé™¤äº†ä¼šè®¡ç®— logits è¿˜ä¼šè®¡ç®— loss\n",
        "# pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))\n",
        "\n",
        "# ä¹Ÿå¯ä»¥è¿”å›æ‰€æœ‰çš„éšçŠ¶æ€å’Œ attention\n",
        "# pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states, all_attentions = pt_outputs[-2:]\n",
        "\n",
        "pt_predictions = F.softmax(pt_outputs[0], dim=-1)\n",
        "print(pt_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZPqS7w-KN-"
      },
      "source": [
        "# å­˜å‚¨å’ŒåŠ è½½ä½¿ç”¨\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# è¿˜å¯ä»¥è½»æ¾çš„åœ¨ PyTorch å’Œ TensorFlow ä¹‹é—´åˆ‡æ¢\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
        "\n",
        "# å¦‚æœç”¨ PyTorch åŠ è½½ TensorFlow æ¨¡å‹ï¼Œåˆ™éœ€è¦è®¾ç½® from_tf = Trueï¼š\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6psYaun5-eJD"
      },
      "source": [
        "###**è‡ªå®šä¹‰æ¨¡å‹ï¼ˆè°ƒæ•´è¶…å‚æ•°ï¼Œä¸æ˜¯å®šä¹‰å…¨æ–°æ¨¡å‹ï¼‰**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTQp9_r-bMs"
      },
      "source": [
        "# éœ€è¦æ„é€ é…ç½®ç±»\n",
        "\n",
        "# å¦‚æœä¿®æ”¹äº†æ ¸å¿ƒçš„è¶…å‚æ•°ï¼Œé‚£ä¹ˆå°±ä¸èƒ½ä½¿ç”¨ from_pretrained åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹äº†ï¼Œå¿…é¡»ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
        "# Tokenizer ä¸€èˆ¬è¿˜æ˜¯å¯ä»¥å¤ç”¨çš„"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
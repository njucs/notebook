{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNIg4EbGnfXHIE89H2LMNUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/HuggingfaceTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaJkFGh3V48"
      },
      "source": [
        "### **ç®€ä»‹** ###\n",
        "ç›®å‰å„ç§Pretrainingçš„Transformeræ¨¡å‹å±‚å‡ºä¸ç©·ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹éƒ½æœ‰å¼€æºä»£ç ï¼Œä½†æ˜¯å®ƒä»¬çš„å®ç°å„ä¸ç›¸åŒï¼Œæˆ‘ä»¬åœ¨å¯¹æ¯”ä¸åŒæ¨¡å‹æ—¶ä¹Ÿä¼šå¾ˆéº»çƒ¦ã€‚Huggingface Transformerèƒ½å¤Ÿå¸®æˆ‘ä»¬è·Ÿè¸ªæµè¡Œçš„æ–°æ¨¡å‹ï¼Œå¹¶ä¸”æä¾›ç»Ÿä¸€çš„ä»£ç é£æ ¼æ¥ä½¿ç”¨BERTã€XLNetå’ŒGPTç­‰ç­‰å„ç§ä¸åŒçš„æ¨¡å‹ã€‚è€Œä¸”å®ƒæœ‰ä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œæ‰€æœ‰å¸¸è§çš„é¢„è®­ç»ƒæ¨¡å‹å’Œä¸åŒä»»åŠ¡ä¸Šfine-tuningçš„æ¨¡å‹éƒ½å¯ä»¥åœ¨è¿™é‡Œæ–¹ä¾¿çš„ä¸‹è½½ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œtransformers æä¾›äº†è¶…è¿‡100ç§è¯­è¨€çš„ï¼Œ32ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç®€å•ï¼Œå¼ºå¤§ï¼Œé«˜æ€§èƒ½ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6Cim5P3mk8"
      },
      "source": [
        "### **å®‰è£…** ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIek8LNY3KjV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGaNwJi-Tlb"
      },
      "source": [
        "###**ä¸»è¦æ¦‚å¿µ**###\n",
        "1. åªæœ‰configurationï¼Œmodelså’Œtokenizerä¸‰ä¸ªä¸»è¦ç±»ã€‚\n",
        "   - è¯¸å¦‚BertModelçš„æ¨¡å‹(Model)ç±»ï¼ŒåŒ…æ‹¬30+PyTorchæ¨¡å‹(torch.nn.Module)å’Œå¯¹åº”çš„TensorFlowæ¨¡å‹(tf.keras.Model)ã€‚æ¨¡å‹åˆ—è¡¨å¯ä»¥å‚è€ƒhttps://huggingface.co/models\n",
        "   - è¯¸å¦‚BertConfigçš„é…ç½®(Config)ç±»ï¼Œå®ƒä¿å­˜äº†æ¨¡å‹çš„ç›¸å…³(è¶…)å‚æ•°ã€‚æˆ‘ä»¬é€šå¸¸ä¸éœ€è¦è‡ªå·±æ¥æ„é€ å®ƒã€‚å¦‚æœæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œæ¨¡å‹çš„ä¿®æ”¹ï¼Œé‚£ä¹ˆåˆ›å»ºæ¨¡å‹æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨å¯¹äºçš„é…ç½®\n",
        "   - è¯¸å¦‚BertTokenizerçš„Tokenizerç±»ï¼Œå®ƒä¿å­˜äº†è¯å…¸ç­‰ä¿¡æ¯å¹¶ä¸”å®ç°äº†æŠŠå­—ç¬¦ä¸²å˜æˆIDåºåˆ—çš„åŠŸèƒ½ã€‚\n",
        "   - æ‰€æœ‰è¿™ä¸‰ç±»å¯¹è±¡éƒ½å¯ä»¥ä½¿ç”¨from_pretrained()å‡½æ•°è‡ªåŠ¨é€šè¿‡åå­—æˆ–è€…ç›®å½•è¿›è¡Œæ„é€ ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨save_pretrained()å‡½æ•°ä¿å­˜ã€‚\n",
        "2. æ‰€æœ‰çš„æ¨¡å‹éƒ½å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„from_pretrained()å‡½æ•°æ¥å®ç°åŠ è½½ï¼Œtransformersä¼šå¤„ç†ä¸‹è½½ã€ç¼“å­˜å’Œå…¶å®ƒæ‰€æœ‰åŠ è½½æ¨¡å‹ç›¸å…³çš„ç»†èŠ‚ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1C7XIttcqQ"
      },
      "source": [
        "##**æ¨¡å‹è¾“å…¥**\n",
        "è™½ç„¶åŸºäºtransformerçš„æ¨¡å‹å„ä¸ç›¸åŒï¼Œä½†æ˜¯å¯ä»¥æŠŠè¾“å…¥æŠ½è±¡æˆç»Ÿä¸€çš„æ ¼å¼ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3wVAhm3tfpl"
      },
      "source": [
        "# è¾“å…¥ ID\n",
        "# è™½ç„¶ä¸åŒçš„ tokenizer å®ç°å·®å¼‚å¾ˆå¤§ï¼Œä½†æ˜¯å®ƒä»¬çš„ä½œç”¨æ˜¯ç›¸åŒçš„ï¼Œå³æŠŠä¸€ä¸ªå¥å­å˜æˆ Token çš„åºåˆ—ï¼Œä¸åŒçš„ Token æœ‰ä¸åŒçš„æ•´æ•° ID\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
        "\n",
        "# æŠŠå¥å­å˜æˆ Token åºåˆ—\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)\n",
        "\n",
        "# æŠŠå¥å­å˜æˆ ID åºåˆ—\n",
        "inputs = tokenizer(sequence)\n",
        "print(inputs)\n",
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "print(encoded_sequence)\n",
        "\n",
        "# ID çš„åºåˆ—æ¯” Token è¦å¤šä¸¤ä¸ªå…ƒç´ ï¼Œè¿™æ˜¯ Tokenizer ä¼šè‡ªåŠ¨å¢åŠ ä¸€äº›ç‰¹æ®Šçš„ Tokenï¼Œæ¯”å¦‚ CLS å’Œ SEP\n",
        "# ç”¨ decode æ¥æŠŠ ID è§£ç æˆ Token\n",
        "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
        "print(decoded_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Squ3f8Pq3P5h"
      },
      "source": [
        "###**å…³äº attention_mask**\n",
        "å¦‚æœè¾“å…¥æ˜¯ä¸€ä¸ªbatchï¼Œé‚£ä¹ˆä¼šè¿”å›Attention Maskï¼Œå®ƒå¯ä»¥å‘Šè¯‰æ¨¡å‹å“ªäº›éƒ¨åˆ†æ˜¯paddingçš„ï¼Œä»è€Œè¦maskæ‰ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuiQzhQ03Vv1"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"This is a short sequence.\"\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
        "\n",
        "# å¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ª ID åºåˆ—åé¢è¡¥äº†å¾ˆå¤šé›¶ã€‚ä½†è¿™å¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼šæ¨¡å‹å¹¶ä¸çŸ¥é“å“ªäº›æ˜¯ padding çš„ã€‚\n",
        "# æˆ‘ä»¬å¯ä»¥çº¦å®š 0 å°±ä»£è¡¨ paddingï¼Œä½†æ˜¯ç”¨èµ·æ¥ä¼šæ¯”è¾ƒéº»çƒ¦ï¼Œæ‰€ä»¥é€šè¿‡ä¸€ä¸ª attention_mask æ˜ç¡®çš„æ ‡å‡ºå“ªä¸ªæ˜¯ padding ä¼šæ›´åŠ æ–¹ä¾¿ã€‚\n",
        "padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
        "print(padded_sequences[\"input_ids\"])\n",
        "print(padded_sequences[\"attention_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_MFlRf41ZD"
      },
      "source": [
        "###**å…³äº token_type_ids**\n",
        "å¦‚æœè¾“å…¥çš„æ˜¯ä¸¤ä¸ªå¥å­ï¼Œéœ€è¦æ˜ç¡®åœ°å‘Šè¯‰æ¨¡å‹æŸä¸ª Token åˆ°åº•å±äºå“ªä¸ªå¥å­ã€‚å°±æ˜¯ token å¯¹åº”çš„å¥å­ idï¼Œå€¼ä¸º 0 æˆ– 1ï¼ˆ0 è¡¨ç¤ºå¯¹åº”çš„ token å±äºç¬¬ä¸€å¥ï¼Œ1 è¡¨ç¤ºå±äºç¬¬äºŒå¥ï¼‰ã€‚**åªèƒ½åŒæ—¶è¾“å…¥ä¸¤ä¸ªå¥å­ä½œä¸ºå‚æ•°ï¼ˆå¾…ç¡®è®¤ï¼‰ã€‚**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6zptl2U48bR"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"HuggingFace is based in NYC\"\n",
        "sequence_b = \"Where is HuggingFace based?\"\n",
        "sequence_c = \"I don't know !\"\n",
        "\n",
        "# ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬åŠ ä¸Š [SEP]\n",
        "encoded_dict = tokenizer(sequence_b, sequence_c)\n",
        "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
        "\n",
        "print(encoded_dict)\n",
        "print(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6psYaun5-eJD"
      },
      "source": [
        "##**è‡ªå®šä¹‰æ¨¡å‹ï¼ˆè°ƒè¶…å‚ï¼Œéå…¨æ–°æ¨¡å‹å®šä¹‰ï¼‰**###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTQp9_r-bMs"
      },
      "source": [
        "# éœ€è¦æ„é€ é…ç½®ç±»\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# å¦‚æœä¿®æ”¹äº†æ ¸å¿ƒçš„è¶…å‚æ•°ï¼Œé‚£ä¹ˆå°±ä¸èƒ½ä½¿ç”¨ from_pretrained åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹äº†ï¼Œå¿…é¡»ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
        "# Tokenizer ä¸€èˆ¬è¿˜æ˜¯å¯ä»¥å¤ç”¨çš„\n",
        "\n",
        "# Case 1: ä¿®æ”¹æ ¸å¿ƒè¶…å‚æ•°ï¼Œæ„é€  Tokenizer å’Œæ¨¡å‹å¯¹è±¡\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512) # ä¿®æ”¹è¶…å‚æ•°\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # Tokenizer è¿˜æ˜¯å¯ä»¥å¤ç”¨\n",
        "model = DistilBertForSequenceClassification(config) # model ä¸èƒ½ç”¨ from_pretrained åŠ è½½äº†ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ\n",
        "\n",
        "# Case 2: åªæ”¹å˜æœ€åä¸€å±‚ï¼Œæ¯”å¦‚æŠŠä¸€ä¸ªä¸¤åˆ†ç±»çš„æ¨¡å‹æ”¹æˆ 10 åˆ†ç±»çš„æ¨¡å‹\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10) # é€šè¿‡è®¾ç½® num_labels å‚æ•°æ¥å®ç°å¯¹æœ€åä¸€å±‚çš„ä¿®æ”¹\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "'''\n",
        "class DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3a_JHDX-6xk"
      },
      "source": [
        "##**ä½¿ç”¨**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8goWHm--T7"
      },
      "source": [
        "# ä½¿ç”¨ pipeline\n",
        "\n",
        "'''\n",
        "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨pipelineå‡½æ•°ï¼Œå®ƒæ”¯æŒå¦‚ä¸‹çš„ä»»åŠ¡ï¼š\n",
        "- æƒ…æ„Ÿåˆ†æ(Sentiment analysis)ï¼šä¸€æ®µæ–‡æœ¬æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„æƒ…æ„Ÿå€¾å‘\n",
        "- æ–‡æœ¬ç”Ÿæˆ(Text generation)ï¼šç»™å®šä¸€æ®µæ–‡æœ¬ï¼Œè®©æ¨¡å‹è¡¥å……åé¢çš„å†…å®¹\n",
        "- å‘½åå®ä½“è¯†åˆ«(Name entity recognition)ï¼šè¯†åˆ«æ–‡å­—ä¸­å‡ºç°çš„äººååœ°åçš„å‘½åå®ä½“\n",
        "- é—®ç­”(Question answering)ï¼šç»™å®šä¸€æ®µæ–‡æœ¬ä»¥åŠé’ˆå¯¹å®ƒçš„ä¸€ä¸ªé—®é¢˜ï¼Œä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆ\n",
        "- å¡«è¯(Filling masked text)ï¼šæŠŠä¸€æ®µæ–‡å­—çš„æŸäº›éƒ¨åˆ†maskä½ï¼Œç„¶åè®©æ¨¡å‹å¡«ç©º\n",
        "- æ‘˜è¦(Summarization)ï¼šæ ¹æ®ä¸€æ®µé•¿æ–‡æœ¬ä¸­ç”Ÿæˆç®€çŸ­çš„æ‘˜è¦\n",
        "- ç¿»è¯‘(Translation)ï¼šæŠŠä¸€ç§è¯­è¨€çš„æ–‡å­—ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€\n",
        "- ç‰¹å¾æå–(Feature extraction)ï¼šæŠŠä¸€æ®µæ–‡å­—ç”¨ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤º\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# é™¤äº†é€šè¿‡åå­—æ¥åˆ¶å®š model å‚æ•°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ ç»™ model ä¸€ä¸ªåŒ…å«æ¨¡å‹çš„ç›®å½•çš„è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ä¼ é€’ä¸€ä¸ªæ¨¡å‹å¯¹è±¡ã€‚\n",
        "# å¦‚æœæˆ‘ä»¬æƒ³ä¼ é€’æ¨¡å‹å¯¹è±¡ï¼Œé‚£ä¹ˆä¹Ÿéœ€è¦ä¼ å…¥ tokenizerã€‚\n",
        "# æˆ‘ä»¬éœ€è¦ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯ AutoTokenizerï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä¸‹è½½å’ŒåŠ è½½ä¸æ¨¡å‹åŒ¹é…çš„ Tokenizerã€‚\n",
        "# å¦ä¸€ä¸ªæ˜¯ AutoModelForSequenceClassificationã€‚\n",
        "# è¿™ä¸¤ä¸ª AutoXXX ç±»ä¼šæ ¹æ®åŠ è½½çš„æ¨¡å‹è‡ªåŠ¨é€‰æ‹© Tokenizer å’Œ Modelï¼Œå¦‚æœæˆ‘ä»¬æå‰çŸ¥é“äº†ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨å¯¹åº”çš„æ¨¡å‹å’Œ Tokenizer è¿›è¡Œ from_pretrained è°ƒç”¨\n",
        "# æ³¨æ„ï¼šæ¨¡å‹ç±»æ˜¯ä¸ä»»åŠ¡ç›¸å…³çš„ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯æƒ…æ„Ÿåˆ†ç±»çš„åˆ†ç±»ä»»åŠ¡ï¼Œæ‰€ä»¥æ˜¯AutoModelForSequenceClassificationã€‚\n",
        "\n",
        "# classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy to show you the ğŸ¤— Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvpz-zHQ5KTb"
      },
      "source": [
        "# å…³äº Tokenizer å’Œ Model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizer çš„ä½œç”¨å¤§è‡´å°±æ˜¯åˆ†è¯ï¼Œç„¶åæŠŠè¯å˜æˆçš„æ•´æ•° IDï¼Œæœ€ç»ˆçš„ç›®çš„æ˜¯æŠŠä¸€æ®µæ–‡æœ¬å˜æˆ ID çš„åºåˆ—ã€‚\n",
        "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
        "print(inputs)\n",
        "\n",
        "# ä¹Ÿå¯ä»¥è¾“å…¥ä¸€ä¸ª batch\n",
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# pt_batch ä»ç„¶æ˜¯ä¸€ä¸ª dictï¼Œinput_ids æ˜¯ä¸€ä¸ª batch çš„ ID åºåˆ—ï¼Œ\n",
        "# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬äºŒä¸ªå­—ç¬¦ä¸²è¾ƒçŸ­ï¼Œæ‰€ä»¥å®ƒè¢« padding æˆå’Œç¬¬ä¸€ä¸ªä¸€æ ·é•¿ã€‚\n",
        "# å¦‚æœæŸä¸ªå¥å­çš„é•¿åº¦è¶…è¿‡ max_lengthï¼Œä¹Ÿä¼šè¢«åˆ‡æ‰å¤šä½™çš„éƒ¨åˆ†ã€‚\n",
        "for key, value in pt_batch.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "# Tokenizer çš„å¤„ç†ç»“æœå¯ä»¥è¾“å…¥ç»™æ¨¡å‹ï¼Œå¯¹äº PyTorch éœ€è¦ä½¿ç”¨ ** æ¥å±•å¼€å‚æ•°\n",
        "# Transformers çš„æ‰€æœ‰è¾“å‡ºéƒ½æ˜¯ tupleï¼Œ é»˜è®¤è¿”å› logitsï¼Œå¦‚æœéœ€è¦æ¦‚ç‡ï¼Œå¯ä»¥è‡ªå·±åŠ  softmax\n",
        "pt_outputs = pt_model(**pt_batch)\n",
        "\n",
        "# å¦‚æœæœ‰è¾“å‡ºåˆ†ç±»å¯¹åº”çš„æ ‡ç­¾ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥ä¼ å…¥ï¼Œè¿™æ ·å®ƒé™¤äº†ä¼šè®¡ç®— logits è¿˜ä¼šè®¡ç®— loss\n",
        "# pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))\n",
        "\n",
        "# ä¹Ÿå¯ä»¥è¿”å›æ‰€æœ‰çš„éšçŠ¶æ€å’Œ attention\n",
        "# pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states, all_attentions = pt_outputs[-2:]\n",
        "\n",
        "pt_predictions = F.softmax(pt_outputs[0], dim=-1)\n",
        "print(pt_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZPqS7w-KN-"
      },
      "source": [
        "# å­˜å‚¨å’ŒåŠ è½½ä½¿ç”¨\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# è¿˜å¯ä»¥è½»æ¾çš„åœ¨ PyTorch å’Œ TensorFlow ä¹‹é—´åˆ‡æ¢\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
        "\n",
        "# å¦‚æœç”¨ PyTorch åŠ è½½ TensorFlow æ¨¡å‹ï¼Œåˆ™éœ€è¦è®¾ç½® from_tf = Trueï¼š\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsYtLIZ4-Inv"
      },
      "source": [
        "##**å¸¸è§ä»»åŠ¡**\n",
        "- æ­¤å¤„æ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯ä½¿ç”¨è‡ªåŠ¨æ„é€ çš„æ¨¡å‹(Auto Models)ï¼Œå®ƒä¼šä»æŸä¸ªcheckpointæ¢å¤æ¨¡å‹å‚æ•°ï¼Œå¹¶ä¸”è‡ªåŠ¨æ„é€ ç½‘ç»œ\n",
        "- ä¸ºäº†è·å¾—å¥½çš„æ•ˆæœï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°é€‚åˆè¿™ä¸ªä»»åŠ¡çš„checkpointã€‚è¿™äº›checkpointé€šå¸¸æ˜¯åœ¨å¤§é‡æ— æ ‡æ³¨æ•°æ®ä¸Šè¿›pretrainingå¹¶ä¸”åœ¨æŸä¸ªç‰¹å®šä»»åŠ¡ä¸Šfine-tuningåçš„ç»“æœ\n",
        "- å¹¶ä¸æ˜¯æ‰€æœ‰ä»»åŠ¡éƒ½æœ‰fine-tuningçš„æ¨¡å‹\n",
        "- fine-tuningçš„æ•°æ®é›†ä¸è§å¾—å’Œæˆ‘ä»¬çš„å®é™…ä»»åŠ¡å®Œå…¨åŒ¹é…ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è‡ªå·±fine-tuning\n",
        "- ä¸ºäº†è¿›è¡Œé¢„æµ‹ï¼ŒTransformersæä¾›ä¸¤ç§æ–¹æ³•ï¼špipelineå’Œè‡ªå·±æ„é€ æ¨¡å‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_B42sV-oqI"
      },
      "source": [
        "###**åˆ†ç±»**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNA7FH5E4YIl"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# ä½¿ç”¨ pipeline è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "result = nlp(\"I hate you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "result = nlp(\"I love you\")[0]\n",
        "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "\n",
        "# è‡ªå·±æ„é€ æ¨¡å‹åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸åŒå«ä¹‰ paraphrase\n",
        "'''\n",
        "1. ä» checkpoint æ„é€ ä¸€ä¸ª Tokenizer å’Œæ¨¡å‹\n",
        "2. ç»™å®šä¸¤ä¸ªè¾“å…¥å¥å­ï¼Œé€šè¿‡ tokenizer çš„__call__æ–¹æ³•æ­£ç¡®åœ°æ„é€ è¾“å…¥ï¼ŒåŒ…æ‹¬ token ç±»å‹å’Œ attention mask\n",
        "3. æŠŠè¾“å…¥ä¼ ç»™æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡º logits\n",
        "4. è®¡ç®— softmax å˜æˆæ¦‚ç‡\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
        "\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "\n",
        "paraphrase_classification_logits = model(**paraphrase).logits\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
        "\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "# Should be paraphrase\n",
        "for i in range(len(classes)):\n",
        "  print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
        "# Should not be paraphrase\n",
        "for i in range(len(classes)):\n",
        "  print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRxeOqpy-t7r"
      },
      "source": [
        "###**æŠ½å–å¼é—®ç­”**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoI2S6hP4ZFr"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# ä½¿ç”¨ pipeline\n",
        "nlp = pipeline(\"question-answering\")\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
        "\"\"\"\n",
        "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "\n",
        "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "\n",
        "# è‡ªå·±æ„é€ æ¨¡å‹\n",
        "'''\n",
        "1. æ„é€  Tokenizer å’Œæ¨¡å‹\n",
        "2. å®šä¹‰æ–‡æœ¬å’Œä¸€äº›é—®é¢˜\n",
        "3. å¯¹æ¯ä¸€ä¸ªé—®é¢˜æ„é€ è¾“å…¥ï¼ŒTokenizer ä¼šå¸®æˆ‘ä»¬æ’å…¥åˆé€‚çš„ç‰¹æ®Šç¬¦å·å’Œ attention mask\n",
        "4. è¾“å…¥æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°æ˜¯å¼€å§‹å’Œç»“æŸä¸‹æ ‡çš„ logits\n",
        "5. è®¡ç®— softmax å¹¶ä¸”é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ start å’Œ end\n",
        "6. æœ€ç»ˆæ ¹æ® start å’Œ end æˆªå–ç­”æ¡ˆæ–‡æœ¬\n",
        "'''\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "text = r\"\"\"\n",
        "ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"How many pretrained models are available in ğŸ¤— Transformers?\",\n",
        "    \"What does ğŸ¤— Transformers provide?\",\n",
        "    \"ğŸ¤— Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    # æ‹¿åˆ° token çš„å¼€å§‹å’Œç»“æŸä¸‹æ ‡åï¼Œéœ€è¦ç”¨ tokenizer.convert_ids_to_tokens å…ˆæŠŠ id å˜æˆ token\n",
        "    # ç„¶åç”¨ convert_tokens_to_string æŠŠ token å˜æˆå­—ç¬¦ä¸²\n",
        "    # å‰é¢çš„ pipeline æŠŠè¿™äº›å·¥ä½œéƒ½ç›´æ¥å¸®æˆ‘ä»¬åšå¥½äº†\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cV511NT-3Ey"
      },
      "source": [
        "###**æ–‡æœ¬ç”Ÿæˆ**\n",
        "æˆ‘ä»¬å¯ä»¥ç”¨è¯­è¨€æ¨¡å‹ fine-tuning é‡‡æ ·çš„æ–¹å¼ä¸€ä¸ªæ¥ä¸€ä¸ªçš„ç”Ÿæˆæ›´å¤šçš„æ–‡æœ¬ï¼Œä½†æ˜¯ Transformers å¸®æˆ‘ä»¬å®ç°äº†è¿™äº›é€»è¾‘ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oyxPz3k4Zv0"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "# æä¾›ä¸€æ®µ context çš„æ–‡æœ¬ï¼ŒæŒ‡å®šæœ€å¤šç”Ÿæˆ 50 ä¸ª Token\n",
        "# do_sample ä¸º False æŒ‡å®šé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è€Œä¸æ˜¯é‡‡æ ·ï¼Œä»è€Œæ¯æ¬¡è¿è¡Œçš„ç»“æœéƒ½æ˜¯å›ºå®šçš„\n",
        "# é»˜è®¤ä¼šä½¿ç”¨ gpt-2 çš„æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬\n",
        "# GPT-2ã€OpenAi-GPTã€CTRLã€XLNetã€Transfo-XL å’Œ Reformer ç­‰æ¨¡å‹éƒ½å¯ä»¥ç”¨äºç”Ÿæˆæ–‡æœ¬\n",
        "# XLNet é€šå¸¸éœ€è¦ padding ä¸€ä¸‹æ‰ä¼šè¾¾åˆ°æ¯”è¾ƒå¥½çš„æ•ˆæœï¼Œè€Œ GPT-2 åˆ™ä¸éœ€è¦\n",
        "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))\n",
        "print(text_generator(\"How to success?\", max_length=100, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7M2-coN-5DF"
      },
      "source": [
        "###**å‘½åå®ä½“è¯†åˆ«**\n",
        "æŠŠå‘½åå®ä½“è¯†åˆ«å½“æˆä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd6pIqXg4aPl"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
        "           \"close to the Manhattan Bridge which is visible from the window.\"\n",
        "print(nlp(sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUE_jjS--nW"
      },
      "source": [
        "###**æ‘˜è¦**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL8LaH1B4a9S"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "# pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\"\n",
        "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n",
        "\n",
        "# è‡ªå·±æ„é€ \n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
        "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n",
        "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "#print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(outputs[1])))\n",
        "print(outputs[-1]) # è¿˜éœ€è¦ä» ID è½¬åŒ–ä¸º Token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2IOoQC_Aod"
      },
      "source": [
        "###**ç¿»è¯‘**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqQ_CrZCZsuV"
      },
      "source": [
        "# for using Helsinki-NLP/opus-mt-en-zh\n",
        "# have to restart the runtime\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mORdTU-e4bcd"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "from transformers import pipeline, AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "# pipeline\n",
        "translator = pipeline(\"translation_en_to_de\")\n",
        "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))\n",
        "\n",
        "# è‡ªå®šä¹‰æ¨¡å‹\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
        "print(outputs) # è¿˜éœ€è¦ä» ID è½¬åŒ–ä¸º Token\n",
        "\n",
        "# ä¸­æ–‡ç¿»è¯‘\n",
        "model_cn = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
        "tokenizer_cn = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
        "translation = pipeline(\"translation_en_to_zh\", model=model_cn, tokenizer=tokenizer_cn)\n",
        "\n",
        "text = \"I like to study Data Science and Machine Learning\"\n",
        "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
        "print(translated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPSnQVR_J_2"
      },
      "source": [
        "###**è¯­è¨€æ¨¡å‹**\n",
        "1. é€šå¸¸æ˜¯ç”¨æ¥é¢„è®­ç»ƒåŸºç¡€çš„æ¨¡å‹ï¼Œç„¶åä¹Ÿå¯ä»¥ä½¿ç”¨é¢†åŸŸçš„æœªæ ‡æ³¨æ•°æ®æ¥fine-tuningè¯­è¨€æ¨¡å‹\n",
        "2. æ¯”å¦‚æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºåŸºç¡€çš„BERTæ¨¡å‹åœ¨æˆ‘ä»¬çš„åˆ†ç±»æ•°æ®ä¸Šfine-tuningæ¨¡å‹ã€‚å¯ä»¥ç”¨é¢†åŸŸçš„æœªæ ‡æ³¨æ•°æ®å¯¹åŸºç¡€çš„BERTç”¨è¯­è¨€æ¨¡å‹è¿™ä¸ªä»»åŠ¡è¿›è¡Œå†æ¬¡è¿›è¡Œpretrainingï¼Œç„¶åå†ç”¨æ ‡æ³¨çš„æ•°æ®fine-tuningåˆ†ç±»ä»»åŠ¡ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXMqNtSP4b-5"
      },
      "source": [
        "# Case 1: fine-tuning MaskedLM\n",
        "# pipeline\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "from pprint import pprint\n",
        "\n",
        "nlp = pipeline(\"fill-mask\")\n",
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))\n",
        "\n",
        "# è‡ªå·±æ„é€  Tokenizer å’Œæ¨¡å‹\n",
        "'''\n",
        "1. æ„é€ Tokenizerå’Œæ¨¡å‹ã€‚æ¯”å¦‚å¯ä»¥ä½¿ç”¨DistilBERTä»checkpointåŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹\n",
        "2. æ„é€ è¾“å…¥åºåˆ—ï¼ŒæŠŠéœ€è¦maskçš„è¯æ›¿æ¢æˆtokenizer.mask_token\n",
        "3. ç”¨tokenizeræŠŠè¾“å…¥å˜æˆID list\n",
        "4. è·å–é¢„æµ‹çš„ç»“æœï¼Œå®ƒçš„sizeæ˜¯è¯å…¸å¤§å°ï¼Œè¡¨ç¤ºé¢„æµ‹æŸä¸ªè¯çš„æ¦‚ç‡\n",
        "5. è·å–topkä¸ªæ¦‚ç‡æœ€å¤§çš„è¯\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "\n",
        "token_logits = model(input).logits\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "for token in top_5_tokens:\n",
        "  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgsuOmAaRKaX"
      },
      "source": [
        "# Case 2: é¢„æµ‹ä¸‹ä¸€ä¸ªè¯\n",
        "# æ ¹æ®æ¦‚ç‡é‡‡æ ·ä¸‹ä¸€ä¸ªè¯ï¼Œç„¶åä¸æ–­çš„é‡å¤è¿™ä¸ªè¿‡ç¨‹æ¥ç”Ÿæˆæ›´å¤šçš„æ–‡æœ¬\n",
        "\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "\n",
        "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
        "\n",
        "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "\n",
        "# filter\n",
        "# top_k_top_p_filtering çš„ä½œç”¨æ˜¯æŠŠé top-k çš„ logits å˜æˆè´Ÿæ— ç©·å¤§ï¼Œè¿™æ · softmax æ—¶è¿™äº›é¡¹å°±æ˜¯ 0\n",
        "# ä¹Ÿå¯ä»¥ä¼ å…¥å‚æ•° top_pï¼Œå®ƒçš„å«ä¹‰æ˜¯æ»¤æ‰æ¦‚ç‡å°äºå®ƒçš„é¡¹ç›®\n",
        "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "\n",
        "# sample é‡‡æ ·\n",
        "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vf_-mFeaxpR"
      },
      "source": [
        "##**Huggingface Transformer ä½¿ç”¨æ€»ç»“**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOqOC9iEa48L"
      },
      "source": [
        "###**é¢„å¤„ç†æ•°æ®**\n",
        "1. Transformers å¤„ç†æ•°æ®çš„ä¸»è¦å·¥å…·æ˜¯ tokenizer\n",
        "2. æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸æŸä¸ªæ¨¡å‹åŒ¹é…çš„ç‰¹å®š tokenizerï¼Œä¹Ÿå¯ä»¥é€šè¿‡ AutoTokenizer ç±»è‡ªåŠ¨å¸®æˆ‘ä»¬é€‰æ‹©åˆé€‚çš„ tokenizer\n",
        "3. Tokenizer çš„ä½œç”¨æ˜¯æŠŠè¾“å…¥æ–‡æœ¬åˆ‡åˆ†æˆ Tokenï¼Œç„¶åæŠŠ Token å˜æˆæ•´æ•° IDï¼Œé™¤æ­¤ä¹‹å¤–å®ƒä¹Ÿä¼šå¢åŠ ä¸€äº›é¢å¤–çš„ç‰¹æ®Š Token ä»¥å¤„ç†ç‰¹å®šçš„ä»»åŠ¡\n",
        "4. å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œé‚£ä¹ˆä¸€å®šè¦ä½¿ç”¨å®ƒçš„ Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bk_MEGZcDxG"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ä½¿ç”¨ AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# ç”¨æ³•1: __call__\n",
        "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
        "# è¿”å›ä¸€ä¸ª dictï¼ŒåŒ…å« input_idsã€token_type_ids å’Œ attention_mask\n",
        "print(encoded_input)\n",
        "# å¯ä»¥ç”¨ decode æ–¹æ³•æŠŠ ID æ¢å¤æˆå­—ç¬¦ä¸²\n",
        "# ä¼šå¢åŠ ä¸€äº›ç‰¹æ®Šçš„ Tokenï¼Œæ¯”å¦‚ [CLS] å’Œ [SEP]\n",
        "# å¹¶ä¸æ˜¯æ‰€æœ‰çš„æ¨¡å‹éƒ½éœ€è¦å¢åŠ ç‰¹æ®Š Tokenï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‚æ•° add_special_tokens=False æ¥ç¦ç”¨è¿™ä¸ªç‰¹æ€§\n",
        "print(tokenizer.decode(encoded_input[\"input_ids\"]))\n",
        "\n",
        "# ç”¨æ³•2: å¤„ç†ä¸€ä¸ª batch\n",
        "batch_sentences = [\"Hello I'm a single sentence\",\n",
        "                   \"And another sentence\",\n",
        "                   \"And the very very last one\"]\n",
        "encoded_inputs = tokenizer(batch_sentences)\n",
        "print(encoded_inputs)\n",
        "# å¯¹äºå¤§éƒ¨åˆ†åº”ç”¨ï¼Œbatch çš„å¤„ç†é€šå¸¸ä¼šéœ€è¦è¡¥é½æˆ–è€…æˆªæ–­ï¼Œä½¿å¾— batch å†…æ¯ä¸ªå¥å­éƒ½ä¸€æ ·é•¿ï¼Œå¹¶è¿”å› tensor\n",
        "# å¦‚æœæ²¡æœ‰æŒ‡å®šæœ€å¤§é•¿åº¦é™åˆ¶ï¼Œåˆ™ truncation ä¸èµ·ä½œç”¨\n",
        "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(batch)\n",
        "\n",
        "# ç”¨æ³•3: å¤„ç†ä¸¤ä¸ªè¾“å…¥\n",
        "# å¯ä»¥ç»™ __call__ æ–¹æ³•ä¼ å…¥ä¸¤ä¸ªå‚æ•°(ä¸æ˜¯ä¸€ä¸ª list çš„å‚æ•°)\n",
        "encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")\n",
        "print(encoded_input)\n",
        "print(tokenizer.decode(encoded_input[\"input_ids\"]))\n",
        "\n",
        "# ç”¨æ³•4: ä¼ å…¥ä¸¤ä¸ª listï¼Œä»è€Œè¿›è¡Œ batch å¤„ç†\n",
        "batch_sentences = [\"Hello I'm a single sentence\",\n",
        "                   \"And another sentence\",\n",
        "                   \"And the very very last one\"]\n",
        "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
        "                             \"And I should be encoded with the second sentence\",\n",
        "                             \"And I go with the very last one\"]\n",
        "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)\n",
        "print(encoded_inputs)\n",
        "for ids in encoded_inputs[\"input_ids\"]:\n",
        "  print(tokenizer.decode(ids))\n",
        "# ä¹Ÿå¯ä»¥ padding å’Œ truncate ä»¥åŠè¿”å› tensor\n",
        "# æ­¤å¤„æ˜¯ç¡®ä¿ batch å†…ä¸¤ä¸ªå¥å­æ‹¼æ¥åé•¿åº¦ä¸€è‡´ï¼Œä¸æ˜¯å¯¹ä¸¤ä¸ªæ‹¼æ¥çš„å¥å­è¿›è¡Œ padding æˆ–è€… truncation\n",
        "batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(batch)\n",
        "for ids in batch[\"input_ids\"]:\n",
        "  print(tokenizer.decode(ids))\n",
        "\n",
        "# ç”¨æ³•5: Pre-tokenized\n",
        "# Pre-tokenized æŒ‡çš„æ˜¯æå‰è¿›è¡Œäº†åˆ†è¯ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰è¿›è¡Œ subword çš„å¤„ç†\n",
        "# å¦‚æœè¾“å…¥æ˜¯ Pre-tokenizedï¼Œåˆ™å¯ä»¥ç»™å®šå‚æ•° is_split_into_words=True\n",
        "encoded_input = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"], is_split_into_words=True)\n",
        "print(encoded_input)\n",
        "# å¦‚æœå¤„ç†ä¸€ä¸ª batchï¼Œé‚£ä¹ˆå¯ä»¥ä¼ å­—ç¬¦ä¸² list\n",
        "batch_sentences = [[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],\n",
        "                   [\"And\", \"another\", \"sentence\"],\n",
        "                   [\"And\", \"the\", \"very\", \"very\", \"last\", \"one\"]]\n",
        "encoded_inputs = tokenizer(batch_sentences, is_split_into_words=True)\n",
        "print(encoded_inputs)\n",
        "# å¦‚æœæ¯ä¸ªè¾“å…¥éƒ½æ˜¯ä¸¤ä¸ªå¥å­ï¼Œå¯ä»¥ä¼ å…¥ä¸¤ä¸ªè¿™æ ·çš„å­—ç¬¦ä¸² list\n",
        "batch_of_second_sentences = [[\"I'm\", \"a\", \"sentence\", \"that\", \"goes\", \"with\", \"the\", \"first\", \"sentence\"],\n",
        "                             [\"And\", \"I\", \"should\", \"be\", \"encoded\", \"with\", \"the\", \"second\", \"sentence\"],\n",
        "                             [\"And\", \"I\", \"go\", \"with\", \"the\", \"very\", \"last\", \"one\"]]\n",
        "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=True)\n",
        "print(encoded_inputs)\n",
        "# ä¹Ÿå¯ä»¥ padding/truncating å¹¶ä¸”è¿”å› tensorï¼š\n",
        "batch = tokenizer(batch_sentences,\n",
        "                  batch_of_second_sentences,\n",
        "                  is_split_into_words=True,\n",
        "                  padding=True,\n",
        "                  truncation=True,\n",
        "                  return_tensors=\"pt\")\n",
        "print(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEwSEm1bbH-8"
      },
      "source": [
        "###**è®­ç»ƒå’Œ fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l-TTLxAwQvU"
      },
      "source": [
        "# ä½¿ç”¨ Trainer\n",
        "\n",
        "'''\n",
        "TrainingArgumentså‚æ•°æŒ‡å®šäº†è®­ç»ƒçš„è®¾ç½®ï¼š\n",
        "1. è¾“å‡ºç›®å½•\n",
        "2. æ€»çš„ epochs\n",
        "3. è®­ç»ƒçš„ batch_size\n",
        "4. é¢„æµ‹çš„ batch_size\n",
        "5. warmup çš„ step æ•°\n",
        "6. weight_decay å’Œ log ç›®å½•ã€‚\n",
        "\n",
        "ç„¶åä½¿ç”¨ trainer.train() å’Œ trainer.evaluate() å‡½æ•°å°±å¯ä»¥è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚\n",
        "æˆ‘ä»¬ä¹Ÿå¯ä»¥è‡ªå·±å®ç°æ¨¡å‹ï¼Œä½†æ˜¯è¦æ±‚å®ƒçš„ forward è¿”å›çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ lossã€‚\n",
        "'''\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset            # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# å¦‚æœæˆ‘ä»¬æƒ³è®¡ç®—é™¤äº† loss ä¹‹å¤–çš„æŒ‡æ ‡ï¼Œéœ€è¦ç»™ Trainer ä¼ å…¥ compute_metrics å‡½æ•°\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
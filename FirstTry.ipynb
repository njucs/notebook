{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMAFG5bYl0Nd8OwKBSb3o/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/FirstTry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgHcVQeTZe4v",
        "outputId": "cdf46945-0279-4c67-cb6a-194c3b02143a"
      },
      "source": [
        "# 临时测试代码\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "print(x.size(), y.size(), z.size())\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "torch.Size([5, 3])\n",
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
            "tensor([[-0.0330,  1.2461,  1.2818,  0.5821],\n",
            "        [-0.9219, -2.0599,  1.7669,  0.2533],\n",
            "        [-0.9650, -0.8131,  2.0064, -0.3247],\n",
            "        [-1.2489, -1.4372,  1.0121, -1.6097]])\n",
            "tensor([-0.0330,  1.2461,  1.2818,  0.5821, -0.9219, -2.0599,  1.7669,  0.2533,\n",
            "        -0.9650, -0.8131,  2.0064, -0.3247, -1.2489, -1.4372,  1.0121, -1.6097])\n",
            "tensor([[-0.0330,  1.2461,  1.2818,  0.5821, -0.9219, -2.0599,  1.7669,  0.2533],\n",
            "        [-0.9650, -0.8131,  2.0064, -0.3247, -1.2489, -1.4372,  1.0121, -1.6097]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pytorch文档**"
      ],
      "metadata": {
        "id": "yx9A9_IdexkD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXoP9d1dq3x"
      },
      "source": [
        "### **参考资料：**\n",
        "https://pytorch.apachecn.org/docs/1.7/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJsK425nKc6t"
      },
      "source": [
        "### **准备工作**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eohcOUEguDnu"
      },
      "source": [
        "# import 导入模块，每次使用模块中的函数都要是定是哪个模块\n",
        "# from … import * 导入模块，每次使用模块中的函数直接用就可以了，因为已经知道该函数是哪个模块中的了。\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import torchvision as tv\n",
        "from torchvision import models,transforms,datasets\n",
        "\n",
        "# 查看Python解释器\n",
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "# 测试GPU是否可用\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using gpu: %s ' % torch.cuda.is_available())\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# 把Tensor转成Image，方便可视化\n",
        "'''\n",
        "from torchvision.transforms import ToPILImage\n",
        "show = ToPILImage()\n",
        "\n",
        "x = torch.randn(300,500)\n",
        "show(x)#.resize((100, 100))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpsShNc8JzaY"
      },
      "source": [
        "### **数据加载和预处理**\n",
        "**Dataset**对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。\n",
        "\n",
        "**Dataloader**是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vJo3M5EJ3FR"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 定义对数据的预处理\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # 转为Tensor\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n",
        "                             ])\n",
        "\n",
        "# 训练集\n",
        "trainset = tv.datasets.CIFAR10(\n",
        "                    root='./data/tmp/', \n",
        "                    train=True, \n",
        "                    download=True,\n",
        "                    transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "                    trainset, \n",
        "                    batch_size=4,\n",
        "                    shuffle=True, \n",
        "                    num_workers=2)\n",
        "\n",
        "# 测试集\n",
        "testset = tv.datasets.CIFAR10(\n",
        "                    './data/tmp/',\n",
        "                    train=False, \n",
        "                    download=True, \n",
        "                    transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "                    testset,\n",
        "                    batch_size=4, \n",
        "                    shuffle=False,\n",
        "                    num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssx2oMPAQvYU"
      },
      "source": [
        "# 可以查看一下部分数据内容\n",
        "'''\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next() # 返回4张图片及标签\n",
        "print(' '.join('%11s'%classes[labels[j]] for j in range(4)))\n",
        "show(tv.utils.make_grid((images + 1) / 2)).resize((400,100))\n",
        "#show(images[2]).resize((100,100))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd07c6YN4mrs"
      },
      "source": [
        "### **定义网络**\n",
        "定义网络时，需要继承nn.Module，并实现它的forward方法，**把网络中具有可学习参数的层放在构造函数\\__init__中**。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放，但建议不放在其中，而在forward中使用nn.functional代替。\n",
        "\n",
        "**只要在nn.Module的子类中定义了forward函数，backward函数就会自动被实现(利用autograd)**。在forward 函数中可使用任何tensor支持的函数，还可以使用if、for循环、print、log等Python语法，写法和标准的Python写法一致。\n",
        "\n",
        "torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 input.unsqueeze(0)将batch_size设为１。即输入必须是N个samples，但N可以设为1。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWtNCYMn40KX"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        # nn.Module子类的函数必须在构造函数中执行父类的构造函数\n",
        "        # 下式等价于nn.Module.__init__(self)\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # 卷积层\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        \n",
        "        # 全连接层\n",
        "        self.fc1   = nn.Linear(16*5*5, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        # reshape，‘-1’表示自适应\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "if(use_gpu):\n",
        "    net = net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRSqMvbS7JhM"
      },
      "source": [
        "### **查看网络的可学习参数**\n",
        "\n",
        "网络的可学习参数通过net.parameters()返回，net.named_parameters可同时返回可学习的参数及名称。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axx1SbE17ZLy"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(params)\n",
        "\n",
        "for name,parameters in net.named_parameters():\n",
        "    print(name,':',parameters.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djwNlqXOJ93f"
      },
      "source": [
        "### **定义损失函数和优化器**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g-mvpBjqP3A"
      },
      "source": [
        "# 损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "'''\n",
        "criterion = nn.MSELoss() # 均方误差损失, 计算 output 和 target 之差的均方差.\n",
        "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数, 描述两个概率分布的差异, 当训练有 C 个类别的分类问题时很有效.\n",
        "criterion = nn.KLDivLoss() # 计算 input 和 target 之间的 KL 散度. KL 散度可用于衡量不同的连续分布之间的距离, 在连续的输出分布的空间上(离散采样)上进行直接回归时很有效.\n",
        "criterion = nn.BCELoss() # 二进制交叉熵损失 BCELoss. 二分类任务时的交叉熵计算函数. 注意目标的值的范围为0到1之间.\n",
        "criterion = nn.MultiLabelMarginLoss() # 多标签分类损失 MultiLabelMarginLoss\n",
        "criterion = nn.MultiLabelSoftMarginLoss() # 多标签 one-versus-all 损失\n",
        "criterion = nn.CosineEmbeddingLoss() # cosine 损失\n",
        "criterion = nn.MultiMarginLoss(p=1, margin=1.0) # 多类别分类的hinge损失\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, reduction='mean') # 三元组损失\n",
        "criterion = nn.NLLLoss() # 负对数似然损失. 用于训练 C 个类别的分类问题.\n",
        "'''\n",
        "\n",
        "# 优化器\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "'''\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adagrad(net.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0) # 一种自适应优化方法，是自适应的为各个参数分配不同的学习率\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) # 对Adagrad的一种改进，可缓解Adagrad学习率下降较快的问题\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) # 结合了Momentum和RMSprop，并进行了偏差修正\n",
        "'''\n",
        "\n",
        "if(use_gpu):\n",
        "    criterion = criterion.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6022z_bJ3en"
      },
      "source": [
        "### **训练网络并更新网络参数**\n",
        "\n",
        "所有网络的训练流程都是类似的，不断地执行如下流程：\n",
        "\n",
        "1. 输入数据\n",
        "2. 前向传播+反向传播\n",
        "3. 更新参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttWPDj5Iqclt"
      },
      "source": [
        "torch.set_num_threads(8)\n",
        "for epoch in range(20):  \n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "        # 输入数据\n",
        "        inputs, labels = data\n",
        "        if(use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        \n",
        "        # 梯度清零\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward + backward \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()   \n",
        "        \n",
        "        # 更新参数 \n",
        "        optimizer.step()\n",
        "        \n",
        "        # 打印log信息\n",
        "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999: # 每2000个batch打印一下训练状态\n",
        "            print('[%d, %5d] loss: %.3f' \\\n",
        "                  % (epoch+1, i+1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsc7w5qDqhjf"
      },
      "source": [
        "### **测试网络**\n",
        "测试部分看看效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0hZawuKqsw7"
      },
      "source": [
        "'''\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next() # 一个batch返回4张图片\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "print('实际的label: ', ' '.join(\\\n",
        "            '%08s'%classes[labels[j]] for j in range(4)))\n",
        "show(tv.utils.make_grid(images / 2 - 0.5)).resize((400,100))\n",
        "\n",
        "# 计算图片在每个类别上的分数\n",
        "outputs = net(images)\n",
        "# 得分最高的那个类\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "print('预测结果: ', ' '.join('%5s'\\\n",
        "            % classes[predicted[j]] for j in range(4)))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNzYsEX4qxgm"
      },
      "source": [
        "完整的测试结果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkmmRG0rqvyv"
      },
      "source": [
        "correct = 0 # 预测正确的图片数\n",
        "total = 0 # 总共的图片数\n",
        "\n",
        "# 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        if(use_gpu):\n",
        "            images = images.cuda()\n",
        "        outputs = net(images)\n",
        "        if(use_gpu):\n",
        "            outputs = outputs.cpu()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "print('10000张测试集中的准确率为: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMwJpmpcw0X1"
      },
      "source": [
        "### **其他常用技巧**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE92rqKXwlrF"
      },
      "source": [
        "#### 绘制损失曲线（TBD）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L13RQRWTwtEl"
      },
      "source": [
        "# 绘制损失曲线"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIJaZXihwvS3"
      },
      "source": [
        "#### 注意力可视化（TBD）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jVI91ajw1Vw"
      },
      "source": [
        "# 注意力可视化\n",
        "''' 这部分内容在Attention部分详细介绍\n",
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(trained_attn, cmap='viridis')\n",
        "ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "ax.set_yticklabels([''] + sentences[1].split(), fontdict={'fontsize': 14})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVQMBnSgwgYH"
      },
      "source": [
        "#### 模型序列化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEB1RW_Gw787"
      },
      "source": [
        "# 模型序列化\n",
        "\n",
        "# 保存\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# 加载\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# 在每一轮训练-验证过程中出现的最佳模型保存下来\n",
        "if epoch_acc > best_acc:\n",
        "    best_acc = epoch_acc\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# 最终加载最佳模型后返回\n",
        "model.load_state_dict(best_model_wts)\n",
        "return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-p_JSaw35R"
      },
      "source": [
        "#### 模型微调"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjii34R5w_Wh"
      },
      "source": [
        "# 模型微调\n",
        "\n",
        "'''\n",
        "Two types of transfer learning: finetuning and feature extraction.\n",
        "1. In finetuning, we start with a pretrained model and update all of the model’s\n",
        "   parameters for our new task, in essence retraining the whole model. \n",
        "2. In feature extraction, we start with a pretrained model and only update the \n",
        "   final layer weights from which we derive predictions. \n",
        "\n",
        "In general both transfer learning methods follow the same few steps:\n",
        "1. Initialize the pretrained model\n",
        "2. Reshape the final layer(s) to have the same number of outputs as the number of classes in the new dataset\n",
        "3. Define for the optimization algorithm which parameters we want to update during training\n",
        "4. Run the training step\n",
        "\n",
        "torch.autograd跟踪所有将其requires_grad标志设置为True的张量的操作。\n",
        "对于不需要梯度的张量，将此属性设置为False会将其从梯度计算 DAG 中排除。\n",
        "在 NN 中，不计算梯度的参数通常称为冻结参数。这种策略可以用来调整预训练网络。\n",
        "'''\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all the parameters in the network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Initialize and Reshape the Network\n",
        "# this is not an automatic procedure and is unique to each model\n",
        "# 在 resnet 中，分类器是最后一个线性层model.fc\n",
        "# 我们可以简单地将其替换为充当我们的分类器的新线性层（默认情况下未冻结）\n",
        "model.fc = nn.Linear(512, 10)\n",
        "'''\n",
        "其他常见模型的Reshape：\n",
        "model.classifier[6] = nn.Linear(4096,num_classes) # Alexnet\n",
        "model.classifier[6] = nn.Linear(4096,num_classes) # VGG\n",
        "model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) # Squeezenet\n",
        "model.classifier = nn.Linear(1024, num_classes) # DenseNet\n",
        "\n",
        "# Inception v3, to finetune this model we must reshape both layers.\n",
        "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
        "model.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "# 以下是一个比较完整的Reshape代码段：\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)\n",
        "'''\n",
        "\n",
        "# Optimize only the classifier\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05iqwh1gw6_c"
      },
      "source": [
        "#### 基于TensorBoard进行模型可视化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkmAeIatyych"
      },
      "source": [
        "# 基于TensorBoard进行模型可视化\n",
        "\n",
        "'''\n",
        "Tensorboard的工作流程简单来说是：\n",
        "1. 将代码运行过程中的，某些你关心的数据保存在一个文件夹中：这一步由代码中的writer完成\n",
        "2. 再读取这个文件夹中的数据，用浏览器显示出来：这一步通过在命令行运行tensorboard完成。\n",
        "'''\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "### 设置 TensorBoard\n",
        "writer = SummaryWriter('./path/to/log')\n",
        "\n",
        "### 写入 TensorBoard\n",
        "\n",
        "# 针对数值\n",
        "# tag指定可视化时这个变量的名字\n",
        "# scalar_value是你要存的值\n",
        "# global_step可以理解为x轴坐标。\n",
        "writer.add_scalar(tag, scalar_value, global_step=None, walltime=None)\n",
        "\n",
        "# 针对图像\n",
        "writer.add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')\n",
        "writer.add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW')\n",
        "\n",
        "# 可视化构建的模型\n",
        "writer.add_graph(net, images)\n",
        "writer.close()\n",
        "\n",
        "# 通过add_embedding方法可视化高维数据的低维表示\n",
        "class_labels = [classes[lab] for lab in labels]\n",
        "features = images.view(-1, 28 * 28)\n",
        "writer.add_embedding(features,\n",
        "                    metadata=class_labels,\n",
        "                    label_img=images.unsqueeze(1))\n",
        "writer.close()\n",
        "\n",
        "# 模型评估\n",
        "writer.add_pr_curve(classes[class_index],\n",
        "                    tensorboard_preds,\n",
        "                    tensorboard_probs,\n",
        "                    global_step=global_step)\n",
        "writer.close()\n",
        "\n",
        "### 可视化\n",
        "'''\n",
        "命令行：tensorboard --logdir=./path/to/the/folder --port 8123\n",
        "打开浏览器，访问地址 http://localhost:8123/ 即可\n",
        "'''\n",
        "# 变量归类\n",
        "# 命名变量的时候可以使用如下的格式，这样3个loss就会被显示在同一个section\n",
        "writer.add_scalar('loss/loss1', loss1, epoch)\n",
        "writer.add_scalar('loss/loss2', loss2, epoch)\n",
        "writer.add_scalar('loss/loss3', loss3, epoch)\n",
        "\n",
        "# 同时显示多个折线图\n",
        "# 只需要将两个日志文件夹放到同一目录下，并在命令行运行\n",
        "tensorboard --logdir=./path/to/the/root --port 8123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZ4T62iw-Y0"
      },
      "source": [
        "#### 并行和分布式训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4mL15sFn1uf"
      },
      "source": [
        "# 并行和分布式训练\n",
        "\n",
        "'''\n",
        "1. 最简单的是使用DataParallel在多个 GPU 上训练神经网络； \n",
        "   此功能将相同的模型复制到所有 GPU，其中每个 GPU 消耗输入数据的不同分区。\n",
        "   但不适用于模型太大而无法容纳单个 GPU 的某些用例\n",
        "2. 分布式数据并行训练（DDP）是一种广泛采用的单程序多数据训练范例。 \n",
        "   使用 DDP，可以在每个流程上复制模型，并且每个模型副本都将获得一组不同的输入数据样本。 \n",
        "   DDP 负责梯度通信，以保持模型副本同步，并使其与梯度计算重叠，以加快训练速度。\n",
        "3. 基于 RPC 的分布式训练（RPC）开发来支持无法适应数据并行训练的常规训练结构，\n",
        "   例如分布式管道并行性，参数服务器范式以及 DDP 与其他训练范式的组合。 \n",
        "   它有助于管理远程对象的生命周期，并将自动微分引擎扩展到机器范围之外。\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1gkqsuQxBTn"
      },
      "source": [
        "#### 模型优化及超参调整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdgeG0hrplNO"
      },
      "source": [
        "# 模型优化\n",
        "\n",
        "### 使用Profiler进行性能调试\n",
        "'''\n",
        "用于跟踪代码中各种 PyTorch 操作的时间和内存成本，有助于识别模型中的性能瓶颈\n",
        "使用 Profiler 会产生一些开销，并且最好仅用于调查代码\n",
        "如果要对运行时进行基准测试，请记住将其删除。\n",
        "'''\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.autograd.profiler as profiler\n",
        "\n",
        "# 使用profiler.record_function(\"label\")将每个子任务的代码包装在单独的带标签的上下文管理器中\n",
        "# 在事件探查器输出中，子任务中所有操作的综合性能指标将显示在其相应的标签下\n",
        "class MyModule(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        with profiler.record_function(\"LINEAR PASS\"):\n",
        "            out = self.linear(input)\n",
        "\n",
        "        with profiler.record_function(\"MASK INDICES\"):\n",
        "            threshold = out.sum(axis=1).mean().item()\n",
        "            hi_idx = np.argwhere(mask.cpu().numpy() > threshold)\n",
        "            hi_idx = torch.from_numpy(hi_idx).cuda()\n",
        "\n",
        "        return out, hi_idx\n",
        "\n",
        "# 在运行探查器之前，需要对 CUDA 进行预热，以确保进行准确的性能基准测试\n",
        "# with_stack=True参数在跟踪中附加操作的文件和行号\n",
        "model = MyModule(500, 10).cuda()\n",
        "input = torch.rand(128, 500).cuda()\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.double).cuda()\n",
        "\n",
        "model(input, mask) # warm-up\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)\n",
        "\n",
        "# 打印分析结果\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n",
        "\n",
        "### 使用 Ray Tune 进行超参数调整\n",
        "'''\n",
        "Ray Tune 是用于分布式超参数调整的行业标准工具，包含最新的超参数搜索算法。\n",
        "\n",
        "只需要添加一些细微的修改即可：\n",
        "1. 在函数中包装数据加载和训练，\n",
        "2. 使一些网络参数可配置，\n",
        "3. 添加检查点（可选），\n",
        "4. 定义用于模型调整的搜索空间\n",
        "\n",
        "需要预先安装以下包：\n",
        "ray[tune]：分布式超参数调整库\n",
        "torchvision：用于数据转换器\n",
        "'''\n",
        "\n",
        "# 导入\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from ray import tune ###\n",
        "from ray.tune import CLIReporter ###\n",
        "from ray.tune.schedulers import ASHAScheduler ###\n",
        "\n",
        "# 数据加载器\n",
        "# 传递一个全局数据目录，可以在不同的试验之间共享数据目录\n",
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset\n",
        "\n",
        "# 可配置的神经网络\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 训练\n",
        "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
        "# config参数将接收要训练的超参数\n",
        "# checkpoint_dir参数用于还原检查点\n",
        "# data_dir指定了加载和存储数据的目录\n",
        "\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        # 保存一个检查点，然后将一些指标报告给 Ray Tune\n",
        "        # Ray Tune 可以使用这些指标来决定哪种超参数配置可以带来最佳结果\n",
        "        # 这些指标还可用于尽早停止效果不佳的试验，以避免浪费资源进行试验\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "# 测试集的准确率\n",
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# main函数\n",
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data(data_dir)\n",
        "\n",
        "    # 定义 Ray Tune 的搜索空间\n",
        "    # tune.sample_from()函数可以定义自己的采样方法以获得超参数\n",
        "    config = {\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "\n",
        "    # 使用ASHAScheduler，它将尽早终止效果不佳的测试\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    \n",
        "    # 用functools.partial包装train_cifar函数以设置常量data_dir参数\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    # 训练完模型后，我们将找到表现最好的模型，并从检查点文件中加载训练后的网络\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzosdOB7xFim"
      },
      "source": [
        "#### 模型压缩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StsjsoTjqLkH"
      },
      "source": [
        "# 模型压缩\n",
        "\n",
        "### 方法一：模型剪裁，基于torch.nn.utils.prune\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Step1: 选择一种剪裁技术（或通过子类化BasePruningMethod实现您自己的东西）\n",
        "# Step2: 指定模块和该模块中要剪裁的参数的名称\n",
        "# Step3: 使用所选剪裁技术所需的适当关键字参数，指定剪裁参数\n",
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
        "prune.l1_unstructured(module, name=\"bias\", amount=3)\n",
        "\n",
        "# 使剪裁永久化，使用torch.nn.utils.prune的remove函数\n",
        "# 通过将参数weight重新分配给模型参数（剪裁后的版本）来使其永久不变\n",
        "prune.remove(module, 'weight')\n",
        "\n",
        "# 剪裁模型中的多个参数\n",
        "# 通过指定所需的剪裁技术和参数，可以轻松地剪裁网络中的多个张量\n",
        "for name, module in new_model.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "# 全局裁剪\n",
        "# 可能导致每个层的剪裁百分比不同\n",
        "model = LeNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")\n",
        "\n",
        "### 方法二：动态量化，减小模型大小的简单方法，且对精度的影响有限\n",
        "\n",
        "### 方法三：模型蒸馏"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **小土堆教程**"
      ],
      "metadata": {
        "id": "6J1q9118e88S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# from model import *\n",
        "# 准备数据集\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 定义训练的设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 定义数据集\n",
        "train_data = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=torchvision.transforms.ToTensor(),\n",
        "                                          download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n",
        "                                         download=True)\n",
        "\n",
        "''' 自定义数据集。实现 init、getitem 和 len 三个方法\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "class MyData(Dataset):\n",
        "\n",
        "    def __init__(self, file_dir, file_name, transform):\n",
        "        self.file_dir = file_dir\n",
        "        self.file_name = file_name\n",
        "        self.file_path = os.path.join(self.file_dir, self.file_name)\n",
        "        self.transform = transform\n",
        "        \n",
        "         with open(file_path, 'r') as f:\n",
        "            instance = f.readline()\n",
        "            # transform\n",
        "            self.data = ...\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "    file_dir = \"dataset/train\"\n",
        "    file_name1 = \"test1.txt\"\n",
        "    dataset1 = MyData(file_dir, file_name1, transform)\n",
        "    file_name2 = \"test2.txt\"\n",
        "    dataset2 = MyData(file_dir, file_name2, transform)\n",
        "    train_dataset = dataset1 + dataset2\n",
        "    dataloader = DataLoader(train_dataset, batch_size=1, num_workers=2)\n",
        "   \n",
        "'''\n",
        "\n",
        "# length 长度\n",
        "train_data_size = len(train_data)\n",
        "test_data_size = len(test_data)\n",
        "print(\"训练数据集的长度为：{}\".format(train_data_size))\n",
        "print(\"测试数据集的长度为：{}\".format(test_data_size))\n",
        "\n",
        "\n",
        "# 利用 DataLoader 来加载数据集\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# 创建网络模型\n",
        "class Test(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Test, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 5, 1, 2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 32, 5, 1, 2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 5, 1, 2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*4*4, 64),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    # tensor: reshape, squeeze, unsqueeze, cat, ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Test()\n",
        "\n",
        "'''\n",
        "# 修改网络\n",
        "\n",
        "model.add_module('add_linear', nn.Linear(10, 5)) # 在网络后加\n",
        "model.{label}.add_module('add_linear', nn.Linear(10, 5)) # 在网络内加\n",
        "model.{label}[2] = nn.Linear(10, 5) # 在网络内修改第二层\n",
        "\n",
        "'''\n",
        "model = model.to(device)\n",
        "\n",
        "# 损失函数\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "# 优化器\n",
        "# learning_rate = 0.01\n",
        "# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 设置训练网络的一些参数\n",
        "# 记录训练的次数\n",
        "total_train_step = 0\n",
        "# 记录测试的次数\n",
        "total_test_step = 0\n",
        "# 训练的轮数\n",
        "epoch = 10\n",
        "\n",
        "# 添加tensorboard\n",
        "writer = SummaryWriter(\"../logs_train\")\n",
        "\n",
        "for i in range(epoch):\n",
        "    print(\"-------第 {} 轮训练开始-------\".format(i+1))\n",
        "\n",
        "    # 训练步骤开始\n",
        "    model.train()  # be necessary when Normalization or Dropout layers are used in network\n",
        "    for data in train_dataloader:\n",
        "        imgs, targets = data\n",
        "        imgs = imgs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(imgs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # 优化器优化模型\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_step = total_train_step + 1\n",
        "        if total_train_step % 100 == 0:\n",
        "            print(\"训练次数：{}, Loss: {}\".format(total_train_step, loss.item()))\n",
        "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
        "\n",
        "    # 测试步骤开始\n",
        "    model.eval()  # be necessary when Normalization or Dropout layers are used in network\n",
        "    total_test_loss = 0\n",
        "    total_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            imgs, targets = data\n",
        "            imgs = imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            total_test_loss = total_test_loss + loss.item()\n",
        "            accuracy = (outputs.argmax(1) == targets).sum()\n",
        "            total_accuracy = total_accuracy + accuracy\n",
        "\n",
        "    print(\"整体测试集上的Loss: {}\".format(total_test_loss))\n",
        "    print(\"整体测试集上的正确率: {}\".format(total_accuracy/test_data_size))\n",
        "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
        "    writer.add_scalar(\"test_accuracy\", total_accuracy/test_data_size, total_test_step)\n",
        "    total_test_step = total_test_step + 1\n",
        "\n",
        "    # save model, or save model.state_dict(), the latter is recommended\n",
        "    torch.save(model, \"model {}.pth\".format(i))\n",
        "    print(\"模型已保存\")\n",
        "\n",
        "writer.close()\n",
        "\n",
        "# When the gradient is close to zero, optimization fails. We call this \"critical point\", not \"local minima\", because \"saddle point\" is also possible.\n",
        "# \"local minima\" may be a \"saddle point\" in higher dimension\n",
        "# Solutions to solve \"critical point\" problem: small batch, momentum\n",
        "# Training stuck (loss convergence but gradient not): adaptive learning rate, learning rate decay"
      ],
      "metadata": {
        "id": "MW4HZD8GfFH4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
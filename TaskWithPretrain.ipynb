{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TaskWithPretrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCfoP4HwJWElSC0b6oraR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/TaskWithPretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4bWlBGErAMH"
      },
      "source": [
        "### **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiUbuy1hrE38"
      },
      "source": [
        "'''\n",
        "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by wmathor\n",
        "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
        "              https://github.com/JayParks/transformer\n",
        "\n",
        "为什么说 Transformer 可以代替 seq2seq：\n",
        "这里用代替这个词略显不妥当，seq2seq 虽已老，但始终还是有其用武之地。\n",
        "seq2seq 最大的问题在于将 Encoder 端的所有信息压缩到一个固定长度的向量中，\n",
        "并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词 (token) 的隐藏状态。\n",
        "在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，\n",
        "而且这样一股脑的把该固定向量送入 Decoder 端，Decoder 端不能够关注到其想要关注的信息。\n",
        "Transformer 不但对 seq2seq 模型这两点缺点有了实质性的改进 (多头交互式 attention 模块)，\n",
        "而且还引入了 self-attention 模块，让源序列和目标序列首先 “自关联” 起来，这样的话，\n",
        "源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力，\n",
        "并且 Transformer 并行计算的能力远远超过了 seq2seq 系列模型\n",
        "'''\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "sentences = [\n",
        "        # enc_input           dec_input         dec_output\n",
        "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
        "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
        "]\n",
        "\n",
        "# 分开建词库\n",
        "# Padding Should be Zero\n",
        "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
        "src_vocab_size = len(src_vocab)\n",
        "\n",
        "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
        "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "src_len = 5 # enc_input max sequence length\n",
        "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
        "\n",
        "# Transformer Parameters\n",
        "d_model = 512  # Embedding Size，字嵌入 & 位置嵌入的维度，这俩值是相同的，因此用一个变量就行了\n",
        "d_ff = 2048 # FeedForward dimension，多头自注意力模型之后要传入两层线性层，该值表示 FeedForward 层隐藏神经元个数\n",
        "d_k = d_v = 64  # dimension of K(=Q), V，其中 Q 与 K 的维度必须相等，V 的维度没有限制\n",
        "n_layers = 6  # number of Encoder of Decoder Layer\n",
        "n_heads = 8  # number of heads in Multi-Head Attention\n",
        "\n",
        "def make_data(sentences):\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "    for i in range(len(sentences)):\n",
        "      enc_input = [[src_vocab[n] for n in sentences[i][0].split()]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
        "      dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]] # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
        "      dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]] # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
        "\n",
        "      enc_inputs.extend(enc_input)\n",
        "      dec_inputs.extend(dec_input)\n",
        "      dec_outputs.extend(dec_output)\n",
        "\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
        "\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self.enc_inputs = enc_inputs\n",
        "    self.dec_inputs = dec_inputs\n",
        "    self.dec_outputs = dec_outputs\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.enc_inputs.shape[0]\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
        "\n",
        "'''\n",
        "位置嵌入的维度为 [max_sequence_length, embedding_dimension], \n",
        "位置嵌入的维度与词向量的维度是相同的，都是 embedding_dimension。\n",
        "max_sequence_length 属于超参数，指的是限定每个句子最长由多少个词构成。\n",
        "\n",
        "一般以字为单位训练 Transformer 模型，位置嵌入的设计应该满足以下条件：\n",
        "1. 它应该为每个字输出唯一的编码\n",
        "2. 不同长度的句子之间，任何两个字之间的差值应该保持一致\n",
        "3. 它的值应该是有界的\n",
        "位置嵌入定义了一对sin和cos函数，频率随着向量维度下标的递增而递减，周期变长，t时刻字的位置编码是一个包含sin和cos函数的向量。\n",
        "这就好比二进制表示，但是用二进制表示一个数字太浪费空间了，因此我们可以使用与之对应的连续函数（正弦函数）。\n",
        "每一个位置在 embedding_dimension​维度上都会得到不同周期的sin和cos函数的取值组合，从而产生独一的纹理位置信息，最终使得模型学到位置之间的依赖关系和自然语言的时序特性。\n",
        "\n",
        "【注】位置嵌入在 Transformer 中不进行训练，但在 BERT 中会进行训练。\n",
        "'''\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: [seq_len, batch_size, d_model]\n",
        "        '''\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# 针对句子不够长，加了 pad，因此需要对 pad 进行 mask\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    '''\n",
        "    seq_q: [batch_size, seq_len]\n",
        "    seq_k: [batch_size, seq_len]\n",
        "    seq_len could be src_len or it could be tgt_len\n",
        "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
        "    '''\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    # 返回一个大小和 seq_k 一样的 tensor，只不过里面的值只有 True 和 False。\n",
        "    # 如果 seq_k 某个位置的值等于 0，那么对应位置就是 True (masked)，否则即为 False。\n",
        "    # word embedding 是三维的，要扩展一维\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], True is masked\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
        "\n",
        "'''\n",
        "Decoder input 不能看到未来时刻单词信息，因此需要 mask。\n",
        "\n",
        "Transformer Decoder 抛弃了 RNN，改为 Self-Attention，\n",
        "由此就产生了一个问题，在训练过程中，整个 ground truth 都暴露在 Decoder 中，\n",
        "这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 Mask。\n",
        "\n",
        "（以下似乎不是代码中的做法）\n",
        "Mask 很简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可，\n",
        "之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重。\n",
        "'''\n",
        "def get_attn_subsequence_mask(seq):\n",
        "    '''\n",
        "    seq: [batch_size, tgt_len]\n",
        "    '''\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    # k = 1 表示对角线位置上移一位\n",
        "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix\n",
        "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
        "    return subsequence_mask # [batch_size, tgt_len, tgt_len]\n",
        "\n",
        "'''\n",
        "查询矩阵 Q，键矩阵 K，值矩阵 V。\n",
        "\n",
        "为了获得第一个字的注意力权重，我们需要用第一个字的查询向量 q_1 乘以键矩阵 K，\n",
        "之后还需要将得到的值经过 softmax，使得它们的和为 1，\n",
        "有了权重之后，将权重其分别乘以对应字的值向量v_t，\n",
        "最后将这些权重化后的值向量求和，得到第一个字的输出。\n",
        "对其它的输入向量也执行相同的操作，即可得到通过 self-attention 后的所有输出。\n",
        "\n",
        "可以把上面的向量计算变成矩阵的形式，从而一次计算出所有时刻的输出：softmax(Q x K^T / sqrt(d_k))·V。\n",
        "'''\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        '''\n",
        "        Q: [batch_size, n_heads, len_q, d_k]\n",
        "        K: [batch_size, n_heads, len_k, d_k]\n",
        "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
        "        '''\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
        "\n",
        "        # softmax 中被 padding 的部分会参与运算，相当于让无效的部分参与了运算，这可能会产生很大的隐患\n",
        "        # 因此需要做一个 mask 操作，让这些无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置\n",
        "        # 此处和 attn_mask 相加，把一些需要屏蔽的信息屏蔽掉，attn_mask 是一个仅由 True 和 False 组成的 tensor，并且一定会保证 attn_mask 和 scores 的维度四个值相同\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
        "        \n",
        "        # 对 scores 进行 softmax，然后再与 V 相乘，得到 context\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
        "        return context, attn\n",
        "\n",
        "# Multi-Head Attention 的概念就是我们可以定义多组 Q/K/V，让它们分别关注不同的上下文。\n",
        "'''\n",
        "有三处地方调用 MultiHeadAttention()：\n",
        "1. Encoder Layer 调用一次，传入的 input_Q、input_K、input_V 全部都是 enc_inputs\n",
        "2. Decoder Layer 中两次调用，第一次传入的全是 dec_inputs，第二次传入的分别是 dec_outputs，enc_outputs，enc_outputs\n",
        "'''\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
        "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
        "        '''\n",
        "        input_Q: [batch_size, len_q, d_model]\n",
        "        input_K: [batch_size, len_k, d_model]\n",
        "        input_V: [batch_size, len_v(=len_k), d_model]\n",
        "        attn_mask: [batch_size, seq_len, seq_len]\n",
        "        '''\n",
        "        residual, batch_size = input_Q, input_Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
        "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
        "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
        "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
        "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
        "\n",
        "        # 加起来做残差连接\n",
        "        # Layer Normalization 的作用是把神经网络中隐藏层归一为标准正态分布，以起到加快训练速度，加速收敛的作用\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual), attn\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        # 两层线性映射并用激活函数激活\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, bias=False)\n",
        "        )\n",
        "    def forward(self, inputs):\n",
        "        '''\n",
        "        inputs: [batch_size, seq_len, d_model]\n",
        "        '''\n",
        "        residual = inputs\n",
        "        output = self.fc(inputs)\n",
        "        # FeedForward 残差连接与 Layer Normalization\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len, d_model]\n",
        "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
        "        '''\n",
        "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
        "        # 在 encoder-decoder attention 阶段，K，V 是从 encoder 传入的，所以此处前三个参数还是拆开分别传入\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
        "        return enc_outputs, attn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = MultiHeadAttention()\n",
        "        self.dec_enc_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        '''\n",
        "        dec_inputs: [batch_size, tgt_len, d_model]\n",
        "        enc_outputs: [batch_size, src_len, d_model]\n",
        "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
        "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
        "        '''\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        # 参数是列表list，列表里面存了 n_layers 个 Encoder Layer\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, enc_inputs):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        '''\n",
        "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
        "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
        "        enc_self_attns = []\n",
        "        for layer in self.layers: # 做 N 次\n",
        "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
        "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
        "            enc_self_attns.append(enc_self_attn)\n",
        "        return enc_outputs, enc_self_attns\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
        "        '''\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        enc_intpus: [batch_size, src_len]\n",
        "        enc_outputs: [batsh_size, src_len, d_model]\n",
        "        '''\n",
        "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\n",
        "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda() # [batch_size, tgt_len, d_model]\n",
        "        # 不仅要把 \"pad\"mask 掉，还要 mask 未来时刻的信息\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
        "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
        "        # torch.gt(a, value) 的意思是，将 a 中各个位置上的元素和 value 比较，若大于 value，则该位置取 1，否则取 0\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).cuda() # [batch_size, tgt_len, tgt_len]\n",
        "\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder().cuda()\n",
        "        self.decoder = Decoder().cuda()\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        '''\n",
        "        # tensor to store decoder outputs\n",
        "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        \n",
        "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
        "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
        "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
        "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
        "\n",
        "model = Transformer().cuda()\n",
        "# \"pad\" 的索引为 0，这样设置以后，就不会计算 \"pad\" 的损失\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
        "      '''\n",
        "      enc_inputs: [batch_size, src_len]\n",
        "      dec_inputs: [batch_size, tgt_len]\n",
        "      dec_outputs: [batch_size, tgt_len]\n",
        "      '''\n",
        "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n",
        "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
        "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "      loss = criterion(outputs, dec_outputs.view(-1))\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "def greedy_decoder(model, enc_input, start_symbol):\n",
        "    \"\"\"\n",
        "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
        "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
        "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
        "    :param model: Transformer Model\n",
        "    :param enc_input: The encoder input\n",
        "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
        "    :return: The target input\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
        "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
        "    terminal = False\n",
        "    next_symbol = start_symbol\n",
        "    while not terminal:         \n",
        "        dec_input=torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype)],-1)\n",
        "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
        "        projected = model.projection(dec_outputs)\n",
        "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
        "        next_word = prob.data[-1]\n",
        "        next_symbol = next_word\n",
        "        if next_symbol == tgt_vocab[\".\"]:\n",
        "            terminal = True\n",
        "        print(next_word)            \n",
        "    return dec_input\n",
        "\n",
        "# Test\n",
        "enc_inputs, _, _ = next(iter(loader))\n",
        "for i in range(len(enc_inputs)):\n",
        "    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n",
        "    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvc2zsR0y2Y_"
      },
      "source": [
        "### **BERT的实现**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE8fF0sby5aX"
      },
      "source": [
        "'''\n",
        "  code by Tae Hwan Jung(Jeff Jung) @graykode, modify by wmathor\n",
        "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
        "         https://github.com/JayParks/transformer, https://github.com/dhlee347/pytorchic-bert\n",
        "\n",
        "BERT 是以无监督的方式利用大量无标注文本「炼成」的语言模型，其架构为 Transformer 中的 Encoder。\n",
        "\n",
        "BERT 语言模型任务一：Masked Language Model\n",
        "- 随机遮盖或替换一句话里面的任意字或词\n",
        "- 让模型通过上下文预测那一个被遮盖或替换的部分\n",
        "- 做 Loss 的时候也只计算被遮盖部分的 Loss\n",
        "- 其余部分不做损失，其余部分无论输出什么东西，都无所谓\n",
        "\n",
        "BERT 语言模型任务二：Next Sentence Prediction\n",
        "- 判断第 2 个句子在原始本文中是否跟第 1 个句子相接\n",
        "- 在实际训练中，相接和不相接的情况出现的数量为 1:1\n",
        "- Segment Embedding 的作用是用 embedding 的信息让模型分开上下句\n",
        "- Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的\n",
        "\n",
        "BERT 预训练阶段实际上是将上述两个任务结合起来（Multi-task Learning），同时进行，然后将所有的 Loss 相加。\n",
        "\n",
        "拓展：GPT（Generative Pre-training Transformer）\n",
        "- GPT 使用的 Transformer 结构就是将 Encoder 中的 Self-Attention 替换成了 Masked Self-Attention\n",
        "  * GPT 原文是：We trained a 12-layer decoder-only transformer with masked self-attention heads\n",
        "  * 相当于 BERT 和 GPT 分别利用了 Transformer 的 Encoder 和 Decoder\n",
        "- 训练的过程也非常简单，就是将 n 个词的词嵌入加上位置嵌入，然后输入到 Transformer 中，n 个输出分别预测该位置的下一个词\n",
        "- 位置编码没有使用传统 Transformer 固定编码的方式，而是动态学习的\n",
        "- Pretraining 之后，再针对特定任务进行 Fine-Tuning，过程中与上述语言模型（基于任务数据）联调（Multi-Task Learning）\n",
        "- 场景1: Classification：对于分类问题，不需要做什么修改\n",
        "- 场景2: Entailment：对于推理问题，可以将先验与假设使用一个分隔符分开\n",
        "- 场景3: Similarity：对于相似度问题，由于模型是单向的，但相似度与顺序无关，所以要将两个句子顺序颠倒后，把两次输入的结果相加来做最后的推测\n",
        "- 场景4: Multiple-Choice：对于问答问题，则是将上下文、问题放在一起与答案分隔开，然后进行预测\n",
        "'''\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import *\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "\n",
        "text = (\n",
        "    'Hello, how are you? I am Romeo.\\n' # R\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
        "    'Nice meet you too. How are you today?\\n' # R\n",
        "    'Great. My baseball team won the competition.\\n' # J\n",
        "    'Oh Congratulations, Juliet\\n' # R\n",
        "    'Thank you Romeo\\n' # J\n",
        "    'Where are you going today?\\n' # R\n",
        "    'I am going shopping. What about you?\\n' # J\n",
        "    'I am going to visit my grandmother. she is not very well' # R\n",
        ")\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
        "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
        "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
        "for i, w in enumerate(word_list):\n",
        "    word2idx[w] = i + 4\n",
        "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "# 最终 token_list 是个二维的 list，里面每一行代表一句话\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word2idx[s] for s in sentence.split()]\n",
        "    token_list.append(arr)\n",
        "\n",
        "# BERT Parameters\n",
        "'''\n",
        "maxlen 表示同一个 batch 中的所有句子都由 30 个 token 组成，不够的补 PAD\n",
        "max_pred 表示最多需要预测多少个单词，即 BERT 中的完形填空任务\n",
        "n_layers 表示 Encoder Layer 的数量\n",
        "d_model 表示 Token Embeddings、Segment Embeddings、Position Embeddings 的维度\n",
        "d_ff 表示 Encoder Layer 中全连接层的维度\n",
        "n_segments 表示 Decoder input 由几句话组成\n",
        "'''\n",
        "maxlen = 30\n",
        "batch_size = 6\n",
        "max_pred = 5 # max tokens of prediction\n",
        "n_layers = 6\n",
        "n_heads = 12\n",
        "d_model = 768\n",
        "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2\n",
        "\n",
        "# 根据概率随机 mask 或者替换（以下统称 mask）一句话中 15% 的 token，还需要拼接任意两句话\n",
        "# sample IsNext and NotNext to be same in small batch size\n",
        "def make_data():\n",
        "    batch = []\n",
        "    # positive 变量代表两句话是连续的个数，negative 代表两句话不是连续的个数\n",
        "    # 在一个 batch 中，这两个样本的比例为 1:1\n",
        "    # 随机选取的两句话是否连续，只要通过判断 tokens_a_index + 1 == tokens_b_index 即可\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
        "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # MASK LM\n",
        "        # n_pred 变量代表的是即将 mask 的 token 数量\n",
        "        # cand_maked_pos 代表的是有哪些位置是候选的、可以 mask 的（因为像 [SEP]，[CLS] 这些不能做 mask，没有意义）\n",
        "        # 最后 shuffle() 一下，然后根据 random() 的值选择是替换为 [MASK] 还是替换为其它的 token\n",
        "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word2idx['[MASK]'] # make mask\n",
        "            elif random() > 0.9:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n",
        "                  index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index # replace\n",
        "\n",
        "        # Zero Paddings\n",
        "        # 补齐句子的长度，使得一个 batch 中的句子都是相同长度\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # Zero Padding (100% - 15%) tokens\n",
        "        # 补齐 mask 的数量，因为不同句子长度，会导致不同数量的单词进行 mask\n",
        "        # 需要保证同一个 batch 中，mask 的数量（必须）是相同的，所以也需要在后面补一些没有意义的东西，比方说 [0]\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch\n",
        "# Proprecessing Finished\n",
        "\n",
        "batch = make_data()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
        "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
        "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
        "    self.input_ids = input_ids\n",
        "    self.segment_ids = segment_ids\n",
        "    self.masked_tokens = masked_tokens\n",
        "    self.masked_pos = masked_pos\n",
        "    self.isNext = isNext\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, seq_len = seq_q.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "      Implementation of the gelu activation function.\n",
        "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "      Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\n",
        "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\n",
        "        return enc_outputs\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        # fc2 is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.fc2.weight = embed_weight\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids) # [bach_size, seq_len, d_model]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n",
        "        for layer in self.layers:\n",
        "            # output: [batch_size, max_len, d_model]\n",
        "            output = layer(output, enc_self_attn_mask)\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.fc(output[:, 0]) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
        "        # masked_pos 中第一行的 tensor 会作用于 output 的第一个 batch，使得 batch 中的句子顺序按照 masked_pos 中定义的重新调换\n",
        "        # masked_pos 中第二行的 tensor 会作用于 output 的第二个 batch，使得 batch 中的句子顺序按照 masked_pos 中定义的重新调换\n",
        "        # 以此类推\n",
        "        # 目的是计算 loss 时要对齐，即把预测位置的词往前移到对应位置，计算 loss 时正好一一对应\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
        "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
        "        return logits_lm, logits_clsf\n",
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
        "\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "for epoch in range(1000):\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
        "      logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "      loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
        "      loss_lm = (loss_lm.float()).mean()\n",
        "      loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "      loss = loss_lm + loss_clsf\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print (end-start)\n",
        "\n",
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\n",
        "print(text)\n",
        "print('================================')\n",
        "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
        "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)\n",
        "\n",
        "'''\n",
        "补充：让 BERT 可以处理超长文本\n",
        "\n",
        "BERT 无法处理超长文本的根本原因是 BERT 使用了从随机初始化训练出来的绝对位置编码，\n",
        "一般的最大位置设为了 512，因此顶多只能处理 512 个 token，多出来的部分就没有位置编码可用了。\n",
        "当然，还有一个重要的原因是 Attention 的复杂度高，导致长序列时显存用量大大增加，一般显卡也 finetune 不了。\n",
        "\n",
        "在有限资源的情况下，最理想的方案还是想办法延拓训练好的 BERT 的位置编码，而不用重新训练模型。\n",
        "苏剑林提出过一种层次分解方案，构建一种位置编码的延拓方法，且跟原来的前 n 个编码相容，然后还能外推到更多的位置。\n",
        "参考：https://kexue.fm/archives/7947\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSLDwSRpvD7q"
      },
      "source": [
        "### **作为预训练用于具体任务**\n",
        "如果仅仅是用来作为预训练模型实现具体任务，可以参考 Huggingface Transformer 的 notebook，提供了基于 Transformer 的更简单的方案。\n",
        "\n",
        "此处主要针对需要基于新的数据重新进行训练（fine-tuning）的情况。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMKU22kEvIwQ"
      },
      "source": [
        "'''\n",
        "BERT 的 Fine-Tuning 共分为 4 种类型：\n",
        "1. Classification：即输入是一个句子，输出是一个类别，应用场景：【情感分析】、【文本分类】\n",
        "   - 首先在输入句子的开头加一个代表分类的符号 [CLS]\n",
        "   - 将该位置的 output，丢给 Linear Classifier，让其 predict 一个 class\n",
        "   - 过程中 Linear Classifier 的参数是需要从头开始学习的，而 BERT 中的参数微调就可以了\n",
        "   - 基于 Transformer 结构，[CLS] 的 output 里面含有整句话的完整信息，并且由于 [CLS] 是没有任何实际意义的，所以 [CLS] 的 output 中自己的值占大头也不会影响到最终的结果\n",
        "2. Slot Filling：即输入是一个句子，输出是每个词对应的类别，应用场景：【Slot Filling】\n",
        "   - 将句子中各个字对应位置的 output 分别送入不同的 Linear，预测出该字的标签\n",
        "   - 本质上还是个分类问题，只不过是对每个字都要预测一个类别\n",
        "3. NLI：即输入是两个句子，输出是类别，应用场景：【自然语言推断NLI】\n",
        "   - 给定一个前提，然后给出一个假设，模型要判断出这个假设是正确、错误还是不知道\n",
        "   - 本质上是一个三分类的问题，和 Case 1 差不多，对 [CLS] 的 output 进行预测即可\n",
        "4. QA（抽取式）：输入是文档 D 和问题 Q，输出是答案的起始位置 <s, e>\n",
        "   - 将问题和文章通过 [SEP] 分隔，送入 BERT 之后，得到输出 O\n",
        "   - 训练两个 vector V_s & V_e，\n",
        "   - 首先将 V_s 和所有的 O 向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，即为 s\n",
        "   - 然后将 V_e 和所有的 O 向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，即为 e\n",
        "   - 如果 s > e，则没有答案\n",
        "\n",
        "其实 BERT 的预训练模式天然地适合【文本纠错】任务，另表。\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15NWXRdvDmEG",
        "outputId": "099301eb-bbbc-49e7-fd41-009747726fa2"
      },
      "source": [
        "# 安装 Huggingface Transformer\n",
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRdF1t5zF6lo"
      },
      "source": [
        "####**数据集下载**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFs-Xi-6CcI6",
        "outputId": "a5ce9d87-480f-4eaf-ddc8-e7782ec06c0b"
      },
      "source": [
        "# 当要使用 Kaggle API 直接下载数据时，需要安装 Kaggle\n",
        "# !pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle # Colab 中使用，否则会报错"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 36.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 4.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=5e67a37929aa9b510e1c733bc226ad08fe92fc8e0a611966aa916f6771974c6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0yaxprtEx2N",
        "outputId": "c90d1f1d-9612-41e3-b89c-cd131c2904ff"
      },
      "source": [
        "# 授权 Colab 访问 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtHlVf2JFcpY",
        "outputId": "ec9949a0-06ab-499a-99ee-171539648f2a"
      },
      "source": [
        "# 将 Kaggle 个人账户的 API Token 拷贝到 ~/.kaggle/ 目录\n",
        "!mkdir ~/.kaggle/\n",
        "!cp drive/MyDrive/'Colab Notebooks'/PracticalAI/kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmvboADoGtmA",
        "outputId": "b9566316-a6e9-40da-e03c-53e085bcf9b1"
      },
      "source": [
        "# 数据集比较大时，建议后台执行：\n",
        "# !nohup kaggle competitions download -c nlp-getting-started > download.log 2>&1 &\n",
        "\n",
        "''' Kaggle 下载之前，先要确保账户已有权限下载，可以在 Kaggle 对应页面上确认 '''\n",
        "!mkdir datasets\n",
        "!mkdir datasets/kaggle\n",
        "\n",
        "# 下载文本分类数据集: Kaggle competition：Real or Not? NLP with Disaster Tweets\n",
        "!kaggle competitions download -c nlp-getting-started\n",
        "!mv nlp-getting-started.zip datasets/kaggle/\n",
        "!unzip datasets/kaggle/nlp-getting-started.zip -d datasets/kaggle/nlp-getting-started/\n",
        "\n",
        "# 其他数据集下载\n",
        "\n",
        "# 查看下载数据集\n",
        "!ls\n",
        "!ls datasets/kaggle/nlp-getting-started/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n",
            "mkdir: cannot create directory ‘datasets/kaggle’: File exists\n",
            "Downloading nlp-getting-started.zip to /content\n",
            "  0% 0.00/593k [00:00<?, ?B/s]\n",
            "100% 593k/593k [00:00<00:00, 71.2MB/s]\n",
            "Archive:  datasets/kaggle/nlp-getting-started.zip\n",
            "replace datasets/kaggle/nlp-getting-started/sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/kaggle/nlp-getting-started/sample_submission.csv  \n",
            "replace datasets/kaggle/nlp-getting-started/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/kaggle/nlp-getting-started/test.csv  \n",
            "replace datasets/kaggle/nlp-getting-started/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace datasets/kaggle/nlp-getting-started/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/kaggle/nlp-getting-started/train.csv  \n",
            "datasets  drive  sample_data  sample_submission.csv  test.csv  train.csv\n",
            "sample_submission.csv  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vTlaZyRJR5A"
      },
      "source": [
        "####**文本分类**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "1q89A9uUJYFi",
        "outputId": "68b4a57b-7885-4e10-8dd1-a12d7e491123"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "''' BERT 预处理 '''\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# 先读取预训练的 bert-base-uncased 模型，用来进行分词，以及词向量转化\n",
        "# Get text values and labels\n",
        "text_values = train['final_text'].values\n",
        "labels = train['target'].values\n",
        "\n",
        "# Load the pretrained Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# 定义一个encode_fn把数据集的整个文本都转化为tokens\n",
        "# Function to get token ids for a list of texts \n",
        "def encode_fn(text_list):\n",
        "    all_input_ids = []    \n",
        "    for text in text_list:\n",
        "        input_ids = tokenizer.encode(\n",
        "                        text,                      \n",
        "                        add_special_tokens = True,  # 添加special tokens， 也就是CLS和SEP\n",
        "                        max_length = 160,           # 设定最大文本长度\n",
        "                        pad_to_max_length = True,   # pad到最大的长度  \n",
        "                        return_tensors = 'pt'       # 返回的类型为pytorch tensor\n",
        "                   )\n",
        "        all_input_ids.append(input_ids)    \n",
        "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "    return all_input_ids\n",
        "\n",
        "all_input_ids = encode_fn(text_values)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# 把数据分为训练集与验证集，并构建dataloader\n",
        "epochs = 4\n",
        "batch_size = 32\n",
        "\n",
        "# Split data into train and validation\n",
        "dataset = TensorDataset(all_input_ids, labels)\n",
        "train_size = int(0.90 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create train and validation dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "# 加载预训练的bert模型， 并定义optimizer与learning rate scheduler\n",
        "# Load the pretrained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=False, output_hidden_states=False)\n",
        "model.cuda()\n",
        "\n",
        "# create optimizer and learning rate schedule\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# 定义一个计算accuracy的方法，方便在训练的时候print出精确度的变化\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return accuracy_score(labels_flat, pred_flat)\n",
        "\n",
        "''' BERT 训练与验证 '''\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss, total_val_loss = 0, 0\n",
        "    total_eval_accuracy = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        model.zero_grad()\n",
        "        loss, logits = model(batch[0].to(device), token_type_ids=None, attention_mask=(batch[0]>0).to(device), labels=batch[1].to(device))\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step() \n",
        "        scheduler.step()\n",
        "        \n",
        "    model.eval()\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(batch[0].to(device), token_type_ids=None, attention_mask=(batch[0]>0).to(device), labels=batch[1].to(device))\n",
        "                \n",
        "            total_val_loss += loss.item()\n",
        "            \n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = batch[1].to('cpu').numpy()\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "    \n",
        "    print(f'Train loss     : {avg_train_loss}')\n",
        "    print(f'Validation loss: {avg_val_loss}')\n",
        "    print(f'Accuracy: {avg_val_accuracy:.2f}')\n",
        "    print('\\n')\n",
        "\n",
        "''' 模型预测 '''\n",
        "# Create the test data loader\n",
        "text_values = df_test['final_text'].values\n",
        "all_input_ids = encode_fn(text_values)\n",
        "pred_data = TensorDataset(all_input_ids)\n",
        "pred_dataloader = DataLoader(pred_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "for i, (batch,) in enumerate(pred_dataloader):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch.to(device), token_type_ids=None, attention_mask=(batch>0).to(device))\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        preds.append(logits)\n",
        "\n",
        "final_preds = np.concatenate(preds, axis=0)\n",
        "final_preds = np.argmax(final_preds, axis=1)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = df_test['id']\n",
        "submission['target'] = final_preds\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-99cf4623cc48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 先读取预训练的 bert-base-uncased 模型，用来进行分词，以及词向量转化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Get text values and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtext_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPAPi24dJYqR"
      },
      "source": [
        "####**Slot Filling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiX3BDCaJbvi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-3Ld8QJcIX"
      },
      "source": [
        "####**NLI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVXqEL6IJfhL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba7tEebaJgII"
      },
      "source": [
        "####**抽取式QA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SHj_pnlJiEW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
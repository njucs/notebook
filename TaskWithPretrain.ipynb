{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TaskWithPretrain.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I4bWlBGErAMH",
        "Vvc2zsR0y2Y_"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1W4ZvzaYywPLCqmBUDjUv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/TaskWithPretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4bWlBGErAMH"
      },
      "source": [
        "### **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiUbuy1hrE38"
      },
      "source": [
        "'''\n",
        "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by wmathor\n",
        "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
        "              https://github.com/JayParks/transformer\n",
        "\n",
        "为什么说 Transformer 可以代替 seq2seq：\n",
        "这里用代替这个词略显不妥当，seq2seq 虽已老，但始终还是有其用武之地。\n",
        "seq2seq 最大的问题在于将 Encoder 端的所有信息压缩到一个固定长度的向量中，\n",
        "并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词 (token) 的隐藏状态。\n",
        "在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，\n",
        "而且这样一股脑的把该固定向量送入 Decoder 端，Decoder 端不能够关注到其想要关注的信息。\n",
        "Transformer 不但对 seq2seq 模型这两点缺点有了实质性的改进 (多头交互式 attention 模块)，\n",
        "而且还引入了 self-attention 模块，让源序列和目标序列首先 “自关联” 起来，这样的话，\n",
        "源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力，\n",
        "并且 Transformer 并行计算的能力远远超过了 seq2seq 系列模型\n",
        "'''\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "sentences = [\n",
        "        # enc_input           dec_input         dec_output\n",
        "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
        "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
        "]\n",
        "\n",
        "# 分开建词库\n",
        "# Padding Should be Zero\n",
        "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
        "src_vocab_size = len(src_vocab)\n",
        "\n",
        "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
        "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "src_len = 5 # enc_input max sequence length\n",
        "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
        "\n",
        "# Transformer Parameters\n",
        "d_model = 512  # Embedding Size，字嵌入 & 位置嵌入的维度，这俩值是相同的，因此用一个变量就行了\n",
        "d_ff = 2048 # FeedForward dimension，多头自注意力模型之后要传入两层线性层，该值表示 FeedForward 层隐藏神经元个数\n",
        "d_k = d_v = 64  # dimension of K(=Q), V，其中 Q 与 K 的维度必须相等，V 的维度没有限制\n",
        "n_layers = 6  # number of Encoder of Decoder Layer\n",
        "n_heads = 8  # number of heads in Multi-Head Attention\n",
        "\n",
        "def make_data(sentences):\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "    for i in range(len(sentences)):\n",
        "      enc_input = [[src_vocab[n] for n in sentences[i][0].split()]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
        "      dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]] # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
        "      dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]] # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
        "\n",
        "      enc_inputs.extend(enc_input)\n",
        "      dec_inputs.extend(dec_input)\n",
        "      dec_outputs.extend(dec_output)\n",
        "\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
        "\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self.enc_inputs = enc_inputs\n",
        "    self.dec_inputs = dec_inputs\n",
        "    self.dec_outputs = dec_outputs\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.enc_inputs.shape[0]\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
        "\n",
        "'''\n",
        "位置嵌入的维度为 [max_sequence_length, embedding_dimension], \n",
        "位置嵌入的维度与词向量的维度是相同的，都是 embedding_dimension。\n",
        "max_sequence_length 属于超参数，指的是限定每个句子最长由多少个词构成。\n",
        "\n",
        "一般以字为单位训练 Transformer 模型，位置嵌入的设计应该满足以下条件：\n",
        "1. 它应该为每个字输出唯一的编码\n",
        "2. 不同长度的句子之间，任何两个字之间的差值应该保持一致\n",
        "3. 它的值应该是有界的\n",
        "位置嵌入定义了一对sin和cos函数，频率随着向量维度下标的递增而递减，周期变长，t时刻字的位置编码是一个包含sin和cos函数的向量。\n",
        "这就好比二进制表示，但是用二进制表示一个数字太浪费空间了，因此我们可以使用与之对应的连续函数（正弦函数）。\n",
        "每一个位置在 embedding_dimension​维度上都会得到不同周期的sin和cos函数的取值组合，从而产生独一的纹理位置信息，最终使得模型学到位置之间的依赖关系和自然语言的时序特性。\n",
        "\n",
        "【注】位置嵌入在 Transformer 中不进行训练，但在 BERT 中会进行训练。\n",
        "'''\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: [seq_len, batch_size, d_model]\n",
        "        '''\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# 针对句子不够长，加了 pad，因此需要对 pad 进行 mask\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    '''\n",
        "    seq_q: [batch_size, seq_len]\n",
        "    seq_k: [batch_size, seq_len]\n",
        "    seq_len could be src_len or it could be tgt_len\n",
        "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
        "    '''\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    # 返回一个大小和 seq_k 一样的 tensor，只不过里面的值只有 True 和 False。\n",
        "    # 如果 seq_k 某个位置的值等于 0，那么对应位置就是 True (masked)，否则即为 False。\n",
        "    # word embedding 是三维的，要扩展一维\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], True is masked\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
        "\n",
        "'''\n",
        "Decoder input 不能看到未来时刻单词信息，因此需要 mask。\n",
        "\n",
        "Transformer Decoder 抛弃了 RNN，改为 Self-Attention，\n",
        "由此就产生了一个问题，在训练过程中，整个 ground truth 都暴露在 Decoder 中，\n",
        "这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 Mask。\n",
        "\n",
        "（以下似乎不是代码中的做法）\n",
        "Mask 很简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可，\n",
        "之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重。\n",
        "'''\n",
        "def get_attn_subsequence_mask(seq):\n",
        "    '''\n",
        "    seq: [batch_size, tgt_len]\n",
        "    '''\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    # k = 1 表示对角线位置上移一位\n",
        "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix\n",
        "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
        "    return subsequence_mask # [batch_size, tgt_len, tgt_len]\n",
        "\n",
        "'''\n",
        "查询矩阵 Q，键矩阵 K，值矩阵 V。\n",
        "\n",
        "为了获得第一个字的注意力权重，我们需要用第一个字的查询向量 q_1 乘以键矩阵 K，\n",
        "之后还需要将得到的值经过 softmax，使得它们的和为 1，\n",
        "有了权重之后，将权重其分别乘以对应字的值向量v_t，\n",
        "最后将这些权重化后的值向量求和，得到第一个字的输出。\n",
        "对其它的输入向量也执行相同的操作，即可得到通过 self-attention 后的所有输出。\n",
        "\n",
        "可以把上面的向量计算变成矩阵的形式，从而一次计算出所有时刻的输出：softmax(Q x K^T / sqrt(d_k))·V。\n",
        "'''\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        '''\n",
        "        Q: [batch_size, n_heads, len_q, d_k]\n",
        "        K: [batch_size, n_heads, len_k, d_k]\n",
        "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
        "        '''\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
        "\n",
        "        # softmax 中被 padding 的部分会参与运算，相当于让无效的部分参与了运算，这可能会产生很大的隐患\n",
        "        # 因此需要做一个 mask 操作，让这些无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置\n",
        "        # 此处和 attn_mask 相加，把一些需要屏蔽的信息屏蔽掉，attn_mask 是一个仅由 True 和 False 组成的 tensor，并且一定会保证 attn_mask 和 scores 的维度四个值相同\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
        "        \n",
        "        # 对 scores 进行 softmax，然后再与 V 相乘，得到 context\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
        "        return context, attn\n",
        "\n",
        "# Multi-Head Attention 的概念就是我们可以定义多组 Q/K/V，让它们分别关注不同的上下文。\n",
        "'''\n",
        "有三处地方调用 MultiHeadAttention()：\n",
        "1. Encoder Layer 调用一次，传入的 input_Q、input_K、input_V 全部都是 enc_inputs\n",
        "2. Decoder Layer 中两次调用，第一次传入的全是 dec_inputs，第二次传入的分别是 dec_outputs，enc_outputs，enc_outputs\n",
        "'''\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
        "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
        "        '''\n",
        "        input_Q: [batch_size, len_q, d_model]\n",
        "        input_K: [batch_size, len_k, d_model]\n",
        "        input_V: [batch_size, len_v(=len_k), d_model]\n",
        "        attn_mask: [batch_size, seq_len, seq_len]\n",
        "        '''\n",
        "        residual, batch_size = input_Q, input_Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\n",
        "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
        "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
        "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
        "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
        "\n",
        "        # 加起来做残差连接\n",
        "        # Layer Normalization 的作用是把神经网络中隐藏层归一为标准正态分布，以起到加快训练速度，加速收敛的作用\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual), attn\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        # 两层线性映射并用激活函数激活\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, bias=False)\n",
        "        )\n",
        "    def forward(self, inputs):\n",
        "        '''\n",
        "        inputs: [batch_size, seq_len, d_model]\n",
        "        '''\n",
        "        residual = inputs\n",
        "        output = self.fc(inputs)\n",
        "        # FeedForward 残差连接与 Layer Normalization\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len, d_model]\n",
        "        enc_self_attn_mask: [batch_size, src_len, src_len]\n",
        "        '''\n",
        "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
        "        # 在 encoder-decoder attention 阶段，K，V 是从 encoder 传入的，所以此处前三个参数还是拆开分别传入\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
        "        return enc_outputs, attn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = MultiHeadAttention()\n",
        "        self.dec_enc_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        '''\n",
        "        dec_inputs: [batch_size, tgt_len, d_model]\n",
        "        enc_outputs: [batch_size, src_len, d_model]\n",
        "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
        "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
        "        '''\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        # 参数是列表list，列表里面存了 n_layers 个 Encoder Layer\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, enc_inputs):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        '''\n",
        "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]\n",
        "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\n",
        "        enc_self_attns = []\n",
        "        for layer in self.layers: # 做 N 次\n",
        "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
        "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
        "            enc_self_attns.append(enc_self_attn)\n",
        "        return enc_outputs, enc_self_attns\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
        "        '''\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        enc_intpus: [batch_size, src_len]\n",
        "        enc_outputs: [batsh_size, src_len, d_model]\n",
        "        '''\n",
        "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\n",
        "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda() # [batch_size, tgt_len, d_model]\n",
        "        # 不仅要把 \"pad\"mask 掉，还要 mask 未来时刻的信息\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
        "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\n",
        "        # torch.gt(a, value) 的意思是，将 a 中各个位置上的元素和 value 比较，若大于 value，则该位置取 1，否则取 0\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).cuda() # [batch_size, tgt_len, tgt_len]\n",
        "\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder().cuda()\n",
        "        self.decoder = Decoder().cuda()\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        '''\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        '''\n",
        "        # tensor to store decoder outputs\n",
        "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        \n",
        "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
        "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
        "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
        "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
        "\n",
        "model = Transformer().cuda()\n",
        "# \"pad\" 的索引为 0，这样设置以后，就不会计算 \"pad\" 的损失\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
        "      '''\n",
        "      enc_inputs: [batch_size, src_len]\n",
        "      dec_inputs: [batch_size, tgt_len]\n",
        "      dec_outputs: [batch_size, tgt_len]\n",
        "      '''\n",
        "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n",
        "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
        "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "      loss = criterion(outputs, dec_outputs.view(-1))\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "def greedy_decoder(model, enc_input, start_symbol):\n",
        "    \"\"\"\n",
        "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
        "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
        "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
        "    :param model: Transformer Model\n",
        "    :param enc_input: The encoder input\n",
        "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
        "    :return: The target input\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
        "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
        "    terminal = False\n",
        "    next_symbol = start_symbol\n",
        "    while not terminal:         \n",
        "        dec_input=torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype)],-1)\n",
        "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
        "        projected = model.projection(dec_outputs)\n",
        "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
        "        next_word = prob.data[-1]\n",
        "        next_symbol = next_word\n",
        "        if next_symbol == tgt_vocab[\".\"]:\n",
        "            terminal = True\n",
        "        print(next_word)            \n",
        "    return dec_input\n",
        "\n",
        "# Test\n",
        "enc_inputs, _, _ = next(iter(loader))\n",
        "for i in range(len(enc_inputs)):\n",
        "    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n",
        "    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvc2zsR0y2Y_"
      },
      "source": [
        "### **BERT的实现**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE8fF0sby5aX"
      },
      "source": [
        "'''\n",
        "  code by Tae Hwan Jung(Jeff Jung) @graykode, modify by wmathor\n",
        "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
        "         https://github.com/JayParks/transformer, https://github.com/dhlee347/pytorchic-bert\n",
        "\n",
        "BERT 是以无监督的方式利用大量无标注文本「炼成」的语言模型，其架构为 Transformer 中的 Encoder。\n",
        "\n",
        "BERT 语言模型任务一：Masked Language Model\n",
        "- 随机遮盖或替换一句话里面的任意字或词\n",
        "- 让模型通过上下文预测那一个被遮盖或替换的部分\n",
        "- 做 Loss 的时候也只计算被遮盖部分的 Loss\n",
        "- 其余部分不做损失，其余部分无论输出什么东西，都无所谓\n",
        "\n",
        "BERT 语言模型任务二：Next Sentence Prediction\n",
        "- 判断第 2 个句子在原始本文中是否跟第 1 个句子相接\n",
        "- 在实际训练中，相接和不相接的情况出现的数量为 1:1\n",
        "- Segment Embedding 的作用是用 embedding 的信息让模型分开上下句\n",
        "- Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的\n",
        "\n",
        "BERT 预训练阶段实际上是将上述两个任务结合起来（Multi-task Learning），同时进行，然后将所有的 Loss 相加。\n",
        "\n",
        "拓展：GPT（Generative Pre-training Transformer）\n",
        "- GPT 使用的 Transformer 结构就是将 Encoder 中的 Self-Attention 替换成了 Masked Self-Attention\n",
        "  * GPT 原文是：We trained a 12-layer decoder-only transformer with masked self-attention heads\n",
        "  * 相当于 BERT 和 GPT 分别利用了 Transformer 的 Encoder 和 Decoder\n",
        "- 训练的过程也非常简单，就是将 n 个词的词嵌入加上位置嵌入，然后输入到 Transformer 中，n 个输出分别预测该位置的下一个词\n",
        "- 位置编码没有使用传统 Transformer 固定编码的方式，而是动态学习的\n",
        "- Pretraining 之后，再针对特定任务进行 Fine-Tuning，过程中与上述语言模型（基于任务数据）联调（Multi-Task Learning）\n",
        "- 场景1: Classification：对于分类问题，不需要做什么修改\n",
        "- 场景2: Entailment：对于推理问题，可以将先验与假设使用一个分隔符分开\n",
        "- 场景3: Similarity：对于相似度问题，由于模型是单向的，但相似度与顺序无关，所以要将两个句子顺序颠倒后，把两次输入的结果相加来做最后的推测\n",
        "- 场景4: Multiple-Choice：对于问答问题，则是将上下文、问题放在一起与答案分隔开，然后进行预测\n",
        "'''\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import *\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "\n",
        "text = (\n",
        "    'Hello, how are you? I am Romeo.\\n' # R\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
        "    'Nice meet you too. How are you today?\\n' # R\n",
        "    'Great. My baseball team won the competition.\\n' # J\n",
        "    'Oh Congratulations, Juliet\\n' # R\n",
        "    'Thank you Romeo\\n' # J\n",
        "    'Where are you going today?\\n' # R\n",
        "    'I am going shopping. What about you?\\n' # J\n",
        "    'I am going to visit my grandmother. she is not very well' # R\n",
        ")\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
        "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
        "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
        "for i, w in enumerate(word_list):\n",
        "    word2idx[w] = i + 4\n",
        "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "# 最终 token_list 是个二维的 list，里面每一行代表一句话\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word2idx[s] for s in sentence.split()]\n",
        "    token_list.append(arr)\n",
        "\n",
        "# BERT Parameters\n",
        "'''\n",
        "maxlen 表示同一个 batch 中的所有句子都由 30 个 token 组成，不够的补 PAD\n",
        "max_pred 表示最多需要预测多少个单词，即 BERT 中的完形填空任务\n",
        "n_layers 表示 Encoder Layer 的数量\n",
        "d_model 表示 Token Embeddings、Segment Embeddings、Position Embeddings 的维度\n",
        "d_ff 表示 Encoder Layer 中全连接层的维度\n",
        "n_segments 表示 Decoder input 由几句话组成\n",
        "'''\n",
        "maxlen = 30\n",
        "batch_size = 6\n",
        "max_pred = 5 # max tokens of prediction\n",
        "n_layers = 6\n",
        "n_heads = 12\n",
        "d_model = 768\n",
        "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2\n",
        "\n",
        "# 根据概率随机 mask 或者替换（以下统称 mask）一句话中 15% 的 token，还需要拼接任意两句话\n",
        "# sample IsNext and NotNext to be same in small batch size\n",
        "def make_data():\n",
        "    batch = []\n",
        "    # positive 变量代表两句话是连续的个数，negative 代表两句话不是连续的个数\n",
        "    # 在一个 batch 中，这两个样本的比例为 1:1\n",
        "    # 随机选取的两句话是否连续，只要通过判断 tokens_a_index + 1 == tokens_b_index 即可\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
        "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # MASK LM\n",
        "        # n_pred 变量代表的是即将 mask 的 token 数量\n",
        "        # cand_maked_pos 代表的是有哪些位置是候选的、可以 mask 的（因为像 [SEP]，[CLS] 这些不能做 mask，没有意义）\n",
        "        # 最后 shuffle() 一下，然后根据 random() 的值选择是替换为 [MASK] 还是替换为其它的 token\n",
        "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word2idx['[MASK]'] # make mask\n",
        "            elif random() > 0.9:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n",
        "                  index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index # replace\n",
        "\n",
        "        # Zero Paddings\n",
        "        # 补齐句子的长度，使得一个 batch 中的句子都是相同长度\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # Zero Padding (100% - 15%) tokens\n",
        "        # 补齐 mask 的数量，因为不同句子长度，会导致不同数量的单词进行 mask\n",
        "        # 需要保证同一个 batch 中，mask 的数量（必须）是相同的，所以也需要在后面补一些没有意义的东西，比方说 [0]\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch\n",
        "# Proprecessing Finished\n",
        "\n",
        "batch = make_data()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
        "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
        "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
        "    self.input_ids = input_ids\n",
        "    self.segment_ids = segment_ids\n",
        "    self.masked_tokens = masked_tokens\n",
        "    self.masked_pos = masked_pos\n",
        "    self.isNext = isNext\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, seq_len = seq_q.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "      Implementation of the gelu activation function.\n",
        "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "      Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\n",
        "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\n",
        "        return enc_outputs\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        # fc2 is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.fc2.weight = embed_weight\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids) # [bach_size, seq_len, d_model]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n",
        "        for layer in self.layers:\n",
        "            # output: [batch_size, max_len, d_model]\n",
        "            output = layer(output, enc_self_attn_mask)\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.fc(output[:, 0]) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
        "        # masked_pos 中第一行的 tensor 会作用于 output 的第一个 batch，使得 batch 中的句子顺序按照 masked_pos 中定义的重新调换\n",
        "        # masked_pos 中第二行的 tensor 会作用于 output 的第二个 batch，使得 batch 中的句子顺序按照 masked_pos 中定义的重新调换\n",
        "        # 以此类推\n",
        "        # 目的是计算 loss 时要对齐，即把预测位置的词往前移到对应位置，计算 loss 时正好一一对应\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
        "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
        "        return logits_lm, logits_clsf\n",
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
        "\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "for epoch in range(1000):\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
        "      logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "      loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
        "      loss_lm = (loss_lm.float()).mean()\n",
        "      loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "      loss = loss_lm + loss_clsf\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print (end-start)\n",
        "\n",
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\n",
        "print(text)\n",
        "print('================================')\n",
        "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
        "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)\n",
        "\n",
        "'''\n",
        "补充：让 BERT 可以处理超长文本\n",
        "\n",
        "BERT 无法处理超长文本的根本原因是 BERT 使用了从随机初始化训练出来的绝对位置编码，\n",
        "一般的最大位置设为了 512，因此顶多只能处理 512 个 token，多出来的部分就没有位置编码可用了。\n",
        "当然，还有一个重要的原因是 Attention 的复杂度高，导致长序列时显存用量大大增加，一般显卡也 finetune 不了。\n",
        "\n",
        "在有限资源的情况下，最理想的方案还是想办法延拓训练好的 BERT 的位置编码，而不用重新训练模型。\n",
        "苏剑林提出过一种层次分解方案，构建一种位置编码的延拓方法，且跟原来的前 n 个编码相容，然后还能外推到更多的位置。\n",
        "参考：https://kexue.fm/archives/7947\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSLDwSRpvD7q"
      },
      "source": [
        "### **作为预训练用于具体任务**\n",
        "如果仅仅是用来作为预训练模型实现具体任务，可以参考 Huggingface Transformer 的 notebook，提供了基于 Transformer 的更简单的方案。\n",
        "\n",
        "此处主要针对需要基于新的数据重新进行训练（fine-tuning）的情况。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMKU22kEvIwQ"
      },
      "source": [
        "'''\n",
        "BERT 的 Fine-Tuning 共分为 4 种类型：\n",
        "1. Classification：即输入是一个句子，输出是一个类别，应用场景：【情感分析】、【文本分类】\n",
        "   - 首先在输入句子的开头加一个代表分类的符号 [CLS]\n",
        "   - 将该位置的 output，丢给 Linear Classifier，让其 predict 一个 class\n",
        "   - 过程中 Linear Classifier 的参数是需要从头开始学习的，而 BERT 中的参数微调就可以了\n",
        "   - 基于 Transformer 结构，[CLS] 的 output 里面含有整句话的完整信息，并且由于 [CLS] 是没有任何实际意义的，所以 [CLS] 的 output 中自己的值占大头也不会影响到最终的结果\n",
        "2. Slot Filling：即输入是一个句子，输出是每个词对应的类别，应用场景：【Slot Filling】\n",
        "   - 将句子中各个字对应位置的 output 分别送入不同的 Linear，预测出该字的标签\n",
        "   - 本质上还是个分类问题，只不过是对每个字都要预测一个类别\n",
        "3. NLI：即输入是两个句子，输出是类别，应用场景：【自然语言推断NLI】\n",
        "   - 给定一个前提，然后给出一个假设，模型要判断出这个假设是正确、错误还是不知道\n",
        "   - 本质上是一个三分类的问题，和 Case 1 差不多，对 [CLS] 的 output 进行预测即可\n",
        "4. QA（抽取式）：输入是文档 D 和问题 Q，输出是答案的起始位置 <s, e>\n",
        "   - 将问题和文章通过 [SEP] 分隔，送入 BERT 之后，得到输出 O\n",
        "   - 训练两个 vector V_s & V_e，\n",
        "   - 首先将 V_s 和所有的 O 向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，即为 s\n",
        "   - 然后将 V_e 和所有的 O 向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，即为 e\n",
        "   - 如果 s > e，则没有答案\n",
        "\n",
        "其实 BERT 的预训练模式天然地适合【文本纠错】任务，另表。\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15NWXRdvDmEG"
      },
      "source": [
        "# 安装 Huggingface Transformer\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRdF1t5zF6lo"
      },
      "source": [
        "####**数据集下载**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFs-Xi-6CcI6"
      },
      "source": [
        "# 当要使用 Kaggle API 直接下载数据时，需要安装 Kaggle\n",
        "# !pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle # Colab 中使用，否则会报错"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0yaxprtEx2N"
      },
      "source": [
        "# 授权 Colab 访问 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHlVf2JFcpY"
      },
      "source": [
        "# 将 Kaggle 个人账户的 API Token 拷贝到 ~/.kaggle/ 目录\n",
        "!mkdir ~/.kaggle/\n",
        "!cp drive/MyDrive/'Colab Notebooks'/PracticalAI/kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmvboADoGtmA"
      },
      "source": [
        "# 数据集比较大时，建议后台执行：\n",
        "# !nohup kaggle competitions download -c nlp-getting-started > download.log 2>&1 &\n",
        "\n",
        "''' Kaggle 下载之前，先要确保账户已有权限下载，可以在 Kaggle 对应页面上确认 '''\n",
        "!mkdir datasets\n",
        "!mkdir datasets/kaggle\n",
        "\n",
        "# 下载文本分类数据集: Kaggle competition：Real or Not? NLP with Disaster Tweets\n",
        "!kaggle competitions download -c nlp-getting-started\n",
        "!mv nlp-getting-started.zip datasets/kaggle/\n",
        "!unzip datasets/kaggle/nlp-getting-started.zip -d datasets/kaggle/nlp-getting-started/\n",
        "\n",
        "# 下载句子相似度判断数据集：Microsoft Research Paraphrase Corpus (MRPC)\n",
        "\n",
        "# 其他数据集下载\n",
        "\n",
        "# 查看下载数据集\n",
        "!ls\n",
        "!ls datasets/kaggle/nlp-getting-started/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vTlaZyRJR5A"
      },
      "source": [
        "####**文本分类**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q89A9uUJYFi"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# 读取数据\n",
        "for dirname, _, filenames in os.walk('datasets/kaggle/nlp-getting-started/'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "train = pd.read_csv(os.path.join(dirname, 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(dirname,'test.csv'))\n",
        "\n",
        "print(f'Training Set Shape: {train.shape}')\n",
        "print(f'Test Set Shape: {test.shape}')\n",
        "\n",
        "# 数据集中很多单元格的数据是 NaN，需要变成空字符串 ''，否则后期处理的时候会提示 float 和 str 的不兼容\n",
        "for df in [train, test]:\n",
        "  for col in ['keyword', 'location']:\n",
        "    df[col] = df[col].fillna('')\n",
        "\n",
        "# 数据清洗\n",
        "'''\n",
        "def clean(tweet): \n",
        "         \n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
        "            \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    \n",
        "    # Typos, slang and informal abbreviations\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
        "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
        "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
        "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
        "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
        "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
        "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
        "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
        "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
        "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
        "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
        "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
        "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
        "    \n",
        "    # Hashtags and usernames\n",
        "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
        "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
        "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
        "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
        "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
        "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
        "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
        "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
        "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
        "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
        "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
        "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
        "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
        "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
        "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
        "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
        "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
        "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
        "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
        "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
        "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
        "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
        "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
        "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
        "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
        "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
        "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
        "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
        "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
        "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
        "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
        "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
        "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
        "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
        "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
        "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
        "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
        "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
        "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
        "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
        "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
        "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
        "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
        "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
        "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
        "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
        "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
        "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
        "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
        "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
        "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
        "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
        "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
        "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
        "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
        "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
        "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
        "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
        "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
        "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
        "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
        "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
        "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
        "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
        "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
        "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
        "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
        "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
        "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
        "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
        "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
        "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
        "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
        "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
        "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
        "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
        "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
        "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
        "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
        "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
        "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
        "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
        "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
        "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
        "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
        "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
        "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
        "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
        "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
        "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
        "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
        "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
        "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
        "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
        "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
        "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
        "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
        "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
        "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
        "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
        "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
        "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
        "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
        "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
        "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
        "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
        "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
        "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
        "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
        "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
        "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
        "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
        "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
        "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
        "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
        "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
        "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
        "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
        "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
        "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
        "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
        "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
        "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
        "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
        "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
        "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
        "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
        "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
        "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
        "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
        "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
        "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
        "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
        "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
        "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
        "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
        "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
        "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
        "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
        "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
        "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
        "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
        "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
        "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
        "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
        "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
        "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
        "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
        "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
        "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
        "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
        "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
        "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
        "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
        "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
        "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
        "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
        "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
        "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
        "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
        "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
        "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
        "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
        "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
        "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
        "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
        "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
        "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
        "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
        "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
        "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
        "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
        "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
        "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
        "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
        "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
        "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
        "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
        "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
        "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
        "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
        "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
        "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
        "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
        "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
        "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
        "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
        "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
        "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
        "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
        "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
        "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
        "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
        "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
        "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
        "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
        "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
        "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
        "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
        "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
        "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
        "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
        "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
        "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
        "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
        "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
        "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
        "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
        "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
        "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
        "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
        "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
        "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
        "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
        "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
        "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
        "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
        "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
        "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
        "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
        "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
        "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
        "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
        "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
        "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
        "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
        "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
        "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
        "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
        "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
        "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
        "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
        "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
        "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
        "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
        "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
        "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
        "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
        "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
        "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
        "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
        "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
        "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
        "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
        "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
        "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
        "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
        "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
        "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
        "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
        "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
        "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
        "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
        "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
        "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
        "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
        "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
        "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
        "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
        "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
        "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
        "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
        "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
        "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
        "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
        "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
        "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
        "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
        "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
        "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
        "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
        "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
        "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
        "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
        "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
        "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
        "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
        "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
        "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
        "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
        "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
        "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
        "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
        "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
        "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
        "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
        "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
        "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
        "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
        "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
        "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
        "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
        "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
        "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
        "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
        "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
        "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
        "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
        "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
        "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
        "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
        "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
        "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
        "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
        "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
        "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
        "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
        "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
        "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
        "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
        "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
        "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
        "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
        "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
        "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
        "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
        "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
        "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
        "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
        "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
        "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
        "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
        "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
        "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
        "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
        "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
        "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
        "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
        "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
        "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
        "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
        "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
        "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
        "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
        "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
        "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
        "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
        "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
        "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
        "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
        "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
        "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
        "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
        "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
        "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
        "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
        "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
        "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
        "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
        "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
        "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
        "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
        "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
        "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
        "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
        "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
        "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
        "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
        "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
        "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
        "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
        "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
        "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
        "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
        "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
        "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
        "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
        "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
        "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
        "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
        "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
        "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
        "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
        "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
        "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
        "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
        "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
        "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
        "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
        "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
        "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
        "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
        "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
        "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
        "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
        "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
        "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
        "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
        "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
        "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
        "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
        "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
        "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
        "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
        "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
        "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
        "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
        "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
        "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
        "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
        "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
        "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
        "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
        "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
        "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
        "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
        "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
        "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
        "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
        "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
        "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
        "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
        "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
        "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
        "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
        "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
        "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
        "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
        "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
        "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
        "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
        "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
        "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
        "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
        "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
        "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
        "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
        "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
        "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
        "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
        "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
        "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
        "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
        "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
        "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
        "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
        "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
        "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
        "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
        "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
        "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
        "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
        "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
        "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
        "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
        "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
        "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
        "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
        "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
        "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
        "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
        "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
        "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
        "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
        "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
        "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
        "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
        "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
        "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
        "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
        "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
        "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
        "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
        "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
        "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
        "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
        "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
        "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
        "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
        "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
        "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
        "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
        "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
        "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
        "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
        "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
        "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
        "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
        "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
        "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
        "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
        "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
        "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
        "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
        "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
        "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
        "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
        "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
        "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
        "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
        "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
        "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
        "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
        "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
        "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
        "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
        "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
        "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
        "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
        "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
        "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
        "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
        "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
        "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
        "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
        "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
        "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
        "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
        "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
        "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
        "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
        "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
        "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
        "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
        "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
        "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
        "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
        "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
        "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
        "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
        "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
        "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
        "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
        "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
        "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
        "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
        "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
        "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
        "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
        "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
        "           \n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "        \n",
        "    # Words with punctuations and special characters\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
        "    for p in punctuations:\n",
        "        tweet = tweet.replace(p, f' {p} ')\n",
        "        \n",
        "    # ... and ..\n",
        "    tweet = tweet.replace('...', ' ... ')\n",
        "    if '...' not in tweet:\n",
        "        tweet = tweet.replace('..', ' ... ')      \n",
        "        \n",
        "    # Acronyms\n",
        "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
        "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
        "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
        "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
        "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
        "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
        "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
        "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
        "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)\n",
        "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
        "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)\n",
        "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
        "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)\n",
        "\n",
        "    # Grouping same words without embeddings\n",
        "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
        "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
        "    \n",
        "    return tweet\n",
        "'''\n",
        "#train['text_cleaned'] = train['text'].apply(lambda s : clean(s))\n",
        "#test['text_cleaned'] = test['text'].apply(lambda s : clean(s))\n",
        "\n",
        "train['text_cleaned'] = train['text']\n",
        "test['text_cleaned'] = test['text']\n",
        "\n",
        "train['final_text'] = train['keyword'] + ' ' + train['text_cleaned']\n",
        "test['final_text'] = test['keyword'] + ' ' + test['text_cleaned']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4lC2ht6gav_"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "''' BERT 预处理 '''\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# 先读取预训练的 bert-base-uncased 模型，用来进行分词，以及词向量转化\n",
        "# Get text values and labels\n",
        "text_values = train['final_text'].values\n",
        "labels = train['target'].values\n",
        "\n",
        "print(text_values)\n",
        "print(labels.shape)\n",
        "\n",
        "# Load the pretrained Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "print('... tokenizer loaded')\n",
        "\n",
        "# 定义一个encode_fn把数据集的整个文本都转化为tokens\n",
        "# Function to get token ids for a list of texts \n",
        "def encode_fn(text_list):\n",
        "    all_input_ids = []    \n",
        "    for text in text_list:\n",
        "        input_ids = tokenizer.encode(\n",
        "                        text,                      \n",
        "                        add_special_tokens = True,  # 添加special tokens， 也就是CLS和SEP\n",
        "                        max_length = 160,           # 设定最大文本长度\n",
        "                        pad_to_max_length = True,   # pad到最大的长度  \n",
        "                        return_tensors = 'pt'       # 返回的类型为pytorch tensor\n",
        "                   )\n",
        "        all_input_ids.append(input_ids)    \n",
        "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "    return all_input_ids\n",
        "\n",
        "all_input_ids = encode_fn(text_values)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# 把数据分为训练集与验证集，并构建dataloader\n",
        "epochs = 4\n",
        "batch_size = 32\n",
        "\n",
        "# Split data into train and validation\n",
        "dataset = TensorDataset(all_input_ids, labels)\n",
        "train_size = int(0.90 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create train and validation dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "# 加载预训练的bert模型， 并定义optimizer与learning rate scheduler\n",
        "# Load the pretrained BERT model\n",
        "# 从 Transformer 3.X 升级到 4.X 后，此处要添加 return_dict=False\n",
        "# enables the return of dict-like python objects containing the model outputs, instead of the standard tuples\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=False, output_hidden_states=False, return_dict=False)\n",
        "model.cuda()\n",
        "print('... model loaded')\n",
        "\n",
        "# create optimizer and learning rate schedule\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# 定义一个计算accuracy的方法，方便在训练的时候print出精确度的变化\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return accuracy_score(labels_flat, pred_flat)\n",
        "\n",
        "''' BERT 训练与验证 '''\n",
        "print('training starts...')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss, total_val_loss = 0, 0\n",
        "    total_eval_accuracy = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        model.zero_grad()\n",
        "        loss, logits = model(batch[0].to(device), token_type_ids=None, attention_mask=(batch[0]>0).to(device), labels=batch[1].to(device))\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step() \n",
        "        scheduler.step()\n",
        "        \n",
        "    model.eval()\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(batch[0].to(device), token_type_ids=None, attention_mask=(batch[0]>0).to(device), labels=batch[1].to(device))\n",
        "                \n",
        "            total_val_loss += loss.item()\n",
        "            \n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = batch[1].to('cpu').numpy()\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "    \n",
        "    print(f'Train loss     : {avg_train_loss}')\n",
        "    print(f'Validation loss: {avg_val_loss}')\n",
        "    print(f'Accuracy: {avg_val_accuracy:.2f}')\n",
        "    print('\\n')\n",
        "\n",
        "''' 模型预测 '''\n",
        "# Create the test data loader\n",
        "text_values = test['final_text'].values\n",
        "all_input_ids = encode_fn(text_values)\n",
        "pred_data = TensorDataset(all_input_ids)\n",
        "pred_dataloader = DataLoader(pred_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "for i, (batch,) in enumerate(pred_dataloader):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch.to(device), token_type_ids=None, attention_mask=(batch>0).to(device))\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        preds.append(logits)\n",
        "\n",
        "final_preds = np.concatenate(preds, axis=0)\n",
        "final_preds = np.argmax(final_preds, axis=1)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test['id']\n",
        "submission['target'] = final_preds\n",
        "#submission.to_csv('submission.csv', index=False)\n",
        "submission.to_csv(os.path.join(dirname, 'submission.csv'), index=False)\n",
        "\n",
        "print(submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzHHq-tzEMQ"
      },
      "source": [
        "####**句子相似度判断**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbNup9WmzI3E"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from itertools import islice\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class MRPCDataset(Dataset):\n",
        "     def __init__(self):\n",
        "        file = open('data/msr_paraphrase_train.txt', 'r', encoding=\"utf-8\")\n",
        "        data = []\n",
        "        for line in islice(file, 1, None):\n",
        "            content = line.split(sep='\\t')\n",
        "            data.append(content)\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "     def __getitem__(self, index):\n",
        "         if self.data[index][0] == '0':\n",
        "             label = 0\n",
        "         else:\n",
        "             label = 1\n",
        "\n",
        "         sentences = self.data[index][3] + '[SEP]' + self.data[index][4]\n",
        "         return sentences, label\n",
        "\n",
        "     def __len__(self):\n",
        "         return len(self.data)\n",
        "\n",
        "class FCModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCModel, self).__init__()\n",
        "        self.fc = torch.nn.Linear(in_features=768, out_features=1)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        score = self.fc(input)\n",
        "        result = torch.sigmoid(score)\n",
        "        return result\n",
        "\n",
        "#载入数据预处理模块\n",
        "mrpcDataset = MRPCDataset()\n",
        "train_loader = DataLoader(dataset=mrpcDataset, batch_size=32, shuffle=True)\n",
        "print(\"数据载入完成\")\n",
        "\n",
        "#设置运行设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"设备配置完成\")\n",
        "\n",
        "#加载bert模型\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model.to(device)\n",
        "print(\"bert层模型创建完成\")\n",
        "\n",
        "#创建模型对象\n",
        "model = FCModel()\n",
        "model = model.to(device)\n",
        "print(\"全连接层模型创建完成\")\n",
        "\n",
        "#定义优化器&损失函数\n",
        "# 全连接层和 bert 模型的学习率是不一样的，因为 bert 只需要微调，所以学习率应该很小\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "bert_optimizer = torch.optim.Adam(bert_model.parameters(), lr=0.001)\n",
        "crit = torch.nn.BCELoss()\n",
        "\n",
        "#计算准确率的公式\n",
        "def binary_accuracy(predict, label):\n",
        "    rounded_predict = torch.round(predict)\n",
        "    correct = (rounded_predict == label).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "\n",
        "#定义训练方法\n",
        "def train():\n",
        "    #记录统计信息\n",
        "    epoch_loss, epoch_acc = 0., 0.\n",
        "    total_len = 0\n",
        "\n",
        "    #分batch进行训练\n",
        "    for i, data in enumerate(train_loader):\n",
        "        bert_model.train()\n",
        "        model.train()\n",
        "\n",
        "        sentence, label = data\n",
        "        label = label.cuda()\n",
        "\n",
        "        encoding = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
        "        bert_output = bert_model(**encoding.to(device))\n",
        "        pooler_output = bert_output.pooler_output\n",
        "        predict = model(pooler_output).squeeze()\n",
        "\n",
        "        loss = crit(predict, label.float())\n",
        "        acc = binary_accuracy(predict, label)\n",
        "\n",
        "        #gd\n",
        "        optimizer.zero_grad() #把梯度重置为零\n",
        "        bert_optimizer.zero_grad()\n",
        "        loss.backward() #求导\n",
        "        optimizer.step() #更新模型\n",
        "        bert_optimizer.step()\n",
        "\n",
        "        epoch_loss += loss * len(label)\n",
        "        epoch_acc += acc * len(label)\n",
        "        total_len += len(label)\n",
        "\n",
        "        print(\"batch %d loss:%f accuracy:%f\" % (i, loss, acc))\n",
        "\n",
        "    return epoch_loss/total_len, epoch_acc/total_len\n",
        "\n",
        "#开始训练\n",
        "Num_Epoch = 3\n",
        "index = 0\n",
        "for epoch in range(Num_Epoch):\n",
        "    epoch_loss, epoch_acc = train()\n",
        "    index += 1\n",
        "    print(\"EPOCH %d loss:%f accuracy:%f\" % (index, epoch_loss, epoch_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPAPi24dJYqR"
      },
      "source": [
        "####**Slot Filling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiX3BDCaJbvi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-3Ld8QJcIX"
      },
      "source": [
        "####**NLI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVXqEL6IJfhL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba7tEebaJgII"
      },
      "source": [
        "####**抽取式QA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SHj_pnlJiEW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
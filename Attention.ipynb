{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwE9ccI0lyjpgVrBfCvNZu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQaxCCsDwH6"
      },
      "source": [
        "### **Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5r4Ec6D3vP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJQvH_DDq0y"
      },
      "source": [
        "### **一个例子**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "6I8sWPl2DVWA",
        "outputId": "f3484fc0-f9fb-4f78-8a96-e46e306c9b7f"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0400 cost = 0.000522\n",
            "Epoch: 0800 cost = 0.000170\n",
            "Epoch: 1200 cost = 0.000084\n",
            "Epoch: 1600 cost = 0.000050\n",
            "Epoch: 2000 cost = 0.000033\n",
            "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
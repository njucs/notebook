{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN24z/7i0jRX1g9/466OP5y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQaxCCsDwH6"
      },
      "source": [
        "### **Attention**\n",
        "参考：\\\n",
        "https://wmathor.com/index.php/archives/1450/ \\\n",
        "https://wmathor.com/index.php/archives/1451/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWPcdJC0MbQ6"
      },
      "source": [
        "#### 可视化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5r4Ec6D3vP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ddc33e4-f255-4600-b152-48f191ce2a62"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "a = torch.randn(12, 18)\n",
        "b = a.softmax(dim=1)\n",
        "c = a.softmax(dim=0).transpose(0, 1)\n",
        "print(a, '\\n',  b, '\\n', c)\n",
        "d = b.matmul(c)\n",
        "print(d)\n",
        "\n",
        "d = d.numpy()\n",
        "\n",
        "variables = ['A','B','C','D','E','F','G','H','I','J','K','L']\n",
        "labels = ['ID_0','ID_1','ID_2','ID_3','ID_4','ID_5','ID_6','ID_7','ID_8','ID_9','ID_10','ID_11']\n",
        "\n",
        "df = pd.DataFrame(d, columns=variables, index=labels)\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cax = ax.matshow(df, interpolation='nearest', cmap='hot_r')\n",
        "fig.colorbar(cax)\n",
        "\n",
        "tick_spacing = 1\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
        "\n",
        "ax.set_xticklabels([''] + list(df.columns))\n",
        "ax.set_yticklabels([''] + list(df.index))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "nlp = spacy.load('en')    #import spacy，用于分词\n",
        "sent = nlp('Which NFL team represented the AFC at Super Bowl 50')  #对文章提一个问题\n",
        "doc = open('F:/spacy.txt').read()\n",
        "doc = nlp(doc)\n",
        "\n",
        "data = []\n",
        "for token1 in doc:\n",
        "     data.append([token1.similarity(token2) for token2 in sent])\n",
        "\n",
        "d = np.array(data)\n",
        "d = d.transpose()\n",
        "col = [t.text for t in doc]     #需要显示的词\n",
        "index = [t.text for t in sent]  #需要显示的词\n",
        "df = pd.DataFrame(d, columns=col, index=index )\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cax = ax.matshow(df, interpolation='nearest', cmap='hot_r')\n",
        "#cax = ax.matshow(df)\n",
        "fig.colorbar(cax)\n",
        "\n",
        "tick_spacing = 1\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
        "\n",
        "# fontdict = {'rotation': 'vertical'}    #设置文字旋转\n",
        "fontdict = {'rotation': 90}       #或者这样设置文字旋转\n",
        "#ax.set_xticklabels([''] + list(df.columns), rotation=90)  #或者直接设置到这里\n",
        "# Axes.set_xticklabels(labels, fontdict=None, minor=False, **kwargs)\n",
        "ax.set_xticklabels([''] + list(df.columns), fontdict=fontdict)\n",
        "ax.set_yticklabels([''] + list(df.index))\n",
        "\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.3802,  0.1057,  0.2663, -0.6036, -1.1020, -1.1005,  0.1153, -0.7060,\n",
            "          2.0423,  1.4234,  0.9236, -0.3738,  0.9584, -0.2744, -0.8748, -1.5035,\n",
            "          0.5673,  0.0726],\n",
            "        [-0.5579,  0.8985, -0.5760,  1.5135, -2.1968, -0.0455,  0.8756,  1.6081,\n",
            "          1.9654, -0.1541, -1.2582,  1.0514, -0.6922,  0.5173, -1.0673, -1.1837,\n",
            "          0.4460,  0.1508],\n",
            "        [ 1.2952, -0.0948,  0.5961, -0.4509, -0.1048,  0.4977,  1.8255, -1.1422,\n",
            "         -0.3410,  1.2866, -0.6761, -0.5770,  0.8956,  1.7030,  1.4805,  0.4248,\n",
            "          0.9538,  0.2951],\n",
            "        [-0.7492,  0.8579,  0.5780,  0.6131, -0.5323, -0.5800,  0.3876,  0.9761,\n",
            "         -0.3077,  1.6344, -0.8877,  1.4753, -1.0533, -1.9998, -0.0770,  0.1073,\n",
            "         -0.3772,  0.0185],\n",
            "        [-0.0941,  0.0306, -0.1978, -0.5577,  0.0539, -0.0443, -0.3901,  1.0827,\n",
            "         -0.8696,  0.4439,  0.5666,  0.3521,  0.3688,  0.1132,  0.7450,  0.4552,\n",
            "         -0.7735,  0.2189],\n",
            "        [ 0.0306, -0.1846,  0.8665,  0.8362,  0.0294,  0.8840,  0.2984,  0.3326,\n",
            "         -0.9537,  0.2734,  0.5457,  1.6130,  1.3152, -0.7575, -0.3872,  0.5844,\n",
            "         -1.1101, -0.4477],\n",
            "        [-0.0338, -0.5320, -0.7690,  0.2752,  0.0745,  1.8062,  0.5448,  0.6158,\n",
            "          0.8507, -0.6639,  0.3636,  0.6952,  0.5506,  0.8881,  0.7542,  1.0225,\n",
            "         -0.6135, -0.7784],\n",
            "        [ 0.5634,  1.0538, -0.1249,  1.2757,  0.6288,  0.0336,  0.8824, -0.8190,\n",
            "          2.0629,  0.2456, -0.6159,  0.8338, -0.9267, -0.5509, -0.5095,  0.4379,\n",
            "         -1.0080, -0.4291],\n",
            "        [-0.8566, -0.5290, -0.9710,  0.7653,  0.4554,  0.0371, -0.2781, -0.7336,\n",
            "          1.0250, -2.2013,  0.3907, -1.1989, -1.7571,  1.2797, -1.2042, -1.0494,\n",
            "         -1.2509,  1.9335],\n",
            "        [-0.5274, -1.3231,  0.3786, -1.0142, -2.4491, -1.2651, -1.5625,  0.9172,\n",
            "         -1.4539,  1.6116,  0.0242,  0.5003, -2.3094, -0.1584, -2.0274, -0.6017,\n",
            "          0.1901,  0.9532],\n",
            "        [-0.9316,  0.3123,  0.3326,  0.9455,  0.9139, -1.3811, -0.3136, -0.6399,\n",
            "         -1.8113,  0.2248,  2.2940, -0.3957, -0.9820, -0.6807, -0.5807,  1.1659,\n",
            "         -1.1941, -0.5445],\n",
            "        [ 0.3870, -1.8074,  0.8655,  1.1119,  0.9109, -0.0550,  0.4987,  0.7167,\n",
            "          0.9831,  0.5205, -0.1923, -0.5434,  2.1987,  1.7519, -0.2333,  1.2044,\n",
            "         -0.8023,  0.4995]]) \n",
            " tensor([[0.0246, 0.0399, 0.0469, 0.0196, 0.0119, 0.0120, 0.0403, 0.0177, 0.2769,\n",
            "         0.1491, 0.0905, 0.0247, 0.0937, 0.0273, 0.0150, 0.0080, 0.0633, 0.0386],\n",
            "        [0.0172, 0.0738, 0.0169, 0.1365, 0.0033, 0.0287, 0.0721, 0.1500, 0.2144,\n",
            "         0.0258, 0.0085, 0.0860, 0.0150, 0.0504, 0.0103, 0.0092, 0.0469, 0.0349],\n",
            "        [0.0930, 0.0232, 0.0462, 0.0162, 0.0229, 0.0419, 0.1580, 0.0081, 0.0181,\n",
            "         0.0922, 0.0129, 0.0143, 0.0623, 0.1397, 0.1119, 0.0389, 0.0661, 0.0342],\n",
            "        [0.0178, 0.0886, 0.0670, 0.0694, 0.0221, 0.0210, 0.0554, 0.0998, 0.0276,\n",
            "         0.1927, 0.0155, 0.1643, 0.0131, 0.0051, 0.0348, 0.0418, 0.0258, 0.0383],\n",
            "        [0.0412, 0.0467, 0.0371, 0.0259, 0.0478, 0.0433, 0.0306, 0.1336, 0.0190,\n",
            "         0.0706, 0.0798, 0.0644, 0.0655, 0.0507, 0.0953, 0.0714, 0.0209, 0.0563],\n",
            "        [0.0358, 0.0288, 0.0825, 0.0801, 0.0357, 0.0840, 0.0468, 0.0484, 0.0134,\n",
            "         0.0456, 0.0599, 0.1741, 0.1293, 0.0163, 0.0236, 0.0622, 0.0114, 0.0222],\n",
            "        [0.0318, 0.0193, 0.0152, 0.0433, 0.0354, 0.1999, 0.0566, 0.0608, 0.0769,\n",
            "         0.0169, 0.0472, 0.0658, 0.0570, 0.0798, 0.0698, 0.0913, 0.0178, 0.0151],\n",
            "        [0.0567, 0.0926, 0.0285, 0.1156, 0.0605, 0.0334, 0.0780, 0.0142, 0.2540,\n",
            "         0.0413, 0.0174, 0.0743, 0.0128, 0.0186, 0.0194, 0.0500, 0.0118, 0.0210],\n",
            "        [0.0179, 0.0249, 0.0160, 0.0907, 0.0666, 0.0438, 0.0320, 0.0203, 0.1177,\n",
            "         0.0047, 0.0624, 0.0127, 0.0073, 0.1518, 0.0127, 0.0148, 0.0121, 0.2918],\n",
            "        [0.0309, 0.0139, 0.0764, 0.0190, 0.0045, 0.0148, 0.0110, 0.1309, 0.0122,\n",
            "         0.2622, 0.0536, 0.0863, 0.0052, 0.0447, 0.0069, 0.0287, 0.0633, 0.1357],\n",
            "        [0.0144, 0.0501, 0.0511, 0.0944, 0.0915, 0.0092, 0.0268, 0.0193, 0.0060,\n",
            "         0.0459, 0.3636, 0.0247, 0.0137, 0.0186, 0.0205, 0.1177, 0.0111, 0.0213],\n",
            "        [0.0360, 0.0040, 0.0580, 0.0743, 0.0607, 0.0231, 0.0402, 0.0500, 0.0653,\n",
            "         0.0411, 0.0202, 0.0142, 0.2201, 0.1408, 0.0193, 0.0814, 0.0109, 0.0402]]) \n",
            " tensor([[0.0529, 0.0443, 0.2825, 0.0366, 0.0704, 0.0798, 0.0748, 0.1359, 0.0328,\n",
            "         0.0457, 0.0305, 0.1139],\n",
            "        [0.0764, 0.1689, 0.0626, 0.1622, 0.0709, 0.0572, 0.0404, 0.1973, 0.0405,\n",
            "         0.0183, 0.0940, 0.0113],\n",
            "        [0.0836, 0.0360, 0.1162, 0.1141, 0.0525, 0.1523, 0.0297, 0.0565, 0.0242,\n",
            "         0.0935, 0.0893, 0.1521],\n",
            "        [0.0233, 0.1935, 0.0271, 0.0786, 0.0244, 0.0983, 0.0561, 0.1525, 0.0916,\n",
            "         0.0154, 0.1096, 0.1295],\n",
            "        [0.0244, 0.0082, 0.0662, 0.0431, 0.0775, 0.0756, 0.0791, 0.1378, 0.1158,\n",
            "         0.0063, 0.1832, 0.1827],\n",
            "        [0.0202, 0.0579, 0.0996, 0.0339, 0.0579, 0.1466, 0.3687, 0.0626, 0.0629,\n",
            "         0.0171, 0.0152, 0.0573],\n",
            "        [0.0542, 0.1159, 0.2996, 0.0711, 0.0327, 0.0651, 0.0833, 0.1167, 0.0366,\n",
            "         0.0101, 0.0353, 0.0795],\n",
            "        [0.0239, 0.2417, 0.0154, 0.1285, 0.1429, 0.0675, 0.0896, 0.0213, 0.0232,\n",
            "         0.1211, 0.0255, 0.0991],\n",
            "        [0.2324, 0.2152, 0.0214, 0.0222, 0.0126, 0.0116, 0.0706, 0.2373, 0.0840,\n",
            "         0.0070, 0.0049, 0.0806],\n",
            "        [0.1568, 0.0324, 0.1367, 0.1936, 0.0589, 0.0496, 0.0194, 0.0483, 0.0042,\n",
            "         0.1892, 0.0473, 0.0636],\n",
            "        [0.1123, 0.0127, 0.0227, 0.0183, 0.0786, 0.0769, 0.0641, 0.0241, 0.0659,\n",
            "         0.0457, 0.4420, 0.0368],\n",
            "        [0.0307, 0.1276, 0.0250, 0.1949, 0.0634, 0.2237, 0.0893, 0.1026, 0.0134,\n",
            "         0.0735, 0.0300, 0.0259],\n",
            "        [0.1140, 0.0219, 0.1071, 0.0153, 0.0632, 0.1629, 0.0758, 0.0173, 0.0075,\n",
            "         0.0043, 0.0164, 0.3942],\n",
            "        [0.0325, 0.0717, 0.2348, 0.0058, 0.0479, 0.0201, 0.1040, 0.0247, 0.1538,\n",
            "         0.0365, 0.0217, 0.2466],\n",
            "        [0.0312, 0.0257, 0.3286, 0.0692, 0.1575, 0.0508, 0.1589, 0.0449, 0.0224,\n",
            "         0.0098, 0.0418, 0.0592],\n",
            "        [0.0121, 0.0167, 0.0835, 0.0608, 0.0861, 0.0980, 0.1518, 0.0846, 0.0191,\n",
            "         0.0299, 0.1752, 0.1821],\n",
            "        [0.1671, 0.1480, 0.2460, 0.0650, 0.0437, 0.0312, 0.0513, 0.0346, 0.0271,\n",
            "         0.1146, 0.0287, 0.0425],\n",
            "        [0.0556, 0.0602, 0.0695, 0.0527, 0.0644, 0.0331, 0.0238, 0.0337, 0.3577,\n",
            "         0.1342, 0.0300, 0.0852]])\n",
            "tensor([[0.1354, 0.1080, 0.0990, 0.0711, 0.0495, 0.0635, 0.0644, 0.1091, 0.0605,\n",
            "         0.0597, 0.0714, 0.1085],\n",
            "        [0.0903, 0.1580, 0.0842, 0.0841, 0.0568, 0.0720, 0.0792, 0.1192, 0.0667,\n",
            "         0.0507, 0.0458, 0.0930],\n",
            "        [0.0702, 0.0713, 0.1984, 0.0676, 0.0658, 0.0707, 0.0936, 0.0735, 0.0600,\n",
            "         0.0517, 0.0506, 0.1265],\n",
            "        [0.0744, 0.1052, 0.0999, 0.1256, 0.0688, 0.0966, 0.0708, 0.0879, 0.0436,\n",
            "         0.0823, 0.0633, 0.0815],\n",
            "        [0.0623, 0.0856, 0.1170, 0.0829, 0.0821, 0.0854, 0.0940, 0.0683, 0.0595,\n",
            "         0.0612, 0.0873, 0.1144],\n",
            "        [0.0613, 0.0832, 0.0964, 0.0892, 0.0655, 0.1220, 0.0998, 0.0789, 0.0450,\n",
            "         0.0509, 0.0799, 0.1279],\n",
            "        [0.0589, 0.0866, 0.1208, 0.0619, 0.0682, 0.0945, 0.1462, 0.0828, 0.0587,\n",
            "         0.0369, 0.0685, 0.1159],\n",
            "        [0.0980, 0.1281, 0.0932, 0.0755, 0.0487, 0.0734, 0.0813, 0.1422, 0.0650,\n",
            "         0.0346, 0.0646, 0.0952],\n",
            "        [0.0711, 0.0937, 0.0996, 0.0490, 0.0555, 0.0547, 0.0755, 0.0850, 0.1653,\n",
            "         0.0603, 0.0732, 0.1173],\n",
            "        [0.0870, 0.0883, 0.1062, 0.1145, 0.0707, 0.0768, 0.0587, 0.0562, 0.0747,\n",
            "         0.1119, 0.0669, 0.0881],\n",
            "        [0.0728, 0.0559, 0.0729, 0.0618, 0.0707, 0.0845, 0.0765, 0.0735, 0.0636,\n",
            "         0.0469, 0.2251, 0.0958],\n",
            "        [0.0738, 0.0756, 0.1214, 0.0530, 0.0617, 0.0901, 0.0862, 0.0721, 0.0678,\n",
            "         0.0411, 0.0638, 0.1935]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nnlp = spacy.load('en')    #import spacy，用于分词\\nsent = nlp('Which NFL team represented the AFC at Super Bowl 50')  #对文章提一个问题\\ndoc = open('F:/spacy.txt').read()\\ndoc = nlp(doc)\\n\\ndata = []\\nfor token1 in doc:\\n     data.append([token1.similarity(token2) for token2 in sent])\\n\\nd = np.array(data)\\nd = d.transpose()\\ncol = [t.text for t in doc]     #需要显示的词\\nindex = [t.text for t in sent]  #需要显示的词\\ndf = pd.DataFrame(d, columns=col, index=index )\\n\\nfig = plt.figure()\\n\\nax = fig.add_subplot(111)\\n\\ncax = ax.matshow(df, interpolation='nearest', cmap='hot_r')\\n#cax = ax.matshow(df)\\nfig.colorbar(cax)\\n\\ntick_spacing = 1\\nax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\\nax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\\n\\n# fontdict = {'rotation': 'vertical'}    #设置文字旋转\\nfontdict = {'rotation': 90}       #或者这样设置文字旋转\\n#ax.set_xticklabels([''] + list(df.columns), rotation=90)  #或者直接设置到这里\\n# Axes.set_xticklabels(labels, fontdict=None, minor=False, **kwargs)\\nax.set_xticklabels([''] + list(df.columns), fontdict=fontdict)\\nax.set_yticklabels([''] + list(df.index))\\n\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuATdatpQWDR"
      },
      "source": [
        "# Seq2Seq with Attention 的 PyTorch 实现（机器翻译）\n",
        "\n",
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5uQDC3HT5j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35301b6c-8449-443b-fe03-88dae498d858"
      },
      "source": [
        "# !pip install -U torchtext\n",
        "\n",
        "# 需要用老版本的torchtext，新版本的做了很多修改，本套代码需要修改的较多\n",
        "!pip install torchtext==0.8.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.1\n",
            "  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.62.0)\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1 torchtext-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9YIkuxFQWDS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "#from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH2A4ikIQWDg"
      },
      "source": [
        "Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ON-iFjcQWDg"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMdKZebQWDq"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXSIRB_QWDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d78eb3b-8140-4cde-b575-db7d3f688773"
      },
      "source": [
        "! python -m spacy download de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=3bd0ff0f1c2887853f78dc70f6c4e0a0682da6f44de11a30b0198a42951cb601\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5fkyyzoa/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhgQPe_QWDw"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwcVtz_QWDx"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    # Tokenizes German text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    # Tokenizes English text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgLHkhZvQWD9"
      },
      "source": [
        "The fields remain the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TarEqgDQWD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d999335-0dab-4e19-953a-56303295eeb2"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYolkSt-QWEE"
      },
      "source": [
        "**加载数据。**输入的每个句子开头和结尾都带有特殊的标识符。对于每个 batch 内的句子，将它们的长度通过加 <PAD> 变得一样，即一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEMgrcaQWEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d954a19-c3c5-4c3c-9653-6f323c5f784f"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),fields = (SRC, TRG))\n",
        "# train_data, valid_data, test_data = Multi30k()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 11.3MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.94MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 1.88MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F3AgY3mQWER"
      },
      "source": [
        "**构建词库。**需要将源句子和目标句子分开构建字典，也就是单独对德语构建一个词库，对英语构建一个词库。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anTWknPEQWET"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJ1jpSQQWEa"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_No_GgVGQWEb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nfv01ZQWEf"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUHwbfOkQWEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b527cc-6533-4340-a1da-5100bc8e01fc"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoM5wqabQWEk"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY7wvjYfQWEk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # 获得embedding表示，input_dim是词典的大小尺寸，emb_dim是嵌入向量的维度\n",
        "        # 建立了一个“二维表”，存储了词典中每个词的词向量\n",
        "        # 每个mini-batch的训练，都要从词向量表找到mini-batch对应的单词的词向量作为RNN的输入放进网络。\n",
        "        # nn.embedding的输入只能是编号，不能是隐藏变量，比如one-hot\n",
        "        # embedding_dim的选择要合理，不能过度降维，如词典尺寸是1024，那么嵌入怎么也要15-20维左右（2^10=1024）\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) \n",
        "        \n",
        "        # 双向单层GRU，输入的特征维度是emb_dim，输出的特征维度是enc_hid_dim\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) \n",
        "\n",
        "        # 由于是双向GRU，最后的S0状态（decoder的输入信号）是enc_hid_dim * 2维\n",
        "        # 因此通过一个全联接层转化为dec_hid_dim维才可以作为decoder输入\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src): \n",
        "        # 原始的src = [src_len, batch_size]\n",
        "        # 交换0和1维的数据，src = [batch_size, src_len]\n",
        "        src = src.transpose(0, 1)\n",
        "\n",
        "        # nn.Embedding的输入形状为NxM（N是batch size,M是序列的长度），则输出的形状是N*M*embedding_dimension.\n",
        "        # 输入必须是LongTensor，FloatTensor须通过tensor.long()方法转成LongTensor。\n",
        "        # embedding输出为[batch_size, src_len, emb_dim]，交换前两维后变成[src_len, batch_size, emb_dim]\n",
        "        embedded = self.dropout(self.embedding(src)).transpose(0, 1) # embedded = [src_len, batch_size, emb_dim]\n",
        "        \n",
        "        # CNN和RNN中输入的batchSize的默认位置是不同的：\n",
        "        # CNN中batchsize的位置是position 0\n",
        "        # RNN中batchsize的位置是position 1\n",
        "        # RNN中可以用batch_first参数将输入的形式变为[batch_size, src_len, emb_dim]，此处没有设置\n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # enc_hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
        "        enc_output, enc_hidden = self.rnn(embedded) # if h_0 is not give, it will be set 0 acquiescently\n",
        "\n",
        "        # enc_hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # enc_output are always from the last layer\n",
        "        # enc_hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        # enc_hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        # 需要的是 hidden 的最后一层输出（包括正向和反向）作为S0\n",
        "        # 因此可以通过 hidden[-2,:,:] 和 hidden[-1,:,:] 取出最后一层的 hidden states\n",
        "        \n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        # encoder RNNs fed through a linear layer\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # 通过一个激活函数，增加非线性\n",
        "        s = torch.tanh(self.fc(torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        return enc_output, s"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3gcSxyeQWEu"
      },
      "source": [
        "### Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IJ0v66QWEv"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # 注意力就是通过一个align函数，获得enc_output的每一个输出h_t，与S_i的相关性alpha_i\n",
        "        # 在原论文中，这个align函数的设计，需要两个线性层，即下面的attn和v\n",
        "        # 通过attn和v后得到的相关性并没有进行归一化，最终还要经过Softmax做归一化得到最终的注意力\n",
        "\n",
        "        # 第二个参数dec_hid_dim其实可以是任何值，只是将相关性变成一维数值的中间过渡表示\n",
        "        # 将Encoder的输出enc_output和s拼接，最终嵌入向量的维度是enc_hid_dim*2+dec_hid_dim\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, s, enc_output):\n",
        "        \n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        batch_size = enc_output.shape[1]\n",
        "        src_len = enc_output.shape[0]\n",
        "        \n",
        "        # s的维度无法与enc_output拼接，因此要将s转化为[batch_size, src_len, dec_hid_dim]\n",
        "        # repeat decoder hidden state src_len times\n",
        "        # s = [batch_size, src_len, dec_hid_dim]\n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "        \n",
        "        # 通过激活函数增加非线性（似乎每次通过中间的线性层后，都会加一个激活函数增加非线性）\n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
        "        \n",
        "        # 再通过一个线性层后，得到未归一化的相关性\n",
        "        # attention = [batch_size, src_len]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        # 归一化后得到最终的相关性（注意力）\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4VpZbG8QWEz"
      },
      "source": [
        "### Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvuL5awQWE0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim # 输入维度是output_dim（TRG词表大小尺寸）\n",
        "        self.attention = attention # 注意力是在decoder中计算的，所以要将attention传入\n",
        "\n",
        "        # 针对decoder中输入的y_i，获得其embedding\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim) # 获得embedding表示\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, dec_input, s, enc_output):\n",
        "             \n",
        "        # 获得dec_input的embedding，*** 注意：获得embedding后一般都会做一个dropout，防止过拟合！ ***\n",
        "        # dec_input = [batch_size]，输入的是单个词\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        dec_input = dec_input.unsqueeze(1) # dec_input = [batch_size, 1]\n",
        "        embedded = self.dropout(self.embedding(dec_input)).transpose(0, 1) # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        # 获得s与enc_output之间的归一化相关性（注意力）\n",
        "        # a = [batch_size, 1, src_len]  \n",
        "        a = self.attention(s, enc_output).unsqueeze(1)\n",
        "        \n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "\n",
        "        # 基于注意力（相关性），与enc_output做加权求和，作为当前状态的context（c），后续再基于c、s、y生成下一状态的s\n",
        "        # 实际上考虑到了所有时刻的enc_output（h_i），只不过对于某些时刻可能关注的更多，而某些时刻关注的更少，这就是注意力机制\n",
        "        # torch.bmm 是针对 batch 做相乘（即同一个 batch 内的矩阵相乘），结果是[batch_size, 1, enc_hid_dim * 2]\n",
        "        # c = [1, batch_size, enc_hid_dim * 2]\n",
        "        c = torch.bmm(a, enc_output).transpose(0, 1)\n",
        "\n",
        "        # 由于 GRU 只需要两个变量，所以要将embedded（y）和c整合一下\n",
        "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
        "        rnn_input = torch.cat((embedded, c), dim = 2)\n",
        "            \n",
        "        # dec_output = [src_len(=1), batch_size, dec_hid_dim]，因为当前只预测出一个状态，rnn_input和s都是[1, batch_size, ?]\n",
        "        # dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]\n",
        "        # s = [batch_size, dec_hid_dim]，所以应该先将其拓展一个维度\n",
        "        dec_output, dec_hidden = self.rnn(rnn_input, s.unsqueeze(0))\n",
        "        \n",
        "        # 最后需要将三个变量全部拼接在一起，然后通过一个全连接神经网络，得到最终的预测，所以维度要调整一致\n",
        "        # embedded = [batch_size, emb_dim]\n",
        "        # dec_output = [batch_size, dec_hid_dim]\n",
        "        # c = [batch_size, enc_hid_dim * 2]\n",
        "        embedded = embedded.squeeze(0)\n",
        "        dec_output = dec_output.squeeze(0)\n",
        "        c = c.squeeze(0)\n",
        "        \n",
        "        # 实际上就是为了转换维度，因为需要的输出是 TRG_VOCAB_SIZE 大小\n",
        "        # 也可以理解为最后就是基于embedded（y）、c（注意力下的context）和s（dec_output）分类，目标类别数就是 TRG_VOCAB_SIZE 大小\n",
        "        # pred = [batch_size, output_dim]\n",
        "        # 此处的 pred 给出的是在整个 TRG_VOCAB_SIZE 上的分类结果，存储在 output_dim 中，使用时要再通过 argmax 取出最大的结果\n",
        "        pred = self.fc_out(torch.cat((dec_output, c, embedded), dim = 1))\n",
        "        \n",
        "        # 【疑问】如果是多层或者双向RNN，dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]怎么做squeeze(0)？\n",
        "        return pred, dec_hidden.squeeze(0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDFGyoSCQWE2"
      },
      "source": [
        "### Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wJqu2TQWE4"
      },
      "source": [
        "# 用来利用encoder-decoder模型获得一组输出\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        # src = [src_len, batch_size]\n",
        "        # trg = [trg_len, batch_size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # 低于阈值则用ground truth作为下一个输入，否则用预测结果作为下一个输入\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # enc_output is all hidden states of the input sequence, back and forwards\n",
        "        # s is the final forward and backward hidden states, passed through a linear layer\n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        enc_output, s = self.encoder(src)\n",
        "                \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        dec_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # insert dec_input token embedding, previous hidden state and all encoder hidden states\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            # 此处的s在不断更新，新的attention是在decoder中计算，所以要把enc_output传入\n",
        "            # dec_output = [batch_size, output_dim]\n",
        "            # s = [batch_size, dec_hid_dim]\n",
        "            dec_output, s = self.decoder(dec_input, s, enc_output)\n",
        "            \n",
        "            # 对预测的每个结果做记录，output[t] = [batch_size, trg_vocab_size]，即[batch_size, output_dim]\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            # 这里是一种简化的方法，即随机一定概率使用teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # 通过 argmax 返回指定维度最大值的序号，即预测结果中可能性最大的词在词典中的索引\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1) \n",
        "            \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            # 实际情况时表示可以在这里判断dec_output中最大概率是不是超过某个阈值来决定是否使用teacher forcing\n",
        "            dec_input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z08TAGd2QWE9"
      },
      "source": [
        "## Training the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpZLK3xQWE-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# ignore_index=TRG_PAD_IDX，这个参数的作用是忽略某一类别，不计算其 loss\n",
        "# 因为 PAD 是为了补齐句子长度加入的占位符，没有必要对其进行损失的计算\n",
        "# 要注意，忽略的是真实值中的类别，而不是根据预测值的情况进行忽略\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXQuGEoeQWFd"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5l9s4rTQWFe"
      },
      "source": [
        "'''\n",
        "model.train() 和 model.eval() 的作用是什么：\n",
        "1. 设置了训练或者测试模式，定义模型是否需要学习，对部分层有影响，如 Dropout 和 BatchNormal\n",
        "2. Dropout 在训练时，是启用的，有 p% 的几率丢弃神经元，但在测试过程中，不需要去丢弃神经元\n",
        "3. BatchNormal 在训练过程中，模型每次处理一个 minibatch 数据，会根据一个 minibatch 来计算\n",
        "   mean 和 std 后做归一化处理，这也是为什么模型的性能和minibatch的大小关系很大。\n",
        "   测试时，BatchNormal 会利用训练时得到的参数来处理测试数据\n",
        "4. model. train()和model. eval()可以看做是对这种训练和测试需要联动的模块进行一个统一的设置。\n",
        "   当你在写model的时候，你写的是测试和训练通用的model，这个时候，就是通过model. train()和\n",
        "   model. eval()来来设置model的测试阶段和训练阶段。这样在用需要训练和测试联动的模块的时候，\n",
        "   就不用再专门写一个训练的model和一个测试的model了\n",
        "\n",
        "模型中没有 dropout 和 BatchNormal 时，可以不用设置 model.train() 和 model.eval()。\n",
        "'''\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()    \n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src # src = [src_len, batch_size]\n",
        "        trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "        # pred = [trg_len, batch_size, pred_dim]，其中 pred_dim 就是 trg_vocab_size\n",
        "        pred = model(src, trg)\n",
        "        \n",
        "        pred_dim = pred.shape[-1]\n",
        "        \n",
        "        # trg = [(trg len - 1) * batch size]\n",
        "        # pred = [(trg len - 1) * batch size, pred_dim]\n",
        "        trg = trg[1:].view(-1)\n",
        "        pred = pred[1:].view(-1, pred_dim)\n",
        "        \n",
        "        # trg 相当于就是 label， pred 是 outputs\n",
        "        loss = criterion(pred, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "  \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdYNsPWnQWFg"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7bMdwJQWFh"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # 测试阶段不需要更新梯度\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "            # output = [trg_len, batch_size, output_dim]\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "          \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # trg = [(trg_len - 1) * batch_size]\n",
        "            # output = [(trg_len - 1) * batch_size, output_dim]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVCJPGzMQWFl"
      },
      "source": [
        "Finally, define a timing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEjLOd9kQWFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5O0cPiUQWFx"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjn3fk1mQWFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077a0cf0-6ac0-4dbb-d5e8-2ae1052257cc"
      },
      "source": [
        "# 正无穷\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):\n",
        "    # 训练过程开始计时\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # 训练+验证\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # 训练过程结束计时\n",
        "    end_time = time.time()\n",
        "    # 计算训练用时\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # 存储最佳模型\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 53s\n",
            "\tTrain Loss: 4.347 | Train PPL:  77.210\n",
            "\t Val. Loss: 3.631 |  Val. PPL:  37.768\n",
            "Epoch: 02 | Time: 2m 53s\n",
            "\tTrain Loss: 3.114 | Train PPL:  22.502\n",
            "\t Val. Loss: 3.319 |  Val. PPL:  27.626\n",
            "Epoch: 03 | Time: 2m 53s\n",
            "\tTrain Loss: 2.653 | Train PPL:  14.193\n",
            "\t Val. Loss: 3.316 |  Val. PPL:  27.553\n",
            "Epoch: 04 | Time: 2m 53s\n",
            "\tTrain Loss: 2.355 | Train PPL:  10.535\n",
            "\t Val. Loss: 3.254 |  Val. PPL:  25.892\n",
            "Epoch: 05 | Time: 2m 52s\n",
            "\tTrain Loss: 2.136 | Train PPL:   8.465\n",
            "\t Val. Loss: 3.278 |  Val. PPL:  26.531\n",
            "Epoch: 06 | Time: 2m 54s\n",
            "\tTrain Loss: 1.994 | Train PPL:   7.348\n",
            "\t Val. Loss: 3.300 |  Val. PPL:  27.123\n",
            "Epoch: 07 | Time: 2m 52s\n",
            "\tTrain Loss: 1.884 | Train PPL:   6.581\n",
            "\t Val. Loss: 3.306 |  Val. PPL:  27.288\n",
            "Epoch: 08 | Time: 2m 52s\n",
            "\tTrain Loss: 1.777 | Train PPL:   5.915\n",
            "\t Val. Loss: 3.295 |  Val. PPL:  26.985\n",
            "Epoch: 09 | Time: 2m 53s\n",
            "\tTrain Loss: 1.694 | Train PPL:   5.442\n",
            "\t Val. Loss: 3.384 |  Val. PPL:  29.501\n",
            "Epoch: 10 | Time: 2m 52s\n",
            "\tTrain Loss: 1.651 | Train PPL:   5.213\n",
            "\t Val. Loss: 3.441 |  Val. PPL:  31.232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NoPuoZfQWF2"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Yp7lmHQWF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c1ea55-578e-4e85-f5c9-ef6ef5f99855"
      },
      "source": [
        "# 加载最佳模型\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\n",
        "\n",
        "# 评估在测试集上的性能\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.339 | Test PPL:  28.184 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6WwESuz2FZW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Show Attention\n",
        "# ...............\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJQvH_DDq0y"
      },
      "source": [
        "# **一个简化的例子**（但没太看懂……）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8sWPl2DVWA"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "# [seq_len, batch_size]，一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        # enc_outputs 指的是所有时刻的最后一层隐藏状态, enc_hidden 指的是最后一个时刻所有隐藏层的状态\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    # 实际操作中，在数据预处理的时候，需要将源句子和目标句子分开构建字典\n",
        "    # 也就是单独对源语言构建一个词库，对目标语言构建一个词库\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
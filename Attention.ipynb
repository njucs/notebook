{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6/crAGe40DyzW4fLyKQX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQaxCCsDwH6"
      },
      "source": [
        "### **Attention**\n",
        "参考：\\\n",
        "https://wmathor.com/index.php/archives/1450/ \\\n",
        "https://wmathor.com/index.php/archives/1451/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5r4Ec6D3vP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuATdatpQWDR"
      },
      "source": [
        "# Seq2Seq with Attention 的 PyTorch 实现（机器翻译）\n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "Again, the preparation is similar to last time.\n",
        "\n",
        "First we import all the required modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9YIkuxFQWDS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH2A4ikIQWDg"
      },
      "source": [
        "Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ON-iFjcQWDg"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMdKZebQWDq"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXSIRB_QWDr"
      },
      "source": [
        "! python -m spacy download de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhgQPe_QWDw"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwcVtz_QWDx"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    # Tokenizes German text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    # Tokenizes English text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgLHkhZvQWD9"
      },
      "source": [
        "The fields remain the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TarEqgDQWD-"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYolkSt-QWEE"
      },
      "source": [
        "**加载数据。**输入的每个句子开头和结尾都带有特殊的标识符。对于每个 batch 内的句子，将它们的长度通过加 <PAD> 变得一样，即一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEMgrcaQWEF"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F3AgY3mQWER"
      },
      "source": [
        "**构建词库。**需要将源句子和目标句子分开构建字典，也就是单独对德语构建一个词库，对英语构建一个词库。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anTWknPEQWET"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJ1jpSQQWEa"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_No_GgVGQWEb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nfv01ZQWEf"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUHwbfOkQWEg"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoM5wqabQWEk"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY7wvjYfQWEk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src): \n",
        "        '''\n",
        "        src = [src_len, batch_size]\n",
        "        '''\n",
        "        src = src.transpose(0, 1) # src = [batch_size, src_len]\n",
        "        embedded = self.dropout(self.embedding(src)).transpose(0, 1) # embedded = [src_len, batch_size, emb_dim]\n",
        "        \n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # enc_hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
        "        enc_output, enc_hidden = self.rnn(embedded) # if h_0 is not give, it will be set 0 acquiescently\n",
        "\n",
        "        # enc_hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # enc_output are always from the last layer\n",
        "        \n",
        "        # enc_hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        # enc_hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        # encoder RNNs fed through a linear layer\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        s = torch.tanh(self.fc(torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        return enc_output, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3gcSxyeQWEu"
      },
      "source": [
        "### Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IJ0v66QWEv"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, s, enc_output):\n",
        "        \n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        batch_size = enc_output.shape[1]\n",
        "        src_len = enc_output.shape[0]\n",
        "        \n",
        "        # repeat decoder hidden state src_len times\n",
        "        # s = [batch_size, src_len, dec_hid_dim]\n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "        \n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
        "        \n",
        "        # attention = [batch_size, src_len]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4VpZbG8QWEz"
      },
      "source": [
        "### Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvuL5awQWE0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, dec_input, s, enc_output):\n",
        "             \n",
        "        # dec_input = [batch_size]\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        dec_input = dec_input.unsqueeze(1) # dec_input = [batch_size, 1]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(dec_input)).transpose(0, 1) # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        # a = [batch_size, 1, src_len]  \n",
        "        a = self.attention(s, enc_output).unsqueeze(1)\n",
        "        \n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "\n",
        "        # c = [1, batch_size, enc_hid_dim * 2]\n",
        "        c = torch.bmm(a, enc_output).transpose(0, 1)\n",
        "\n",
        "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
        "        rnn_input = torch.cat((embedded, c), dim = 2)\n",
        "            \n",
        "        # dec_output = [src_len(=1), batch_size, dec_hid_dim]\n",
        "        # dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]\n",
        "        dec_output, dec_hidden = self.rnn(rnn_input, s.unsqueeze(0))\n",
        "        \n",
        "        # embedded = [batch_size, emb_dim]\n",
        "        # dec_output = [batch_size, dec_hid_dim]\n",
        "        # c = [batch_size, enc_hid_dim * 2]\n",
        "        embedded = embedded.squeeze(0)\n",
        "        dec_output = dec_output.squeeze(0)\n",
        "        c = c.squeeze(0)\n",
        "        \n",
        "        # pred = [batch_size, output_dim]\n",
        "        pred = self.fc_out(torch.cat((dec_output, c, embedded), dim = 1))\n",
        "        \n",
        "        return pred, dec_hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDFGyoSCQWE2"
      },
      "source": [
        "### Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wJqu2TQWE4"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        # src = [src_len, batch_size]\n",
        "        # trg = [trg_len, batch_size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # enc_output is all hidden states of the input sequence, back and forwards\n",
        "        # s is the final forward and backward hidden states, passed through a linear layer\n",
        "        enc_output, s = self.encoder(src)\n",
        "                \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        dec_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # insert dec_input token embedding, previous hidden state and all encoder hidden states\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            dec_output, s = self.decoder(dec_input, s, enc_output)\n",
        "            \n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1) \n",
        "            \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            dec_input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z08TAGd2QWE9"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n",
        "The rest of this tutorial is very similar to the previous one.\n",
        "\n",
        "We initialise our parameters, encoder, decoder and seq2seq model (placing it on the GPU if we have one). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpZLK3xQWE-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXQuGEoeQWFd"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5l9s4rTQWFe"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()    \n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "        # pred = [trg_len, batch_size, pred_dim]\n",
        "        pred = model(src, trg)\n",
        "        \n",
        "        pred_dim = pred.shape[-1]\n",
        "        \n",
        "        # trg = [(trg len - 1) * batch size]\n",
        "        # pred = [(trg len - 1) * batch size, pred_dim]\n",
        "        trg = trg[1:].view(-1)\n",
        "        pred = pred[1:].view(-1, pred_dim)\n",
        "        \n",
        "        loss = criterion(pred, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "  \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdYNsPWnQWFg"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7bMdwJQWFh"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "            # output = [trg_len, batch_size, output_dim]\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "          \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # trg = [(trg_len - 1) * batch_size]\n",
        "            # output = [(trg_len - 1) * batch_size, output_dim]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVCJPGzMQWFl"
      },
      "source": [
        "Finally, define a timing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEjLOd9kQWFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5O0cPiUQWFx"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjn3fk1mQWFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "71b3f280-290b-4697-d820-0a77875ceeef"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 40s\n",
            "\tTrain Loss: 4.346 | Train PPL:  77.152\n",
            "\t Val. Loss: 3.636 |  Val. PPL:  37.924\n",
            "Epoch: 02 | Time: 2m 41s\n",
            "\tTrain Loss: 3.114 | Train PPL:  22.511\n",
            "\t Val. Loss: 3.281 |  Val. PPL:  26.606\n",
            "Epoch: 03 | Time: 2m 40s\n",
            "\tTrain Loss: 2.653 | Train PPL:  14.192\n",
            "\t Val. Loss: 3.304 |  Val. PPL:  27.221\n",
            "Epoch: 04 | Time: 2m 41s\n",
            "\tTrain Loss: 2.355 | Train PPL:  10.536\n",
            "\t Val. Loss: 3.269 |  Val. PPL:  26.282\n",
            "Epoch: 05 | Time: 2m 40s\n",
            "\tTrain Loss: 2.136 | Train PPL:   8.464\n",
            "\t Val. Loss: 3.279 |  Val. PPL:  26.548\n",
            "Epoch: 06 | Time: 2m 42s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.350\n",
            "\t Val. Loss: 3.292 |  Val. PPL:  26.895\n",
            "Epoch: 07 | Time: 2m 40s\n",
            "\tTrain Loss: 1.881 | Train PPL:   6.563\n",
            "\t Val. Loss: 3.261 |  Val. PPL:  26.070\n",
            "Epoch: 08 | Time: 2m 40s\n",
            "\tTrain Loss: 1.780 | Train PPL:   5.928\n",
            "\t Val. Loss: 3.332 |  Val. PPL:  27.984\n",
            "Epoch: 09 | Time: 2m 41s\n",
            "\tTrain Loss: 1.693 | Train PPL:   5.434\n",
            "\t Val. Loss: 3.360 |  Val. PPL:  28.796\n",
            "Epoch: 10 | Time: 2m 40s\n",
            "\tTrain Loss: 1.656 | Train PPL:   5.236\n",
            "\t Val. Loss: 3.441 |  Val. PPL:  31.209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NoPuoZfQWF2"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Yp7lmHQWF3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "078de211-00b0-4520-b0d0-6cde46faf819"
      },
      "source": [
        "model.load_state_dict(torch.load('tut3-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.337 | Test PPL:  28.140 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJQvH_DDq0y"
      },
      "source": [
        "# **一个简化的例子**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "6I8sWPl2DVWA",
        "outputId": "f3484fc0-f9fb-4f78-8a96-e46e306c9b7f"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "# [seq_len, batch_size]，一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    # 实际操作中，在数据预处理的时候，需要将源句子和目标句子分开构建字典\n",
        "    # 也就是单独对源语言构建一个词库，对目标语言构建一个词库\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0400 cost = 0.000522\n",
            "Epoch: 0800 cost = 0.000170\n",
            "Epoch: 1200 cost = 0.000084\n",
            "Epoch: 1600 cost = 0.000050\n",
            "Epoch: 2000 cost = 0.000033\n",
            "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
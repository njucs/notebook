{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkHxCtXkjII0ycR7Gdn4fi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQaxCCsDwH6"
      },
      "source": [
        "### **Attention**\n",
        "参考：\\\n",
        "https://wmathor.com/index.php/archives/1450/ \\\n",
        "https://wmathor.com/index.php/archives/1451/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5r4Ec6D3vP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuATdatpQWDR"
      },
      "source": [
        "# Seq2Seq with Attention 的 PyTorch 实现（机器翻译）\n",
        "\n",
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5uQDC3HT5j7"
      },
      "source": [
        "# !pip install -U torchtext\n",
        "\n",
        "# 需要用老版本的torchtext，新版本的做了很多修改，本套代码需要修改的较多\n",
        "!pip install torchtext==0.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9YIkuxFQWDS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "#from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH2A4ikIQWDg"
      },
      "source": [
        "Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ON-iFjcQWDg"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMdKZebQWDq"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXSIRB_QWDr"
      },
      "source": [
        "! python -m spacy download de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhgQPe_QWDw"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwcVtz_QWDx"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    # Tokenizes German text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    # Tokenizes English text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgLHkhZvQWD9"
      },
      "source": [
        "The fields remain the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TarEqgDQWD-"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYolkSt-QWEE"
      },
      "source": [
        "**加载数据。**输入的每个句子开头和结尾都带有特殊的标识符。对于每个 batch 内的句子，将它们的长度通过加 <PAD> 变得一样，即一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEMgrcaQWEF"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),fields = (SRC, TRG))\n",
        "# train_data, valid_data, test_data = Multi30k()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F3AgY3mQWER"
      },
      "source": [
        "**构建词库。**需要将源句子和目标句子分开构建字典，也就是单独对德语构建一个词库，对英语构建一个词库。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anTWknPEQWET"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJ1jpSQQWEa"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_No_GgVGQWEb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nfv01ZQWEf"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUHwbfOkQWEg"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoM5wqabQWEk"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY7wvjYfQWEk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # 获得embedding表示，input_dim是词典的大小尺寸，emb_dim是嵌入向量的维度\n",
        "        # 建立了一个“二维表”，存储了词典中每个词的词向量\n",
        "        # 每个mini-batch的训练，都要从词向量表找到mini-batch对应的单词的词向量作为RNN的输入放进网络。\n",
        "        # nn.embedding的输入只能是编号，不能是隐藏变量，比如one-hot\n",
        "        # embedding_dim的选择要合理，不能过度降维，如词典尺寸是1024，那么嵌入怎么也要15-20维左右（2^10=1024）\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) \n",
        "        \n",
        "        # 双向单层GRU，输入的特征维度是emb_dim，输出的特征维度是enc_hid_dim\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) \n",
        "\n",
        "        # 由于是双向GRU，最后的S0状态（decoder的输入信号）是enc_hid_dim * 2维\n",
        "        # 因此通过一个全联接层转化为dec_hid_dim维才可以作为decoder输入\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src): \n",
        "        # 原始的src = [src_len, batch_size]\n",
        "        # 交换0和1维的数据，src = [batch_size, src_len]\n",
        "        src = src.transpose(0, 1)\n",
        "\n",
        "        # nn.Embedding的输入形状为NxM（N是batch size,M是序列的长度），则输出的形状是N*M*embedding_dimension.\n",
        "        # 输入必须是LongTensor，FloatTensor须通过tensor.long()方法转成LongTensor。\n",
        "        # embedding输出为[batch_size, src_len, emb_dim]，交换前两维后变成[src_len, batch_size, emb_dim]\n",
        "        embedded = self.dropout(self.embedding(src)).transpose(0, 1) # embedded = [src_len, batch_size, emb_dim]\n",
        "        \n",
        "        # CNN和RNN中输入的batchSize的默认位置是不同的：\n",
        "        # CNN中batchsize的位置是position 0\n",
        "        # RNN中batchsize的位置是position 1\n",
        "        # RNN中可以用batch_first参数将输入的形式变为[batch_size, src_len, emb_dim]，此处没有设置\n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # enc_hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
        "        enc_output, enc_hidden = self.rnn(embedded) # if h_0 is not give, it will be set 0 acquiescently\n",
        "\n",
        "        # enc_hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # enc_output are always from the last layer\n",
        "        # enc_hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        # enc_hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        # 需要的是 hidden 的最后一层输出（包括正向和反向）作为S0\n",
        "        # 因此可以通过 hidden[-2,:,:] 和 hidden[-1,:,:] 取出最后一层的 hidden states\n",
        "        \n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        # encoder RNNs fed through a linear layer\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # 通过一个激活函数，增加非线性\n",
        "        s = torch.tanh(self.fc(torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        return enc_output, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3gcSxyeQWEu"
      },
      "source": [
        "### Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IJ0v66QWEv"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # 注意力就是通过一个align函数，获得enc_output的每一个输出h_t，与S_i的相关性alpha_i\n",
        "        # 在原论文中，这个align函数的设计，需要两个线性层，即下面的attn和v\n",
        "        # 通过attn和v后得到的相关性并没有进行归一化，最终还要经过Softmax做归一化得到最终的注意力\n",
        "\n",
        "        # 第二个参数dec_hid_dim其实可以是任何值，只是将相关性变成一维数值的中间过渡表示\n",
        "        # 将Encoder的输出enc_output和s拼接，最终嵌入向量的维度是enc_hid_dim*2+dec_hid_dim\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, s, enc_output):\n",
        "        \n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        batch_size = enc_output.shape[1]\n",
        "        src_len = enc_output.shape[0]\n",
        "        \n",
        "        # s的维度无法与enc_output拼接，因此要将s转化为[batch_size, src_len, dec_hid_dim]\n",
        "        # repeat decoder hidden state src_len times\n",
        "        # s = [batch_size, src_len, dec_hid_dim]\n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "        \n",
        "        # 通过激活函数增加非线性（似乎每次通过中间的线性层后，都会加一个激活函数增加非线性）\n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
        "        \n",
        "        # 再通过一个线性层后，得到未归一化的相关性\n",
        "        # attention = [batch_size, src_len]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        # 归一化后得到最终的相关性（注意力）\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4VpZbG8QWEz"
      },
      "source": [
        "### Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvuL5awQWE0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim # 输入维度是output_dim（TRG词表大小尺寸）\n",
        "        self.attention = attention # 注意力是在decoder中计算的，所以要将attention传入\n",
        "\n",
        "        # 针对decoder中输入的y_i，获得其embedding\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim) # 获得embedding表示\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, dec_input, s, enc_output):\n",
        "             \n",
        "        # 获得dec_input的embedding，*** 注意：获得embedding后一般都会做一个dropout，防止过拟合！ ***\n",
        "        # dec_input = [batch_size]，输入的是单个词\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        dec_input = dec_input.unsqueeze(1) # dec_input = [batch_size, 1]\n",
        "        embedded = self.dropout(self.embedding(dec_input)).transpose(0, 1) # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        # 获得s与enc_output之间的归一化相关性（注意力）\n",
        "        # a = [batch_size, 1, src_len]  \n",
        "        a = self.attention(s, enc_output).unsqueeze(1)\n",
        "        \n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "\n",
        "        # 基于注意力（相关性），与enc_output做加权求和，作为当前状态的context（c），后续再基于c、s、y生成下一状态的s\n",
        "        # 实际上考虑到了所有时刻的enc_output（h_i），只不过对于某些时刻可能关注的更多，而某些时刻关注的更少，这就是注意力机制\n",
        "        # torch.bmm 是针对 batch 做相乘（即同一个 batch 内的矩阵相乘），结果是[batch_size, 1, enc_hid_dim * 2]\n",
        "        # c = [1, batch_size, enc_hid_dim * 2]\n",
        "        c = torch.bmm(a, enc_output).transpose(0, 1)\n",
        "\n",
        "        # 由于 GRU 只需要两个变量，所以要将embedded（y）和c整合一下\n",
        "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
        "        rnn_input = torch.cat((embedded, c), dim = 2)\n",
        "            \n",
        "        # dec_output = [src_len(=1), batch_size, dec_hid_dim]，因为当前只预测出一个状态，rnn_input和s都是[1, batch_size, ?]\n",
        "        # dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]\n",
        "        # s = [batch_size, dec_hid_dim]，所以应该先将其拓展一个维度\n",
        "        dec_output, dec_hidden = self.rnn(rnn_input, s.unsqueeze(0))\n",
        "        \n",
        "        # 最后需要将三个变量全部拼接在一起，然后通过一个全连接神经网络，得到最终的预测，所以维度要调整一致\n",
        "        # embedded = [batch_size, emb_dim]\n",
        "        # dec_output = [batch_size, dec_hid_dim]\n",
        "        # c = [batch_size, enc_hid_dim * 2]\n",
        "        embedded = embedded.squeeze(0)\n",
        "        dec_output = dec_output.squeeze(0)\n",
        "        c = c.squeeze(0)\n",
        "        \n",
        "        # 实际上就是为了转换维度，因为需要的输出是 TRG_VOCAB_SIZE 大小\n",
        "        # 也可以理解为最后就是基于embedded（y）、c（注意力下的context）和s（dec_output）分类，目标类别数就是 TRG_VOCAB_SIZE 大小\n",
        "        # pred = [batch_size, output_dim]\n",
        "        # 此处的 pred 给出的是在整个 TRG_VOCAB_SIZE 上的分类结果，存储在 output_dim 中，使用时要再通过 argmax 取出最大的结果\n",
        "        pred = self.fc_out(torch.cat((dec_output, c, embedded), dim = 1))\n",
        "        \n",
        "        # 【疑问】如果是多层或者双向RNN，dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]怎么做squeeze(0)？\n",
        "        return pred, dec_hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDFGyoSCQWE2"
      },
      "source": [
        "### Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wJqu2TQWE4"
      },
      "source": [
        "# 用来利用encoder-decoder模型获得一组输出\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        # src = [src_len, batch_size]\n",
        "        # trg = [trg_len, batch_size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # 低于阈值则用ground truth作为下一个输入，否则用预测结果作为下一个输入\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # enc_output is all hidden states of the input sequence, back and forwards\n",
        "        # s is the final forward and backward hidden states, passed through a linear layer\n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        enc_output, s = self.encoder(src)\n",
        "                \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        dec_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # insert dec_input token embedding, previous hidden state and all encoder hidden states\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            # 此处的s在不断更新，新的attention是在decoder中计算，所以要把enc_output传入\n",
        "            # dec_output = [batch_size, output_dim]\n",
        "            # s = [batch_size, dec_hid_dim]\n",
        "            dec_output, s = self.decoder(dec_input, s, enc_output)\n",
        "            \n",
        "            # 对预测的每个结果做记录，output[t] = [batch_size, trg_vocab_size]，即[batch_size, output_dim]\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            # 这里是一种简化的方法，即随机一定概率使用teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # 通过 argmax 返回指定维度最大值的序号，即预测结果中可能性最大的词在词典中的索引\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1) \n",
        "            \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            # 实际情况时表示可以在这里判断dec_output中最大概率是不是超过某个阈值来决定是否使用teacher forcing\n",
        "            dec_input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z08TAGd2QWE9"
      },
      "source": [
        "## Training the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpZLK3xQWE-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# ignore_index=TRG_PAD_IDX，这个参数的作用是忽略某一类别，不计算其 loss\n",
        "# 因为 PAD 是为了补齐句子长度加入的占位符，没有必要对其进行损失的计算\n",
        "# 要注意，忽略的是真实值中的类别，而不是根据预测值的情况进行忽略\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXQuGEoeQWFd"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5l9s4rTQWFe"
      },
      "source": [
        "'''\n",
        "model.train() 和 model.eval() 的作用是什么：\n",
        "1. 设置了训练或者测试模式，定义模型是否需要学习，对部分层有影响，如 Dropout 和 BatchNormal\n",
        "2. Dropout 在训练时，是启用的，有 p% 的几率丢弃神经元，但在测试过程中，不需要去丢弃神经元\n",
        "3. BatchNormal 在训练过程中，模型每次处理一个 minibatch 数据，会根据一个 minibatch 来计算\n",
        "   mean 和 std 后做归一化处理，这也是为什么模型的性能和minibatch的大小关系很大。\n",
        "   测试时，BatchNormal 会利用训练时得到的参数来处理测试数据\n",
        "4. model. train()和model. eval()可以看做是对这种训练和测试需要联动的模块进行一个统一的设置。\n",
        "   当你在写model的时候，你写的是测试和训练通用的model，这个时候，就是通过model. train()和\n",
        "   model. eval()来来设置model的测试阶段和训练阶段。这样在用需要训练和测试联动的模块的时候，\n",
        "   就不用再专门写一个训练的model和一个测试的model了\n",
        "\n",
        "模型中没有 dropout 和 BatchNormal 时，可以不用设置 model.train() 和 model.eval()。\n",
        "'''\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()    \n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src # src = [src_len, batch_size]\n",
        "        trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "        # pred = [trg_len, batch_size, pred_dim]，其中 pred_dim 就是 trg_vocab_size\n",
        "        pred = model(src, trg)\n",
        "        \n",
        "        pred_dim = pred.shape[-1]\n",
        "        \n",
        "        # trg = [(trg len - 1) * batch size]\n",
        "        # pred = [(trg len - 1) * batch size, pred_dim]\n",
        "        trg = trg[1:].view(-1)\n",
        "        pred = pred[1:].view(-1, pred_dim)\n",
        "        \n",
        "        # trg 相当于就是 label， pred 是 outputs\n",
        "        loss = criterion(pred, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "  \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdYNsPWnQWFg"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7bMdwJQWFh"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # 测试阶段不需要更新梯度\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "            # output = [trg_len, batch_size, output_dim]\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "          \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # trg = [(trg_len - 1) * batch_size]\n",
        "            # output = [(trg_len - 1) * batch_size, output_dim]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVCJPGzMQWFl"
      },
      "source": [
        "Finally, define a timing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEjLOd9kQWFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5O0cPiUQWFx"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjn3fk1mQWFx"
      },
      "source": [
        "# 正无穷\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):\n",
        "    # 训练过程开始计时\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # 训练+验证\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # 训练过程结束计时\n",
        "    end_time = time.time()\n",
        "    # 计算训练用时\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # 存储最佳模型\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NoPuoZfQWF2"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Yp7lmHQWF3"
      },
      "source": [
        "# 加载最佳模型\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\n",
        "\n",
        "# 评估在测试集上的性能\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJQvH_DDq0y"
      },
      "source": [
        "# **一个简化的例子**（但没太看懂……）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8sWPl2DVWA"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "# [seq_len, batch_size]，一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        # enc_outputs 指的是所有时刻的最后一层隐藏状态, enc_hidden 指的是最后一个时刻所有隐藏层的状态\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    # 实际操作中，在数据预处理的时候，需要将源句子和目标句子分开构建字典\n",
        "    # 也就是单独对源语言构建一个词库，对目标语言构建一个词库\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
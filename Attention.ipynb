{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMQLyA2uZAwjuQ2i00xVxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQaxCCsDwH6"
      },
      "source": [
        "### **Attention**\n",
        "参考：\\\n",
        "https://wmathor.com/index.php/archives/1450/ \\\n",
        "https://wmathor.com/index.php/archives/1451/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5r4Ec6D3vP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuATdatpQWDR"
      },
      "source": [
        "# Seq2Seq with Attention 的 PyTorch 实现（机器翻译）\n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "Again, the preparation is similar to last time.\n",
        "\n",
        "First we import all the required modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9YIkuxFQWDS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH2A4ikIQWDg"
      },
      "source": [
        "Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ON-iFjcQWDg"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMdKZebQWDq"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXSIRB_QWDr"
      },
      "source": [
        "! python -m spacy download de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhgQPe_QWDw"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwcVtz_QWDx"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    # Tokenizes German text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    # Tokenizes English text from a string into a list of strings\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgLHkhZvQWD9"
      },
      "source": [
        "The fields remain the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TarEqgDQWD-"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYolkSt-QWEE"
      },
      "source": [
        "**加载数据。**输入的每个句子开头和结尾都带有特殊的标识符。对于每个 batch 内的句子，将它们的长度通过加 <PAD> 变得一样，即一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NEMgrcaQWEF"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F3AgY3mQWER"
      },
      "source": [
        "**构建词库。**需要将源句子和目标句子分开构建字典，也就是单独对德语构建一个词库，对英语构建一个词库。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anTWknPEQWET"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJ1jpSQQWEa"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_No_GgVGQWEb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nfv01ZQWEf"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUHwbfOkQWEg"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoM5wqabQWEk"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY7wvjYfQWEk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # 获得embedding表示，input_dim是词典的大小尺寸，emb_dim是嵌入向量的维度\n",
        "        # 建立了一个“二维表”，存储了词典中每个词的词向量\n",
        "        # 每个mini-batch的训练，都要从词向量表找到mini-batch对应的单词的词向量作为RNN的输入放进网络。\n",
        "        # nn.embedding的输入只能是编号，不能是隐藏变量，比如one-hot\n",
        "        # embedding_dim的选择要合理，不能过度降维，如词典尺寸是1024，那么嵌入怎么也要15-20维左右（2^10=1024）\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) \n",
        "        \n",
        "        # 双向单层GRU，输入的特征维度是emb_dim，输出的特征维度是enc_hid_dim\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) \n",
        "\n",
        "        # 由于是双向GRU，最后的S0状态（decoder的输入信号）是enc_hid_dim * 2维\n",
        "        # 因此通过一个全联接层转化为dec_hid_dim维才可以作为decoder输入\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src): \n",
        "        # 原始的src = [src_len, batch_size]\n",
        "        # 交换0和1维的数据，src = [batch_size, src_len]\n",
        "        src = src.transpose(0, 1)\n",
        "\n",
        "        # nn.Embedding的输入形状为NxM（N是batch size,M是序列的长度），则输出的形状是N*M*embedding_dimension.\n",
        "        # 输入必须是LongTensor，FloatTensor须通过tensor.long()方法转成LongTensor。\n",
        "        # embedding输出为[batch_size, src_len, emb_dim]，交换前两维后变成[src_len, batch_size, emb_dim]\n",
        "        embedded = self.dropout(self.embedding(src)).transpose(0, 1) # embedded = [src_len, batch_size, emb_dim]\n",
        "        \n",
        "        # CNN和RNN中输入的batchSize的默认位置是不同的：\n",
        "        # CNN中batchsize的位置是position 0\n",
        "        # RNN中batchsize的位置是position 1\n",
        "        # RNN中可以用batch_first参数将输入的形式变为[batch_size, src_len, emb_dim]，此处没有设置\n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # enc_hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
        "        enc_output, enc_hidden = self.rnn(embedded) # if h_0 is not give, it will be set 0 acquiescently\n",
        "\n",
        "        # enc_hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # enc_output are always from the last layer\n",
        "        # enc_hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        # enc_hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        # 需要的是 hidden 的最后一层输出（包括正向和反向）作为S0\n",
        "        # 因此可以通过 hidden[-2,:,:] 和 hidden[-1,:,:] 取出最后一层的 hidden states\n",
        "        \n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        # encoder RNNs fed through a linear layer\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # 通过一个激活函数，增加非线性\n",
        "        s = torch.tanh(self.fc(torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        return enc_output, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3gcSxyeQWEu"
      },
      "source": [
        "### Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IJ0v66QWEv"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # 注意力就是通过一个align函数，获得enc_output的每一个输出h_t，与S_i的相关性alpha_i\n",
        "        # 在原论文中，这个align函数的设计，需要两个线性层，即下面的attn和v\n",
        "        # 通过attn和v后得到的相关性并没有进行归一化，最终还要经过Softmax做归一化得到最终的注意力\n",
        "\n",
        "        # 第二个参数dec_hid_dim其实可以是任何值，只是将相关性变成一维数值的中间过渡表示\n",
        "        # 将Encoder的输出enc_output和s拼接，最终嵌入向量的维度是enc_hid_dim*2+dec_hid_dim\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, s, enc_output):\n",
        "        \n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        batch_size = enc_output.shape[1]\n",
        "        src_len = enc_output.shape[0]\n",
        "        \n",
        "        # s的维度无法与enc_output拼接，因此要将s转化为[batch_size, src_len, dec_hid_dim]\n",
        "        # repeat decoder hidden state src_len times\n",
        "        # s = [batch_size, src_len, dec_hid_dim]\n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "        \n",
        "        # 通过激活函数增加非线性（似乎每次通过中间的线性层后，都会加一个激活函数增加非线性）\n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
        "        \n",
        "        # 再通过一个线性层后，得到未归一化的相关性\n",
        "        # attention = [batch_size, src_len]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        # 归一化后得到最终的相关性（注意力）\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4VpZbG8QWEz"
      },
      "source": [
        "### Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvuL5awQWE0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim # 输入维度是output_dim（TRG词表大小尺寸）\n",
        "        self.attention = attention\n",
        "\n",
        "        # 针对decoder中输入的y_i，获得其embedding\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim) # 获得embedding表示\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, dec_input, s, enc_output):\n",
        "             \n",
        "        # 获得dec_input的embedding，*** 注意：获得embedding后一般都会做一个dropout，防止过拟合！ ***\n",
        "        # dec_input = [batch_size]，输入的是单个词\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        dec_input = dec_input.unsqueeze(1) # dec_input = [batch_size, 1]\n",
        "        embedded = self.dropout(self.embedding(dec_input)).transpose(0, 1) # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        # 获得s与enc_output之间的归一化相关性（注意力）\n",
        "        # a = [batch_size, 1, src_len]  \n",
        "        a = self.attention(s, enc_output).unsqueeze(1)\n",
        "        \n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "\n",
        "        # 基于注意力（相关性），与enc_output做加权求和，作为当前状态的context（c），后续再基于c、s、y生成下一状态的s\n",
        "        # 实际上考虑到了所有时刻的enc_output（h_i），只不过对于某些时刻可能关注的更多，而某些时刻关注的更少，这就是注意力机制\n",
        "        # torch.bmm 是针对 batch 做相乘（即同一个 batch 内的矩阵相乘），结果是[batch_size, 1, enc_hid_dim * 2]\n",
        "        # c = [1, batch_size, enc_hid_dim * 2]\n",
        "        c = torch.bmm(a, enc_output).transpose(0, 1)\n",
        "\n",
        "        # 由于 GRU 只需要两个变量，所以要将embedded（y）和c整合一下\n",
        "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
        "        rnn_input = torch.cat((embedded, c), dim = 2)\n",
        "            \n",
        "        # dec_output = [src_len(=1), batch_size, dec_hid_dim]，因为当前只预测出一个状态，rnn_input和s都是[1, batch_size, ?]\n",
        "        # dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]\n",
        "        # s = [batch_size, dec_hid_dim]，所以应该先将其拓展一个维度\n",
        "        dec_output, dec_hidden = self.rnn(rnn_input, s.unsqueeze(0))\n",
        "        \n",
        "        # 最后需要将三个变量全部拼接在一起，然后通过一个全连接神经网络，得到最终的预测，所以维度要调整一致\n",
        "        # embedded = [batch_size, emb_dim]\n",
        "        # dec_output = [batch_size, dec_hid_dim]\n",
        "        # c = [batch_size, enc_hid_dim * 2]\n",
        "        embedded = embedded.squeeze(0)\n",
        "        dec_output = dec_output.squeeze(0)\n",
        "        c = c.squeeze(0)\n",
        "        \n",
        "        # 实际上就是为了转换维度，因为需要的输出是 TRG_VOCAB_SIZE 大小\n",
        "        # 也可以理解为最后就是基于embedded（y）、c（注意力下的context）和s（dec_output）分类，目标类别数就是 TRG_VOCAB_SIZE 大小\n",
        "        # pred = [batch_size, output_dim]\n",
        "        pred = self.fc_out(torch.cat((dec_output, c, embedded), dim = 1))\n",
        "        \n",
        "        return pred, dec_hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDFGyoSCQWE2"
      },
      "source": [
        "### Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wJqu2TQWE4"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        # src = [src_len, batch_size]\n",
        "        # trg = [trg_len, batch_size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # enc_output is all hidden states of the input sequence, back and forwards\n",
        "        # s is the final forward and backward hidden states, passed through a linear layer\n",
        "        enc_output, s = self.encoder(src)\n",
        "                \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        dec_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # insert dec_input token embedding, previous hidden state and all encoder hidden states\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            dec_output, s = self.decoder(dec_input, s, enc_output)\n",
        "            \n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1) \n",
        "            \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            dec_input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z08TAGd2QWE9"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n",
        "The rest of this tutorial is very similar to the previous one.\n",
        "\n",
        "We initialise our parameters, encoder, decoder and seq2seq model (placing it on the GPU if we have one). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpZLK3xQWE-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXQuGEoeQWFd"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5l9s4rTQWFe"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()    \n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "        # pred = [trg_len, batch_size, pred_dim]\n",
        "        pred = model(src, trg)\n",
        "        \n",
        "        pred_dim = pred.shape[-1]\n",
        "        \n",
        "        # trg = [(trg len - 1) * batch size]\n",
        "        # pred = [(trg len - 1) * batch size, pred_dim]\n",
        "        trg = trg[1:].view(-1)\n",
        "        pred = pred[1:].view(-1, pred_dim)\n",
        "        \n",
        "        loss = criterion(pred, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "  \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdYNsPWnQWFg"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7bMdwJQWFh"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg # trg = [trg_len, batch_size]\n",
        "\n",
        "            # output = [trg_len, batch_size, output_dim]\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "          \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            # trg = [(trg_len - 1) * batch_size]\n",
        "            # output = [(trg_len - 1) * batch_size, output_dim]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVCJPGzMQWFl"
      },
      "source": [
        "Finally, define a timing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEjLOd9kQWFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5O0cPiUQWFx"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjn3fk1mQWFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "71b3f280-290b-4697-d820-0a77875ceeef"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 40s\n",
            "\tTrain Loss: 4.346 | Train PPL:  77.152\n",
            "\t Val. Loss: 3.636 |  Val. PPL:  37.924\n",
            "Epoch: 02 | Time: 2m 41s\n",
            "\tTrain Loss: 3.114 | Train PPL:  22.511\n",
            "\t Val. Loss: 3.281 |  Val. PPL:  26.606\n",
            "Epoch: 03 | Time: 2m 40s\n",
            "\tTrain Loss: 2.653 | Train PPL:  14.192\n",
            "\t Val. Loss: 3.304 |  Val. PPL:  27.221\n",
            "Epoch: 04 | Time: 2m 41s\n",
            "\tTrain Loss: 2.355 | Train PPL:  10.536\n",
            "\t Val. Loss: 3.269 |  Val. PPL:  26.282\n",
            "Epoch: 05 | Time: 2m 40s\n",
            "\tTrain Loss: 2.136 | Train PPL:   8.464\n",
            "\t Val. Loss: 3.279 |  Val. PPL:  26.548\n",
            "Epoch: 06 | Time: 2m 42s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.350\n",
            "\t Val. Loss: 3.292 |  Val. PPL:  26.895\n",
            "Epoch: 07 | Time: 2m 40s\n",
            "\tTrain Loss: 1.881 | Train PPL:   6.563\n",
            "\t Val. Loss: 3.261 |  Val. PPL:  26.070\n",
            "Epoch: 08 | Time: 2m 40s\n",
            "\tTrain Loss: 1.780 | Train PPL:   5.928\n",
            "\t Val. Loss: 3.332 |  Val. PPL:  27.984\n",
            "Epoch: 09 | Time: 2m 41s\n",
            "\tTrain Loss: 1.693 | Train PPL:   5.434\n",
            "\t Val. Loss: 3.360 |  Val. PPL:  28.796\n",
            "Epoch: 10 | Time: 2m 40s\n",
            "\tTrain Loss: 1.656 | Train PPL:   5.236\n",
            "\t Val. Loss: 3.441 |  Val. PPL:  31.209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NoPuoZfQWF2"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Yp7lmHQWF3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "078de211-00b0-4520-b0d0-6cde46faf819"
      },
      "source": [
        "model.load_state_dict(torch.load('tut3-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.337 | Test PPL:  28.140 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJQvH_DDq0y"
      },
      "source": [
        "# **一个简化的例子**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "6I8sWPl2DVWA",
        "outputId": "f3484fc0-f9fb-4f78-8a96-e46e306c9b7f"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "# [seq_len, batch_size]，一个 batch 内的句子，长度都是相同的，不同 batch 内的句子长度不一定相同\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        # enc_outputs 指的是所有时刻的最后一层隐藏状态, enc_hidden 指的是最后一个时刻所有隐藏层的状态\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    # 实际操作中，在数据预处理的时候，需要将源句子和目标句子分开构建字典\n",
        "    # 也就是单独对源语言构建一个词库，对目标语言构建一个词库\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0400 cost = 0.000522\n",
            "Epoch: 0800 cost = 0.000170\n",
            "Epoch: 1200 cost = 0.000084\n",
            "Epoch: 1600 cost = 0.000050\n",
            "Epoch: 2000 cost = 0.000033\n",
            "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
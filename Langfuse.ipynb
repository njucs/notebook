{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvjLiX9VKzB2yFaeG2A41b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Langfuse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langfuse是什么？\n",
        "\n",
        "Langfuse 是一个开源 LLM 工程平台，可帮助团队协作调试、分析和迭代其 LLM 应用程序。\n",
        "- 最常用的开源 LLMOps 平台（博客文章）\n",
        "- 与模型和框架无关\n",
        "- API 优先，所有功能均可通过 API 进行自定义集成\n",
        "- Langfuse’s core tracing is MIT licensed and will always be freely available.\n",
        "\n",
        "Langfuse 是一套有助于加速 LLM 应用程序开发工作流程的工具。您可以单独或组合使用这些功能来构建、测试和迭代您的应用程序。它提供以下主要功能：\n",
        "- 监测\n",
        "  - 跟踪：捕捉产品的完整上下文，包括外部 API 或工具调用、上下文、提示等。\n",
        "  - 实时指标：监控关键性能指标，如响应时间、错误率和吞吐量。\n",
        "  - 反馈：收集用户反馈，以改进应用程序的性能和用户体验。\n",
        "- 分析\n",
        "  - 评估：通过设置 llm-as-a-judge 评估或人工标注工作流程，比较不同模型、提示和配置的性能。\n",
        "  - 测试：试验不同版本（A/B）的应用程序，通过测试和提示管理确定最有效的解决方案\n",
        "  - 用户行为：了解用户与人工智能应用程序的交互方式。\n",
        "- 调试\n",
        "  - 详细的调试日志：访问所有应用程序活动的综合日志，以排除故障。\n",
        "  - 错误跟踪：检测和跟踪应用程序中的错误和异常。\n",
        "- 集成\n",
        "  - 框架支持：与 LangChain、LlamaIndex 和 AWS Bedrock 等流行的 LLM 框架集成。\n",
        "  - 工具支持：与 Dify 或 LobeChat 等无代码构建工具集成。\n",
        "  - 应用程序接口（API）：利用我们开放且功能强大的应用程序接口进行自定义集成和工作流程自动化。\n",
        "\n",
        "![images/langfuse-intro.png](images/langfuse-intro.png)\n",
        "\n",
        "[更多介绍](https://langfuse.com/why)"
      ],
      "metadata": {
        "id": "ubA-y2RQsU1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. 安装Langfuse"
      ],
      "metadata": {
        "id": "T-qtJVCJwa3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# 首先，你需要安装 Langfuse 的 Python SDK。\n",
        "\n",
        "!pip install langfuse"
      ],
      "metadata": {
        "id": "Jb1YW15kxV4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 接下来，你需要初始化 Langfuse，并连接到你的 Langfuse 服务器。\n",
        "# 你需要提供你的 Langfuse API 密钥和 Host URL。（新建Project -> settings -> API keys）\n",
        "# **注意:** 你需要提前安装并运行 Langfuse 服务器。\n",
        "\n",
        "from langfuse import Langfuse\n",
        "\n",
        "# 将 \"YOUR_LANGFUSE_PUBLIC_KEY\" 替换为你的 Langfuse API Key\n",
        "# 将 \"YOUR_LANGFUSE_HOST\" 替换为你的 Langfuse Host URL，一般是http://localhost:3000\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"pk-lf-6e020669-deac-49b3-a285-26b357eaf7d4\",\n",
        "    host=\"https://us.cloud.langfuse.com\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzwR4Mkixsb7",
        "outputId": "afb691ce-90e4-45e2-9bb6-df44b92f5602"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langfuse:Langfuse client is disabled since no secret_key was provided as a parameter or environment variable 'LANGFUSE_SECRET_KEY'. See our docs: https://langfuse.com/docs/sdk/python/low-level-sdk#initialize-client\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hello World 一下**"
      ],
      "metadata": {
        "id": "r42zjRj2yCTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 为了演示 Langfuse 的使用，我们创建一个简单的 LLM 应用，它使用 Siliconflow API 来回答问题。\n",
        "\n",
        "# !pip install openai\n",
        "\n",
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ozx8oBYyqQW",
        "outputId": "a8739f07-d008-4b0f-ecf9-c859f4b76abb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Collecting openai\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "Successfully installed openai-1.58.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"sk-tmqnbjqqtvgdobugtdcvrtnlnbtuqlhacyxgurbgndnokwva\", base_url=\"https://api.siliconflow.cn/v1\")\n",
        "response = client.chat.completions.create(\n",
        "    model='THUDM/glm-4-9b-chat',\n",
        "    messages=[\n",
        "        {'role': 'user',\n",
        "        'content': \"中国大模型行业2025年将会迎来哪些机遇和挑战\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zSvhXfLyvvD",
        "outputId": "5640bcde-1d5f-4703-9e53-e73db884a573"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "截至2023，预测中国大模型行业在2025年可能面临的机遇和挑战主要包括以下几方面：\n",
            "\n",
            "**机遇：**\n",
            "\n",
            "1. **政策支持：**中国政府近年来对人工智能行业给予了高度重视，出台了一系列支持政策，这为大模型的发展提供了良好的政策环境。\n",
            "\n",
            "2. **市场需求增长：**随着互联网、软件和信息服务业的快速发展，对高性能、高智能化的计算模型的需求日益增长，为大模型提供了广阔的市场空间。\n",
            "\n",
            "3. **技术进步：**算法的进步、计算能力的提升以及海量数据的积累，都有助于提高大模型的效果和效率。\n",
            "\n",
            "4. **产业链完善：**从数据采集、存储、处理到模型训练、部署，产业链上下游的资源配置更加优化，有利于大模型的商业化应用。\n",
            "\n",
            "5. **国际竞争力提升：**中国在大模型领域的研究已经取得了一系列突破，有望在全球竞争中占据一席之地。\n",
            "\n",
            "**挑战：**\n",
            "\n",
            "1. **技术瓶颈：**大模型的训练和推理需要巨大的算力，而中国在高性能计算设备以及相关技术上仍面临挑战。\n",
            "\n",
            "2. **数据隐私和安全：**大模型往往需要大量个人信息来训练，如何保护数据隐私和安全是一个重要问题。\n",
            "\n",
            "3. **伦理和监管难题：**大模型的决策过程透明度低，可能存在歧视、偏见等问题，需要建立健全的伦理和监管框架。\n",
            "\n",
            "4. **人才短缺：**大模型领域的研发需要大量高级专业人才，而这类人才供给可能不足以满足市场需求。\n",
            "\n",
            "5. **国际竞争压力：**在国际舞台上，中国大模型需要面对来自全球顶尖科技公司的激烈竞争。\n",
            "\n",
            "6. **商业模式待完善：**如何将大模型的应用商业化，以及如何平衡技术发展和社会责任，是业界需要解决的问题。\n",
            "\n",
            "总体来说，中国大模型行业在2025年有望迎来快速发展的机遇，同时也将面对技术、伦理、人才和商业模式等多方面的挑战。行业参与者需要不断进行技术创新，同时关注社会影响，以实现可持续发展。"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 Langfuse 来跟踪我们的 LLM 应用。\n",
        "# 创建一个 Trace 来记录用户的一次提问，创建一个 Span 来记录一次 OpenAI API 调用。\n",
        "# 每次调用都会有相应的 `start_span()`  `update_span()` 和 `end_span()`， 如下\n",
        "\n",
        "def ask_openai_with_langfuse(question):\n",
        "    # 创建一个 Trace\n",
        "    trace = langfuse.trace(name=\"问答机器人\")\n",
        "    # 创建一个 Span 来跟踪 OpenAI API 调用\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input=question)\n",
        "        response = ask_openai(question)\n",
        "        span.update_event(name=\"llm_response\",output=response)\n",
        "\n",
        "    trace.end()\n",
        "    return response\n",
        "\n",
        "question = \"Hello World～ 今天天气咋样？\"\n",
        "answer = ask_openai_with_langfuse(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "\n",
        "# 打开 Langfuse 仪表板，查看刚才生成的 Trace 和 Span，查看每个 Span 中的事件和观测数据。\n",
        "# 可以看到，Langfuse 记录了 LLM 应用中的关键步骤，包括：\n",
        "# *   Trace 的名称\n",
        "# *   Span 的名称\n",
        "# *   Span 中的事件，例如用户提问和 OpenAI 的回答\n",
        "#\n",
        "# 你可以在 Langfuse 仪表板中查看这些数据，也可以使用 Langfuse API 来进行更深入的数据分析。"
      ],
      "metadata": {
        "id": "iXQXCoOyzHPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 除了跟踪之外，Langfuse 还支持对 LLM 应用的输出进行评分和评估。\n",
        "# 例如，我们可以让用户手动对 LLM 回答的质量进行评分。\n",
        "\n",
        "def ask_openai_with_langfuse_and_score(question):\n",
        "    # 创建一个 Trace\n",
        "    trace = langfuse.trace(name=\"评分的问答机器人\")\n",
        "    # 创建一个 Span 来跟踪 OpenAI API 调用\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input=question)\n",
        "        response = ask_openai(question)\n",
        "        span.update_event(name=\"llm_response\",output=response)\n",
        "\n",
        "        score = input(\"请对AI的回答进行评分（1-5， 5分最高）\")\n",
        "        try:\n",
        "            span.score(name=\"response_quality\", value=int(score))\n",
        "        except:\n",
        "           print(\"评分格式错误！\")\n",
        "\n",
        "\n",
        "    trace.end()\n",
        "    return response\n",
        "\n",
        "question = \"请写一首关于冬天的诗\"\n",
        "answer = ask_openai_with_langfuse_and_score(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")"
      ],
      "metadata": {
        "id": "i5BX6TdwyKpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------分割线---------------------------------------------------"
      ],
      "metadata": {
        "id": "_KP3fTOcyaLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# Langfuse 进阶功能探索 Jupyter Notebook 课件\n",
        "\n",
        "这个课件将深入介绍 Langfuse 的一些进阶功能，包括 Observation 跟踪、自定义 Event 和 Span、 Langfuse API 以及与其它工具的集成。\n",
        "\n",
        "**你将学到:**\n",
        "1. 使用 Observation 跟踪 LLM 应用的性能指标\n",
        "2. 如何自定义 Event 和 Span，跟踪更复杂的 LLM 应用\n",
        "3. 如何使用 Langfuse API 进行数据分析\n",
        "4. 如何与其他 LLM 工具集成\n",
        "\"\"\"\n",
        "\n",
        "# In[1]:\n",
        "# ## 1. 安装 Langfuse SDK和OpenAI SDK\n",
        "# 安装Langfuse和OpenAI SDK，如果已经安装，跳过此步骤\n",
        "!pip install langfuse\n",
        "!pip install openai\n",
        "!pip install pandas\n",
        "\n",
        "# In[2]:\n",
        "# ## 2. 初始化 Langfuse\n",
        "# 和之前的课件一样，需要初始化 Langfuse\n",
        "from langfuse import Langfuse\n",
        "\n",
        "# 将 \"YOUR_LANGFUSE_PUBLIC_KEY\" 替换为你的 Langfuse API Key\n",
        "# 将 \"YOUR_LANGFUSE_HOST\" 替换为你的 Langfuse Host URL\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "    host=\"YOUR_LANGFUSE_HOST\"\n",
        ")\n",
        "\n",
        "# In[3]:\n",
        "# ## 3. 初始化 OpenAI SDK\n",
        "# 配置OpenAI的API Key，如果已经配置过，跳过此步骤\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# 将 \"YOUR_OPENAI_API_KEY\" 替换为你的 OpenAI API Key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "def ask_openai(question):\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "\n",
        "# ## 4. 使用 Observation 跟踪 LLM 应用性能指标\n",
        "\n",
        "# 除了 Event 外，Langfuse 还支持使用 Observation 来跟踪 LLM 应用的性能指标，例如延迟和 Token 数。\n",
        "# 你可以在 Span 中使用 `log_observation` 来记录这些数据。\n",
        "# 比如，我们可以用 `log_observation` 记录 LLM 回复的时间， 输入 Token 数，输出 Token 数\n",
        "\n",
        "# In[4]:\n",
        "def ask_openai_with_observations(question):\n",
        "    trace = langfuse.trace(name=\"问答机器人带性能分析\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input=question)\n",
        "        response, latency = ask_openai(question)\n",
        "        span.update_event(name=\"llm_response\",output=response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        span.log_observation(name=\"latency\", value=latency)\n",
        "        span.log_observation(name=\"input_tokens\", value = response[\"usage\"][\"prompt_tokens\"])\n",
        "        span.log_observation(name=\"output_tokens\",value= response[\"usage\"][\"completion_tokens\"])\n",
        "\n",
        "    trace.end()\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "question = \"请用三个字概括一下北京\"\n",
        "answer = ask_openai_with_observations(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "# 在Langfuse仪表盘中查看相关指标\n",
        "\n",
        "# ## 5. 自定义 Event 和 Span，跟踪更复杂的 LLM 应用\n",
        "# Langfuse 允许你自定义 Event 和 Span，以跟踪更复杂的 LLM 应用。\n",
        "# 例如，假设你的应用需要调用多个 API，你可以创建多个 Span 来分别跟踪这些 API 调用。\n",
        "# 在自定义Span中，可以使用`update_event()`来记录不同的Event，例如API请求和响应。\n",
        "\n",
        "# In[5]:\n",
        "import requests\n",
        "def call_other_api(url):\n",
        "    start_time = time.time()\n",
        "    response = requests.get(url)\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "\n",
        "def ask_openai_with_multiple_spans(question):\n",
        "    trace = langfuse.trace(name=\"多步骤问答\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input=question)\n",
        "        response ,latency = ask_openai(question)\n",
        "        span.update_event(name=\"llm_response\",output=response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        span.log_observation(name=\"latency\", value=latency)\n",
        "\n",
        "\n",
        "    with trace.span(name = \"call_other_api\") as span2:\n",
        "      span2.update_event(name=\"api_request\", input= \"https://dummyjson.com/products/1\")\n",
        "      api_response, api_latency = call_other_api(\"https://dummyjson.com/products/1\")\n",
        "      span2.update_event(name=\"api_response\", output = api_response.json())\n",
        "      span2.log_observation(name=\"latency\", value = api_latency)\n",
        "    trace.end()\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"], api_response.json()\n",
        "\n",
        "question = \"请问我有什么产品？\"\n",
        "answer, api_answer = ask_openai_with_multiple_spans(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "print(f\"API返回结果: {api_answer}\")\n",
        "# 在Langfuse仪表盘中查看详细的调用链\n",
        "\n",
        "# ## 6. 使用 Langfuse API 进行数据分析\n",
        "# 你可以使用 Langfuse 的 Python API 来查询和分析你的数据， 例如获取 Trace 的详细信息， 统计指定时间范围的延迟等。\n",
        "# Langfuse 提供了`client.traces()`，`client.spans()`, `client.observations()` 接口来获取对应的数据。\n",
        "# 为了方便使用，我们可以将数据转换为pandas 的 DataFrame 进行处理\n",
        "\n",
        "# In[6]:\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 获取最近 24 小时的 Trace 信息\n",
        "now = datetime.utcnow()\n",
        "start_time = now - timedelta(days=1)\n",
        "\n",
        "traces = langfuse.traces(\n",
        "    start_time=start_time.isoformat()\n",
        ").data\n",
        "\n",
        "traces_df = pd.DataFrame(traces)\n",
        "print(\"最近24小时的Traces信息：\")\n",
        "print(traces_df[[\"id\", \"name\", \"startTime\", \"endTime\"]])\n",
        "\n",
        "# 获取最近24小时的所有Span信息\n",
        "spans = langfuse.spans(\n",
        "    start_time = start_time.isoformat()\n",
        ").data\n",
        "\n",
        "spans_df = pd.DataFrame(spans)\n",
        "print(\"\\n最近24小时的Spans信息：\")\n",
        "print(spans_df[[\"id\", \"name\", \"traceId\", \"startTime\",\"endTime\"]])\n",
        "\n",
        "# 获取最近24小时的所有Observation信息\n",
        "observations = langfuse.observations(\n",
        "    start_time = start_time.isoformat()\n",
        ").data\n",
        "\n",
        "observations_df = pd.DataFrame(observations)\n",
        "print(\"\\n最近24小时的Observations信息:\")\n",
        "print(observations_df[[\"id\", \"name\", \"spanId\", \"value\",\"time\"]])\n",
        "\n",
        "# In[7]:\n",
        "# 使用pandas 进行进一步分析，例如统计平均延迟\n",
        "def calculate_average_latency(start_time, end_time):\n",
        "  spans = langfuse.spans(\n",
        "    start_time = start_time.isoformat(),\n",
        "      end_time = end_time.isoformat()\n",
        "  ).data\n",
        "  spans_df = pd.DataFrame(spans)\n",
        "\n",
        "  observations = langfuse.observations(\n",
        "        start_time = start_time.isoformat(),\n",
        "        end_time = end_time.isoformat()\n",
        "    ).data\n",
        "\n",
        "  observations_df = pd.DataFrame(observations)\n",
        "  latency_observations = observations_df[observations_df[\"name\"] == \"latency\"]\n",
        "\n",
        "  merged_df = pd.merge(spans_df, latency_observations, left_on = \"id\", right_on = \"spanId\", suffixes=('_span', '_observation'))\n",
        "\n",
        "  if not merged_df.empty:\n",
        "    average_latency = merged_df[\"value\"].mean()\n",
        "    print(f\"从 {start_time} 到 {end_time} 的平均延迟为: {average_latency:.4f} 秒\")\n",
        "  else:\n",
        "    print(\"当前时间段内没有延迟数据\")\n",
        "\n",
        "\n",
        "today = datetime.utcnow()\n",
        "last_week = today - timedelta(days = 7)\n",
        "calculate_average_latency(last_week, today)\n",
        "\n",
        "# ## 7. 与其他 LLM 工具集成\n",
        "# Langfuse 可以与其他 LLM 工具集成，例如向量数据库、评估框架等。\n",
        "#\n",
        "#  *   **向量数据库:** Langfuse 可以记录 LLM 调用中使用的向量数据，方便进行分析和调试。\n",
        "#  *   **评估框架:** Langfuse 可以与 LLM 评估框架集成，例如 `Ragas`，记录评估指标，方便开发者快速进行模型性能评估\n",
        "#\n",
        "# 这里演示一个简单的与`Ragas`集成的案例\n",
        "# 需要提前安装ragas库，`pip install ragas`\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "!pip install ragas\n",
        "\n",
        "# In[9]:\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
        "\n",
        "def ragas_evaluate(question, answer, context):\n",
        "  eval_result = evaluate(\n",
        "    [\n",
        "        {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"contexts\": [context]\n",
        "        }\n",
        "    ],\n",
        "      metrics = [faithfulness, answer_relevancy, context_recall, context_precision]\n",
        "  )\n",
        "  return eval_result\n",
        "\n",
        "def ask_openai_with_ragas_and_langfuse(question, context):\n",
        "   trace = langfuse.trace(name=\"ragas 评估的问答\")\n",
        "   with trace.span(name=\"openai_call\") as span:\n",
        "      span.update_event(name = \"user_question\", input = question)\n",
        "      response, latency = ask_openai(question)\n",
        "      span.update_event(name=\"llm_response\", output = response[\"choices\"][0][\"message\"][\"content\"])\n",
        "      span.log_observation(name=\"latency\", value = latency)\n",
        "      eval_result = ragas_evaluate(question, response[\"choices\"][0][\"message\"][\"content\"], context)\n",
        "\n",
        "      for metric_name, metric_value in eval_result.items():\n",
        "          span.score(name = metric_name, value = metric_value)\n",
        "\n",
        "\n",
        "   trace.end()\n",
        "   return  response[\"choices\"][0][\"message\"][\"content\"], eval_result\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"The capital of France is Paris.\"\n",
        "answer, eval_result = ask_openai_with_ragas_and_langfuse(question,context)\n",
        "\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "print(f\"Ragas评估结果: {eval_result}\")\n",
        "# 在Langfuse仪表盘中查看 Ragas 的评估指标。\n",
        "\n",
        "# ## 8. 总结\n",
        "#\n",
        "# 通过这个进阶课件，你已经了解了 Langfuse 的更多功能，包括：\n",
        "#   * 使用 Observation 跟踪性能指标\n",
        "#   * 自定义 Event 和 Span\n",
        "#   * 使用 Langfuse API 进行数据分析\n",
        "#   * 与其他 LLM 工具集成\n",
        "#\n",
        "#  Langfuse 可以帮助你更深入地了解你的 LLM 应用，并不断改进其性能和效果。\n",
        "#  鼓励你继续探索 Langfuse 的更多功能，并在你的实际项目中应用。\n",
        "\n",
        "# In[ ]"
      ],
      "metadata": {
        "id": "yayqo_oo5DWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "模块一：Langfuse 入门 (Getting Started)\n",
        "\n",
        "1.1 什么是 Langfuse?\n",
        "\n",
        "学习目标: 理解 Langfuse 的基本概念、目标和价值。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "LLM 应用开发的挑战 (可观测性、可解释性、评估)\n",
        "\n",
        "Langfuse 的定义和核心功能 (跟踪、分析、评估)\n",
        "\n",
        "Langfuse 的适用场景 (LLM 应用程序、代理、工具等)\n",
        "\n",
        "Langfuse 的优势 (开源、易用、灵活、可扩展)\n",
        "\n",
        "1.2 Langfuse 环境搭建\n",
        "\n",
        "学习目标: 能够成功安装并启动 Langfuse 服务端，配置 Python SDK。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "Langfuse 服务端安装 (Docker 方式或其他方式)\n",
        "\n",
        "Langfuse 服务端配置 (API Key, Host URL)\n",
        "\n",
        "Langfuse Python SDK 安装 (pip)\n",
        "\n",
        "Langfuse Python SDK 初始化\n",
        "\n",
        "验证 Langfuse 连接是否正常\n",
        "\n",
        "1.3 核心概念详解\n",
        "\n",
        "学习目标: 掌握 Langfuse 的核心概念，为后续深入学习打下基础。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "Trace (跟踪): 代表一个完整的交互流程\n",
        "\n",
        "Span (跨度): 代表 Trace 中的一个步骤或事件\n",
        "\n",
        "Event (事件): 代表 Span 中的一个特定事件\n",
        "\n",
        "Observation (观测): 代表对 Span 和 Event 的具体观测数据\n",
        "\n",
        "Score (评分): 代表对 LLM 生成结果的评分\n",
        "\n",
        "理解这些概念之间的关系和层级结构\n",
        "\n",
        "1.4 第一个 Langfuse 应用\n",
        "\n",
        "学习目标: 能够编写简单的代码，使用 Langfuse 跟踪基本的 LLM 应用。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "创建一个简单的 LLM 应用 (例如 OpenAI 的 ChatCompletion)\n",
        "\n",
        "使用 langfuse.trace() 创建一个 Trace\n",
        "\n",
        "使用 trace.span() 创建一个 Span\n",
        "\n",
        "使用 span.update_event() 记录事件 (用户提问、LLM 回答)\n",
        "\n",
        "使用 trace.end() 结束 Trace\n",
        "\n",
        "在 Langfuse 仪表板中查看生成的 Trace 数据\n",
        "\n",
        "模块二：Langfuse 基础 (Basic Functionality)\n",
        "\n",
        "2.1 详细的 Event 记录\n",
        "\n",
        "学习目标: 掌握如何在 Span 中记录更详细的 Event 信息。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "update_event() 函数的更多参数 (input, output, metadata)\n",
        "\n",
        "使用 update_event() 记录更多的信息 (请求参数、响应头等)\n",
        "\n",
        "在 Langfuse 仪表板中查看详细的 Event 数据\n",
        "\n",
        "2.2 观测 (Observation) 的使用\n",
        "\n",
        "学习目标: 掌握如何记录 LLM 应用的性能指标，例如延迟和 Token 数。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "span.log_observation() 函数的使用 (name, value, metadata)\n",
        "\n",
        "记录延迟 (latency)\n",
        "\n",
        "记录输入 Token 数 (input_tokens)\n",
        "\n",
        "记录输出 Token 数 (output_tokens)\n",
        "\n",
        "在 Langfuse 仪表板中查看观测数据\n",
        "\n",
        "2.3 评分 (Score) 的使用\n",
        "\n",
        "学习目标: 掌握如何对 LLM 的输出进行评分和评估。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "span.score() 函数的使用 (name, value, metadata)\n",
        "\n",
        "手动评分 (例如用户对 LLM 回答的质量评分)\n",
        "\n",
        "自动化评分 (与评估框架集成)\n",
        "\n",
        "在 Langfuse 仪表板中查看评分数据\n",
        "\n",
        "2.4 自定义 Span 的使用\n",
        "\n",
        "学习目标: 掌握如何创建和使用自定义 Span 来跟踪复杂的 LLM 应用。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "自定义 Span 的命名和层级结构\n",
        "\n",
        "在 Trace 中创建多个 Span，模拟复杂调用链\n",
        "\n",
        "在不同 Span 中记录不同的 Event 和 Observation\n",
        "\n",
        "理解 Span 的父子关系\n",
        "\n",
        "在 Langfuse 仪表板中查看自定义 Span 和调用链\n",
        "\n",
        "模块三：Langfuse 高级 (Advanced Functionality)\n",
        "\n",
        "3.1 Langfuse API 的使用\n",
        "\n",
        "学习目标: 掌握如何使用 Langfuse Python API 查询和分析数据。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "使用 langfuse.traces() 查询 Trace 数据\n",
        "\n",
        "使用 langfuse.spans() 查询 Span 数据\n",
        "\n",
        "使用 langfuse.observations() 查询 Observation 数据\n",
        "\n",
        "使用 Python 库 (如 pandas) 进行数据分析和处理\n",
        "\n",
        "根据时间范围、Trace ID 等条件查询数据\n",
        "\n",
        "使用 API 实现数据导出和自动化分析\n",
        "\n",
        "3.2 与 LLM 工具集成\n",
        "\n",
        "学习目标: 掌握 Langfuse 如何与其他 LLM 工具集成，例如向量数据库、评估框架等。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "集成向量数据库 (例如记录 LLM 调用中使用的向量数据)\n",
        "\n",
        "集成评估框架 (例如 Ragas, LLMEval)\n",
        "\n",
        "记录评估指标，实现自动化模型评估\n",
        "\n",
        "演示与常用工具集成的代码示例\n",
        "\n",
        "3.3 异步操作\n",
        "\n",
        "学习目标: 掌握如何使用 Langfuse 进行异步操作，避免阻塞主线程\n",
        "\n",
        "重点内容:\n",
        "\n",
        "异步 trace() ,span(), event() 等相关方法\n",
        "\n",
        "异步执行 langfuse.flush()\n",
        "\n",
        "使用 asyncio 模块实现异步编程\n",
        "\n",
        "3.4 Langfuse 配置进阶\n",
        "\n",
        "学习目标: 掌握Langfuse的配置项，优化Langfuse使用体验\n",
        "\n",
        "重点内容:\n",
        "\n",
        "使用 debug, timeout, batch_size 等参数优化性能\n",
        "\n",
        "使用 client_options 配置HTTP请求头等信息\n",
        "\n",
        "自定义 Langfuse SDK 的行为和输出\n",
        "\n",
        "模块四：Langfuse 实战 (Real-World Applications)\n",
        "\n",
        "4.1 监控复杂的 LLM 应用\n",
        "\n",
        "学习目标: 能够使用 Langfuse 监控复杂的 LLM 应用，并找出潜在问题。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "设计合适的 Span 和 Event 结构，跟踪复杂调用链\n",
        "\n",
        "使用 Observation 监控性能指标，并找出瓶颈\n",
        "\n",
        "使用评分功能评估 LLM 输出质量，并找出问题\n",
        "\n",
        "利用 Langfuse API 分析数据，找出潜在的性能或质量问题\n",
        "\n",
        "4.2 优化 LLM 应用\n",
        "\n",
        "学习目标: 能够使用 Langfuse 来指导 LLM 应用的优化过程。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "根据性能指标和评估结果，优化 LLM 模型或参数\n",
        "\n",
        "调整 Prompt Engineering, 提升LLM输出质量\n",
        "\n",
        "使用 Langfuse 监控优化效果，并进行迭代改进\n",
        "\n",
        "4.3 构建可观测的 LLM 代理\n",
        "\n",
        "学习目标: 掌握如何使用 Langfuse 来构建可观测的 LLM 代理。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "使用 Langfuse 跟踪 LLM 代理的执行过程\n",
        "\n",
        "记录代理的决策过程，便于调试和分析\n",
        "\n",
        "使用 Langfuse 评估代理的性能和效果\n",
        "\n",
        "监控代理的调用次数、耗时等关键指标\n",
        "\n",
        "4.4 最佳实践和进阶技巧\n",
        "\n",
        "学习目标: 掌握 Langfuse 的最佳实践，提高使用效率和效果。\n",
        "\n",
        "重点内容:\n",
        "\n",
        "如何清晰命名 Span 和 Event\n",
        "\n",
        "如何合理使用 metadata\n",
        "\n",
        "如何有效利用 Langfuse 仪表板\n",
        "\n",
        "如何与其他工具集成 Langfuse\n",
        "\n",
        "如何解决使用 Langfuse 过程中的常见问题"
      ],
      "metadata": {
        "id": "z43kURL_5htt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "安装和初始化: 代码块中包含了安装 Langfuse 和 OpenAI SDK，以及初始化 Langfuse 的步骤。\n",
        "\n",
        "验证连接: 提供了一个简单的函数来验证 Langfuse SDK 是否成功连接到服务端。\n",
        "\n",
        "核心概念: 详细解释了 Trace, Span, Event, Observation 和 Score 等核心概念。\n",
        "\n",
        "第一个 Langfuse 应用: 创建了一个使用 OpenAI API 的简单应用，并演示了如何使用 Langfuse 跟踪 LLM 调用。\n",
        "\n",
        "注释: 代码中包含了详细的注释，解释每个步骤的意义和目的。"
      ],
      "metadata": {
        "id": "C4ZiMI1c6BaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# 模块一：Langfuse 入门 (Getting Started)\n",
        "\n",
        "这个模块将引导你了解 Langfuse 的基本概念，搭建开发环境，并使用 Langfuse 跟踪你的第一个 LLM 应用。\n",
        "\"\"\"\n",
        "\n",
        "# ## 1.1 什么是 Langfuse?\n",
        "#\n",
        "# **学习目标:** 理解 Langfuse 的基本概念、目标和价值。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   LLM 应用开发的挑战 (可观测性、可解释性、评估)\n",
        "# *   Langfuse 的定义和核心功能 (跟踪、分析、评估)\n",
        "# *   Langfuse 的适用场景 (LLM 应用程序、代理、工具等)\n",
        "# *   Langfuse 的优势 (开源、易用、灵活、可扩展)\n",
        "#\n",
        "# ### 1.1.1 LLM 应用开发的挑战\n",
        "#\n",
        "# 随着大型语言模型（LLM）的兴起，越来越多的开发者开始构建基于 LLM 的应用程序。\n",
        "# 然而，LLM 应用的开发和优化并非易事，面临着以下一些挑战：\n",
        "# *   **可观测性 (Observability):** 难以了解 LLM 应用的内部运行状态，例如输入、输出、中间步骤、调用链等，难以追踪错误和性能瓶颈。\n",
        "# *   **可解释性 (Interpretability):** LLM 的决策过程通常是黑箱式的，难以解释其推理过程和行为，难以理解问题所在和优化方向。\n",
        "# *   **评估 (Evaluation):** 难以有效评估 LLM 应用的性能，缺乏标准化的评估指标和方法，难以衡量优化效果。\n",
        "#\n",
        "# ### 1.1.2 Langfuse 的定义和核心功能\n",
        "#\n",
        "# Langfuse 是一个开源的 LLM 应用可观测平台，旨在帮助开发者应对上述挑战。 它提供以下核心功能：\n",
        "# *   **跟踪 (Tracing):** 记录 LLM 应用中的关键事件，例如用户输入、LLM 输出、API 调用、中间步骤等。\n",
        "# *   **分析 (Analysis):** 提供可视化的仪表板和数据分析工具，帮助开发者理解 LLM 应用的性能和行为。\n",
        "# *   **评估 (Evaluation):** 支持对 LLM 应用的输出进行评分和评估，帮助开发者优化模型性能。\n",
        "#\n",
        "# ### 1.1.3 Langfuse 的适用场景\n",
        "#\n",
        "# Langfuse 适用于各种基于 LLM 的应用程序，例如：\n",
        "# *   问答机器人 (Chatbots)\n",
        "# *   文本生成工具\n",
        "# *   内容审核系统\n",
        "# *   LLM 代理 (Agents)\n",
        "# *   自然语言处理工具 (NLP)\n",
        "# *   各种集成 LLM 的工具和系统\n",
        "#\n",
        "# ### 1.1.4 Langfuse 的优势\n",
        "#\n",
        "# Langfuse 具有以下优势：\n",
        "# *   **开源 (Open Source):** Langfuse 是开源的，这意味着你可以免费使用、修改和贡献代码，并根据自己的需求进行定制。\n",
        "# *   **易用 (Easy to Use):** Langfuse 提供了简单易用的 API 和直观的仪表板，易于上手和操作。\n",
        "# *   **灵活 (Flexible):** Langfuse 可以灵活地跟踪各种类型的 LLM 应用，并支持自定义的 Event 和 Span。\n",
        "# *   **可扩展 (Scalable):** Langfuse 可以处理大规模的 LLM 应用数据，并支持多种存储方式。\n",
        "#\n",
        "#  现在，让我们开始探索 Langfuse 的强大功能!\n",
        "\n",
        "# In[1]:\n",
        "# ## 1.2 Langfuse 环境搭建\n",
        "#\n",
        "# **学习目标:** 能够成功安装并启动 Langfuse 服务端，配置 Python SDK。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   Langfuse 服务端安装 (Docker 方式或其他方式)\n",
        "# *   Langfuse 服务端配置 (API Key, Host URL)\n",
        "# *   Langfuse Python SDK 安装 (pip)\n",
        "# *   Langfuse Python SDK 初始化\n",
        "# *   验证 Langfuse 连接是否正常\n",
        "#\n",
        "# ### 1.2.1 Langfuse 服务端安装\n",
        "#\n",
        "# Langfuse 服务端可以使用 Docker 或其他方式进行安装。\n",
        "# 这里我们推荐使用 Docker 进行安装，因为其安装过程简单且易于管理。\n",
        "#\n",
        "#   1.  **安装 Docker:** 如果你还没有安装 Docker，请访问 [Docker 官方网站](https://www.docker.com/) 下载并安装适合你操作系统的 Docker 版本。\n",
        "#   2.  **运行 Langfuse 服务端:** 打开你的终端或命令提示符，并运行以下 Docker 命令来启动 Langfuse 服务端：\n",
        "#     ```bash\n",
        "#     docker run -d -p 3000:3000 --name langfuse ghcr.io/langfuse/langfuse:latest\n",
        "#     ```\n",
        "#       *   `-d`: 后台运行 Docker 容器。\n",
        "#       *   `-p 3000:3000`: 将 Docker 容器的 3000 端口映射到主机的 3000 端口，以便你可以通过 `http://localhost:3000` 访问 Langfuse 仪表板。\n",
        "#       *   `--name langfuse`: 给 Docker 容器命名为 `langfuse`。\n",
        "#       *   `ghcr.io/langfuse/langfuse:latest`: Langfuse Docker 镜像的地址。\n",
        "#\n",
        "#   3.  **访问 Langfuse 仪表板:** 打开你的浏览器，输入 `http://localhost:3000`，你将看到 Langfuse 的仪表板界面。\n",
        "#   4.  **创建 API Key:**  在 Langfuse 仪表板中，导航到 \"Settings\" -> \"API keys\"，点击 \"Create API Key\" 创建一个新的 API Key。 你稍后将使用此 API Key 来初始化 Langfuse Python SDK。\n",
        "#\n",
        "# ### 1.2.2 Langfuse Python SDK 安装和初始化\n",
        "#\n",
        "#  现在，让我们安装 Langfuse Python SDK 并进行初始化：\n",
        "\n",
        "# In[2]:\n",
        "# 运行以下命令安装 Langfuse Python SDK\n",
        "!pip install langfuse\n",
        "\n",
        "# In[3]:\n",
        "# 导入 Langfuse SDK\n",
        "from langfuse import Langfuse\n",
        "\n",
        "# 初始化 Langfuse SDK\n",
        "# 将 \"YOUR_LANGFUSE_PUBLIC_KEY\" 替换为你的 Langfuse API Key\n",
        "# 将 \"YOUR_LANGFUSE_HOST\" 替换为你的 Langfuse Host URL， 一般是http://localhost:3000\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "    host=\"YOUR_LANGFUSE_HOST\"\n",
        ")\n",
        "\n",
        "# In[4]:\n",
        "# ### 1.2.3 验证 Langfuse 连接\n",
        "#\n",
        "# 为了验证 Langfuse Python SDK 是否成功连接到 Langfuse 服务端，我们调用一个简单的函数，如果能正常返回，则证明连接成功。\n",
        "#\n",
        "# ```python\n",
        "# def verify_langfuse_connection():\n",
        "#   try:\n",
        "#     res = langfuse.client.get(\"/health\")\n",
        "#     if res.status_code == 200:\n",
        "#       print(\"Langfuse 服务端连接成功！\")\n",
        "#     else:\n",
        "#       print(\"Langfuse 服务端连接失败，请检查配置！\")\n",
        "#       print(f\"响应状态码: {res.status_code}\")\n",
        "#   except Exception as e:\n",
        "#     print(\"Langfuse 服务端连接失败，请检查配置！\")\n",
        "#     print(f\"错误信息：{e}\")\n",
        "#\n",
        "# verify_langfuse_connection()\n",
        "# ```\n",
        "#\n",
        "# **下一步：**\n",
        "#\n",
        "# *   确保你的 Langfuse 服务端正常运行。\n",
        "# *   将 `YOUR_LANGFUSE_PUBLIC_KEY` 和 `YOUR_LANGFUSE_HOST` 替换为你的实际值。\n",
        "# *   运行上面的验证代码，确认 Langfuse Python SDK 连接正常。\n",
        "\n",
        "# In[5]:\n",
        "def verify_langfuse_connection():\n",
        "    try:\n",
        "        res = langfuse.client.get(\"/health\")\n",
        "        if res.status_code == 200:\n",
        "            print(\"Langfuse 服务端连接成功！\")\n",
        "        else:\n",
        "            print(\"Langfuse 服务端连接失败，请检查配置！\")\n",
        "            print(f\"响应状态码: {res.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(\"Langfuse 服务端连接失败，请检查配置！\")\n",
        "        print(f\"错误信息：{e}\")\n",
        "\n",
        "verify_langfuse_connection()\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "# ## 1.3 核心概念详解\n",
        "#\n",
        "# **学习目标:** 掌握 Langfuse 的核心概念，为后续深入学习打下基础。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   Trace (跟踪): 代表一个完整的交互流程\n",
        "# *   Span (跨度): 代表 Trace 中的一个步骤或事件\n",
        "# *   Event (事件): 代表 Span 中的一个特定事件\n",
        "# *   Observation (观测): 代表对 Span 和 Event 的具体观测数据\n",
        "# *   Score (评分): 代表对 LLM 生成结果的评分\n",
        "# *   理解这些概念之间的关系和层级结构\n",
        "#\n",
        "# ### 1.3.1 Trace (跟踪)\n",
        "#\n",
        "# Trace 代表一个完整的交互流程，例如：\n",
        "# *   用户向聊天机器人发起一次对话。\n",
        "# *   一个 LLM 代理执行一个完整的任务。\n",
        "# *   一个文本生成工具生成一篇文章。\n",
        "#\n",
        "#  Trace 是 Langfuse 中最高级别的概念，它由一系列 Span 组成。 每个 Trace 都有一个唯一的 ID，可以用来追踪整个交互过程。\n",
        "#\n",
        "# ### 1.3.2 Span (跨度)\n",
        "#\n",
        "# Span 代表 Trace 中的一个步骤或事件，例如：\n",
        "# *   一次用户输入\n",
        "# *   一次 LLM 调用\n",
        "# *   一次 API 调用\n",
        "# *   一次数据查询\n",
        "#\n",
        "# Span 是 Langfuse 中最基本的单位，它记录了执行操作的具体信息。 每个 Span 都有一个名称、开始时间、结束时间、以及相关的 Event 和 Observation。\n",
        "#\n",
        "# ### 1.3.3 Event (事件)\n",
        "#\n",
        "# Event 代表 Span 中的一个特定事件，例如：\n",
        "# *   用户提问\n",
        "# *   LLM 的响应\n",
        "# *   API 请求\n",
        "# *   数据查询结果\n",
        "#\n",
        "# Event 用于记录 Span 中的具体动作，可以包含输入、输出、以及其他相关的信息。\n",
        "#\n",
        "# ### 1.3.4 Observation (观测)\n",
        "#\n",
        "# Observation 代表对 Span 和 Event 的具体观测数据，例如：\n",
        "# *   LLM 调用的延迟\n",
        "# *   输入和输出的 Token 数\n",
        "# *   API 调用的响应状态码\n",
        "# *   数据库查询的耗时\n",
        "#\n",
        "# Observation 用于记录 Span 或 Event 的性能指标和状态信息。\n",
        "#\n",
        "# ### 1.3.5 Score (评分)\n",
        "#\n",
        "# Score 代表对 LLM 生成结果的评分，例如：\n",
        "# *   LLM 回答的质量评分\n",
        "# *   文本生成的结果的相关性评分\n",
        "# *   模型输出的安全性评分\n",
        "#\n",
        "# Score 用于评估 LLM 应用的输出质量。\n",
        "#\n",
        "#  **理解核心概念：**\n",
        "#\n",
        "#  可以将 Trace 理解为一个剧本，Span 为剧本中的场景，Event 为场景中的具体动作， Observation 为场景中的数据，Score 为场景的质量评价，它们共同构成了一个可观测的 LLM 应用。\n",
        "#\n",
        "#  **下一步:**\n",
        "#\n",
        "# *   深入理解 Trace, Span, Event, Observation 和 Score 的概念。\n",
        "# *   思考它们在实际 LLM 应用中的对应关系。\n",
        "# *   准备使用这些概念来跟踪你的第一个 LLM 应用。\n",
        "\n",
        "# In[7]:\n",
        "# ## 1.4 第一个 Langfuse 应用\n",
        "#\n",
        "# **学习目标:** 能够编写简单的代码，使用 Langfuse 跟踪基本的 LLM 应用。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   创建一个简单的 LLM 应用 (例如 OpenAI 的 ChatCompletion)\n",
        "# *   使用 `langfuse.trace()` 创建一个 Trace\n",
        "# *   使用 `trace.span()` 创建一个 Span\n",
        "# *   使用 `span.update_event()` 记录事件 (用户提问、LLM 回答)\n",
        "# *   使用 `trace.end()` 结束 Trace\n",
        "# *   在 Langfuse 仪表板中查看生成的 Trace 数据\n",
        "#\n",
        "# ### 1.4.1 创建一个简单的 LLM 应用\n",
        "#\n",
        "#  为了演示 Langfuse 的使用，我们先创建一个简单的 LLM 应用，使用 OpenAI 的 ChatCompletion API 来回答问题。\n",
        "#\n",
        "#   **注意：** 你需要提前安装 OpenAI Python SDK，并设置好你的 OpenAI API Key\n",
        "#\n",
        "#   ```bash\n",
        "#    !pip install openai\n",
        "#   ```\n",
        "#\n",
        "#   你需要将你的 OpenAI API Key 设置到 `openai.api_key` 中。\n",
        "\n",
        "# In[8]:\n",
        "!pip install openai\n",
        "\n",
        "# In[9]:\n",
        "import openai\n",
        "\n",
        "# 将 \"YOUR_OPENAI_API_KEY\" 替换为你的 OpenAI API Key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "def ask_openai(question):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "# In[10]:\n",
        "# ### 1.4.2 使用 Langfuse 跟踪 LLM 应用\n",
        "#\n",
        "#  现在，我们使用 Langfuse 来跟踪我们的 LLM 应用。\n",
        "#  我们使用 `langfuse.trace()` 创建一个 Trace，使用 `trace.span()` 创建一个 Span，使用 `span.update_event()` 记录 Event。\n",
        "\n",
        "def ask_openai_with_langfuse(question):\n",
        "    # 创建一个 Trace\n",
        "    trace = langfuse.trace(name=\"简单问答机器人\")\n",
        "    # 创建一个 Span 来跟踪 OpenAI API 调用\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "      span.update_event(name=\"user_question\", input = question)\n",
        "      response = ask_openai(question)\n",
        "      span.update_event(name = \"llm_response\", output=response)\n",
        "    trace.end()\n",
        "    return response\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "# ### 1.4.3 运行并查看结果\n",
        "#\n",
        "#  现在，我们运行这个函数，并查看 Langfuse 仪表板中的结果。\n",
        "question = \"你好，今天天气怎么样？\"\n",
        "answer = ask_openai_with_langfuse(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "\n",
        "#  在 Langfuse 仪表板中，你可以看到一个名称为 “简单问答机器人” 的 Trace，以及一个名称为 “openai_call” 的 Span，以及 `user_question` 和 `llm_response` 事件，点击每个 Span 可以查看其详细信息。\n",
        "#\n",
        "#  **下一步：**\n",
        "#\n",
        "# *   运行上面的代码。\n",
        "# *   访问 Langfuse 仪表板 (`http://localhost:3000`)，查看你的第一个 Trace 数据。\n",
        "# *   尝试修改代码，记录更多的信息。\n",
        "\n",
        "# ### 1.4.4 小结\n",
        "#\n",
        "# 你已经成功完成了模块一的学习，你了解了 Langfuse 的基本概念，搭建了开发环境，并使用 Langfuse 跟踪了你的第一个 LLM 应用。\n",
        "# 你已经具备了继续深入学习 Langfuse 的基础。\n",
        "#\n",
        "# **下一步：**\n",
        "#\n",
        "#  * 巩固本模块所学的知识。\n",
        "#  *  准备进入下一个模块的学习，了解更多 Langfuse 的功能。"
      ],
      "metadata": {
        "id": "PPvJtxfc6KiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "详细 Event 记录: 演示了如何使用 update_event() 函数记录更多的信息 (例如请求参数和响应头)。\n",
        "\n",
        "Observation 使用: 演示了如何使用 log_observation() 记录 LLM 应用的性能指标 (例如延迟和 Token 数)。\n",
        "\n",
        "评分使用: 演示了如何使用 score() 对 LLM 输出进行评分 (手动评分)。\n",
        "\n",
        "自定义 Span: 演示了如何创建多个 Span 来跟踪复杂的 LLM 应用，模拟调用链。\n",
        "\n",
        "注释: 代码中包含了详细的注释，解释每个步骤的意义和目的。"
      ],
      "metadata": {
        "id": "ATGMjtX56Vgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# 模块二：Langfuse 基础 (Basic Functionality)\n",
        "\n",
        "这个模块将深入讲解 Langfuse 的基础功能，包括详细的 Event 记录，Observation 的使用，Score 的使用，以及如何自定义 Span。\n",
        "\"\"\"\n",
        "\n",
        "# ## 2.1 详细的 Event 记录\n",
        "#\n",
        "# **学习目标:** 掌握如何在 Span 中记录更详细的 Event 信息。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   `update_event()` 函数的更多参数 (input, output, metadata)\n",
        "# *   使用 `update_event()` 记录更多的信息 (请求参数、响应头等)\n",
        "# *   在 Langfuse 仪表板中查看详细的 Event 数据\n",
        "#\n",
        "# ### 2.1.1 `update_event()` 函数的更多参数\n",
        "#\n",
        "# 在上一模块中，我们使用了 `span.update_event(name=\"event_name\", input=..., output=...)` 来记录事件，\n",
        "# `update_event()` 函数还提供了更多的参数，可以帮助我们记录更详细的 Event 信息：\n",
        "# *   `name`: 事件的名称 (必填)\n",
        "# *   `input`: 事件的输入 (可选)\n",
        "# *   `output`: 事件的输出 (可选)\n",
        "# *   `metadata`: 事件的元数据 (可选，字典类型，可以记录额外的属性信息)\n",
        "\n",
        "# In[1]:\n",
        "# 导入 Langfuse SDK\n",
        "from langfuse import Langfuse\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# 初始化 Langfuse SDK\n",
        "# 将 \"YOUR_LANGFUSE_PUBLIC_KEY\" 替换为你的 Langfuse API Key\n",
        "# 将 \"YOUR_LANGFUSE_HOST\" 替换为你的 Langfuse Host URL， 一般是http://localhost:3000\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "    host=\"YOUR_LANGFUSE_HOST\"\n",
        ")\n",
        "\n",
        "# 将 \"YOUR_OPENAI_API_KEY\" 替换为你的 OpenAI API Key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "\n",
        "def ask_openai(question):\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "# In[2]:\n",
        "# ### 2.1.2 使用 `update_event()` 记录更多的信息\n",
        "#\n",
        "#  现在，我们使用 `update_event()` 函数记录更多的信息，例如请求参数和响应头。\n",
        "\n",
        "def ask_openai_with_detailed_event(question):\n",
        "    trace = langfuse.trace(name=\"详细事件的问答机器人\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        # 记录用户提问，带上用户ID信息\n",
        "        span.update_event(name=\"user_question\", input=question, metadata={\"user_id\": \"12345\"})\n",
        "        response, latency = ask_openai(question)\n",
        "        # 记录 LLM 的响应，带上模型信息\n",
        "        span.update_event(name=\"llm_response\", output=response[\"choices\"][0][\"message\"][\"content\"],\n",
        "                          metadata={\"model\": \"gpt-3.5-turbo\", \"latency\": latency})\n",
        "    trace.end()\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "question = \"今天北京的天气怎么样？\"\n",
        "answer = ask_openai_with_detailed_event(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI 回答: {answer}\")\n",
        "\n",
        "# In[3]:\n",
        "# ### 2.1.3 查看详细的 Event 数据\n",
        "#\n",
        "#  运行上面的代码，并在 Langfuse 仪表板中查看结果。\n",
        "# 你可以在 Span 的详细信息中找到 `user_question` 和 `llm_response` 事件，点击每个事件，查看其 `input`、`output` 和 `metadata` 信息。\n",
        "#\n",
        "#  **下一步：**\n",
        "#\n",
        "# *   运行上面的代码。\n",
        "# *   访问 Langfuse 仪表板 (`http://localhost:3000`)，查看详细的 Event 数据。\n",
        "# *   尝试修改代码，记录更多你需要的 Event 信息。\n",
        "\n",
        "# In[4]:\n",
        "# ## 2.2 观测 (Observation) 的使用\n",
        "#\n",
        "# **学习目标:** 掌握如何记录 LLM 应用的性能指标，例如延迟和 Token 数。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   `span.log_observation()` 函数的使用 (name, value, metadata)\n",
        "# *   记录延迟 (latency)\n",
        "# *   记录输入 Token 数 (input_tokens)\n",
        "# *   记录输出 Token 数 (output_tokens)\n",
        "# *   在 Langfuse 仪表板中查看观测数据\n",
        "#\n",
        "# ### 2.2.1 `span.log_observation()` 函数的使用\n",
        "#\n",
        "#  在上一模块中，我们使用 `update_event()` 记录事件。\n",
        "#  除了 Event 之外，Langfuse 还支持使用 `span.log_observation()` 来记录 LLM 应用的性能指标，例如延迟和 Token 数。\n",
        "# `span.log_observation()` 函数的参数如下：\n",
        "# *   `name`: 观测的名称 (必填)\n",
        "# *   `value`: 观测的值 (必填，可以是数字、字符串、布尔值等)\n",
        "# *   `metadata`: 观测的元数据 (可选，字典类型，可以记录额外的属性信息)\n",
        "#\n",
        "\n",
        "# In[5]:\n",
        "# ### 2.2.2 使用 `span.log_observation()` 记录性能指标\n",
        "\n",
        "def ask_openai_with_observation(question):\n",
        "    trace = langfuse.trace(name=\"性能指标的问答机器人\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input = question)\n",
        "        response, latency = ask_openai(question)\n",
        "        span.update_event(name = \"llm_response\", output = response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        span.log_observation(name=\"latency\", value=latency)\n",
        "        span.log_observation(name=\"input_tokens\", value=response[\"usage\"][\"prompt_tokens\"], metadata={\"model\": \"gpt-3.5-turbo\"})\n",
        "        span.log_observation(name=\"output_tokens\",value = response[\"usage\"][\"completion_tokens\"], metadata={\"model\": \"gpt-3.5-turbo\"})\n",
        "    trace.end()\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "question = \"今天上海的天气怎么样？\"\n",
        "answer = ask_openai_with_observation(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "\n",
        "# In[6]:\n",
        "# ### 2.2.3 查看观测数据\n",
        "#\n",
        "# 运行上面的代码，并在 Langfuse 仪表板中查看结果。\n",
        "# 你可以在 Span 的详细信息中找到 `latency`, `input_tokens` 和 `output_tokens`  等 Observation, 点击每个 Observation，查看其 `value` 和 `metadata` 信息。\n",
        "#\n",
        "# **下一步:**\n",
        "#\n",
        "# *   运行上面的代码。\n",
        "# *   访问 Langfuse 仪表板 (`http://localhost:3000`)，查看观测数据。\n",
        "# *   尝试修改代码，记录更多你需要的性能指标。\n",
        "\n",
        "# In[7]:\n",
        "# ## 2.3  评分 (Score) 的使用\n",
        "#\n",
        "# **学习目标:** 掌握如何对 LLM 的输出进行评分和评估。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   `span.score()` 函数的使用 (name, value, metadata)\n",
        "# *   手动评分 (例如用户对 LLM 回答的质量评分)\n",
        "# *   自动化评分 (与评估框架集成)\n",
        "# *   在 Langfuse 仪表板中查看评分数据\n",
        "#\n",
        "# ### 2.3.1  `span.score()` 函数的使用\n",
        "#\n",
        "# Langfuse 支持使用 `span.score()` 函数来对 LLM 生成的输出进行评分。\n",
        "# `span.score()` 函数的参数如下：\n",
        "# *   `name`: 评分的名称 (必填)\n",
        "# *   `value`: 评分的值 (必填，可以是数字、字符串、布尔值等)\n",
        "# *   `metadata`: 评分的元数据 (可选，字典类型，可以记录额外的属性信息)\n",
        "#\n",
        "# ### 2.3.2 手动评分\n",
        "\n",
        "# In[8]:\n",
        "def ask_openai_with_manual_score(question):\n",
        "    trace = langfuse.trace(name=\"手动评分的问答机器人\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input=question)\n",
        "        response, latency = ask_openai(question)\n",
        "        span.update_event(name=\"llm_response\", output=response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        span.log_observation(name=\"latency\", value=latency)\n",
        "        score = input(\"请对 AI 的回答进行评分 (1-5，5分最高): \")\n",
        "        try:\n",
        "            span.score(name=\"response_quality\", value=int(score))\n",
        "        except:\n",
        "            print(\"评分格式错误，请输入数字 1-5\")\n",
        "    trace.end()\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "question = \"请介绍一下 Langfuse\"\n",
        "answer = ask_openai_with_manual_score(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI 回答: {answer}\")\n",
        "# In[9]:\n",
        "# ### 2.3.3 查看评分数据\n",
        "#\n",
        "# 运行上面的代码，并在 Langfuse 仪表板中查看结果。\n",
        "# 你可以在 Span 的详细信息中找到 `response_quality` 评分，查看其 `value` 信息。\n",
        "#\n",
        "# **注意:**\n",
        "#   *这里我们使用 input() 进行用户输入评分，在实际应用中，你可能会使用一些评分框架（会在后面的模块介绍）自动化评分。\n",
        "#\n",
        "#   **下一步:**\n",
        "#\n",
        "# *   运行上面的代码。\n",
        "# *   访问 Langfuse 仪表板 (`http://localhost:3000`)，查看评分数据。\n",
        "# *   尝试修改代码，记录更多的评分信息。\n",
        "\n",
        "# In[10]:\n",
        "# ## 2.4  自定义 Span 的使用\n",
        "#\n",
        "# **学习目标:** 掌握如何创建和使用自定义 Span 来跟踪复杂的 LLM 应用。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   自定义 Span 的命名和层级结构\n",
        "# *   在 Trace 中创建多个 Span，模拟复杂调用链\n",
        "# *   在不同 Span 中记录不同的 Event 和 Observation\n",
        "# *   理解 Span 的父子关系\n",
        "# *   在 Langfuse 仪表板中查看自定义 Span 和调用链\n",
        "#\n",
        "# ### 2.4.1 自定义 Span 和层级结构\n",
        "#\n",
        "# 在前面的示例中，我们只使用了一个 Span 来跟踪 OpenAI 的 API 调用。\n",
        "# 在实际应用中，我们可能需要创建多个 Span 来跟踪不同的操作，例如：\n",
        "# *   LLM 调用前的数据预处理\n",
        "# *   多个 API 调用\n",
        "# *   LLM 调用后的数据处理\n",
        "# *   数据库查询\n",
        "#\n",
        "# 我们可以使用 `trace.span()` 创建新的 Span。 Span 之间可以有父子关系，形成调用链。\n",
        "# 在 Langfuse 仪表板中，我们可以看到 Span 的层级结构，方便我们理解复杂的调用链。\n",
        "#\n",
        "# ### 2.4.2 在 Trace 中创建多个 Span\n",
        "\n",
        "# In[11]:\n",
        "import requests\n",
        "\n",
        "def call_api(url):\n",
        "    start_time = time.time()\n",
        "    response = requests.get(url)\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "\n",
        "\n",
        "def ask_openai_with_custom_spans(question):\n",
        "    trace = langfuse.trace(name = \"多步骤的问答机器人\")\n",
        "    with trace.span(name=\"preprocess_data\") as preprocess_span:\n",
        "        preprocess_span.update_event(name = \"start_preprocessing\", input = question)\n",
        "        # 模拟数据预处理过程\n",
        "        preprocessed_question = question.upper()\n",
        "        preprocess_span.update_event(name = \"finish_preprocessing\", output = preprocessed_question)\n",
        "\n",
        "    with trace.span(name=\"openai_call\") as openai_span:\n",
        "        openai_span.update_event(name = \"user_question\", input=preprocessed_question)\n",
        "        response, latency = ask_openai(preprocessed_question)\n",
        "        openai_span.update_event(name=\"llm_response\", output=response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        openai_span.log_observation(name=\"latency\", value = latency)\n",
        "\n",
        "    with trace.span(name=\"api_call\") as api_span:\n",
        "        api_span.update_event(name=\"api_request\", input=\"https://dummyjson.com/products/1\")\n",
        "        api_response, api_latency = call_api(\"https://dummyjson.com/products/1\")\n",
        "        api_span.update_event(name = \"api_response\", output=api_response.json())\n",
        "        api_span.log_observation(name = \"latency\", value = api_latency)\n",
        "\n",
        "    trace.end()\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"], api_response.json()\n",
        "\n",
        "question = \"请问我有什么产品？\"\n",
        "answer, api_response = ask_openai_with_custom_spans(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI 回答: {answer}\")\n",
        "print(f\"API 响应：{api_response}\")\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "# ### 2.4.3  查看自定义 Span 和调用链\n",
        "#\n",
        "# 运行上面的代码，并在 Langfuse 仪表板中查看结果。\n",
        "# 你可以在 Trace 的详细信息中看到多个 Span，它们之间有层级关系，形成了一个调用链。\n",
        "#  你可以点击每个 Span，查看其 Event 和 Observation。\n",
        "#\n",
        "#  **下一步：**\n",
        "#\n",
        "# *   运行上面的代码。\n",
        "# *   访问 Langfuse 仪表板 (`http://localhost:3000`)，查看自定义 Span 和调用链。\n",
        "# *   尝试修改代码，添加更多自定义 Span，模拟更复杂的调用流程。\n",
        "#\n",
        "\n",
        "# In[13]:\n",
        "# ### 2.4.4 小结\n",
        "#\n",
        "# 你已经成功完成了模块二的学习，你掌握了 Langfuse 的基础功能，包括：\n",
        "#  *  记录详细的 Event 信息\n",
        "#  *  使用 Observation 记录性能指标\n",
        "#  *  使用 Score 评估 LLM 输出质量\n",
        "#  *  创建自定义 Span 跟踪复杂流程\n",
        "# 你已经具备了使用 Langfuse 监控和评估简单 LLM 应用的能力。\n",
        "#\n",
        "# **下一步：**\n",
        "#  *  巩固本模块所学的知识。\n",
        "#  *  准备进入下一个模块的学习，了解更多 Langfuse 的高级功能。"
      ],
      "metadata": {
        "id": "uzLsaODQ6Y9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langfuse API 使用: 演示了如何使用 langfuse.traces(), langfuse.spans() 和 langfuse.observations() 查询 Langfuse 数据，并使用 pandas 进行数据分析。\n",
        "\n",
        "与 Ragas 集成: 演示了如何集成 Ragas 评估框架，自动评估 LLM 输出质量，并将评估指标记录到 Langfuse 中。\n",
        "\n",
        "异步操作: 演示了如何使用异步 API 进行异步操作，避免阻塞主线程，并使用 asyncio 模块进行异步编程。\n",
        "\n",
        "Langfuse 配置进阶: 介绍了如何使用配置项和 client_options，优化 Langfuse 的性能和使用体验。\n",
        "\n",
        "注释: 代码中包含了详细的注释，解释每个步骤的意义和目的。"
      ],
      "metadata": {
        "id": "LDiTjOdX6ZzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# 模块三：Langfuse 高级 (Advanced Functionality)\n",
        "\n",
        "这个模块将深入介绍 Langfuse 的一些高级功能，包括 Langfuse API 的使用、与 LLM 工具的集成、异步操作和 Langfuse 配置进阶。\n",
        "\"\"\"\n",
        "\n",
        "# ## 3.1  Langfuse API 的使用\n",
        "#\n",
        "# **学习目标:** 掌握如何使用 Langfuse Python API 查询和分析数据。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   使用 `langfuse.traces()` 查询 Trace 数据\n",
        "# *   使用 `langfuse.spans()` 查询 Span 数据\n",
        "# *   使用 `langfuse.observations()` 查询 Observation 数据\n",
        "# *   使用 Python 库 (如 pandas) 进行数据分析和处理\n",
        "# *   根据时间范围、Trace ID 等条件查询数据\n",
        "# *   使用 API 实现数据导出和自动化分析\n",
        "#\n",
        "# ### 3.1.1  Langfuse API 简介\n",
        "#\n",
        "#  Langfuse 提供了 Python API，你可以使用 API 来查询、分析和管理你的 Langfuse 数据。\n",
        "#  你可以使用 API 来：\n",
        "# *   获取 Trace 的详细信息\n",
        "# *   获取 Span 的详细信息\n",
        "# *   获取 Observation 的详细信息\n",
        "# *   查询特定时间段的数据\n",
        "# *   按照指定条件过滤数据\n",
        "# *   导出数据到其他系统\n",
        "#\n",
        "#  Langfuse API 使用 RESTful 风格，并支持 JSON 格式的数据。\n",
        "#  在 Python 中，你可以使用 `langfuse`  SDK 中的`client`对象来调用 Langfuse API。\n",
        "\n",
        "# In[1]:\n",
        "# 导入 Langfuse SDK\n",
        "from langfuse import Langfuse\n",
        "import openai\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 初始化 Langfuse SDK\n",
        "# 将 \"YOUR_LANGFUSE_PUBLIC_KEY\" 替换为你的 Langfuse API Key\n",
        "# 将 \"YOUR_LANGFUSE_HOST\" 替换为你的 Langfuse Host URL， 一般是http://localhost:3000\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "    host=\"YOUR_LANGFUSE_HOST\"\n",
        ")\n",
        "\n",
        "# 将 \"YOUR_OPENAI_API_KEY\" 替换为你的 OpenAI API Key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "def ask_openai(question):\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "\n",
        "def ask_openai_with_observation(question):\n",
        "    trace = langfuse.trace(name=\"性能指标的问答机器人\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input = question)\n",
        "        response, latency = ask_openai(question)\n",
        "        span.update_event(name = \"llm_response\", output = response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        span.log_observation(name=\"latency\", value=latency)\n",
        "        span.log_observation(name=\"input_tokens\", value=response[\"usage\"][\"prompt_tokens\"], metadata={\"model\": \"gpt-3.5-turbo\"})\n",
        "        span.log_observation(name=\"output_tokens\",value = response[\"usage\"][\"completion_tokens\"], metadata={\"model\": \"gpt-3.5-turbo\"})\n",
        "    trace.end()\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "question = \"北京今天天气怎么样？\"\n",
        "answer = ask_openai_with_observation(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "\n",
        "# In[2]:\n",
        "# ### 3.1.2 使用 `langfuse.traces()` 查询 Trace 数据\n",
        "#\n",
        "#  使用 `langfuse.traces()` 可以查询 Trace 数据。\n",
        "#\n",
        "#  `langfuse.traces()` 函数的参数如下：\n",
        "#  *   `start_time`: 查询数据的开始时间 (可选，datetime 对象或 ISO 格式字符串)\n",
        "#  *   `end_time`: 查询数据的结束时间 (可选，datetime 对象或 ISO 格式字符串)\n",
        "#  *   `limit`: 查询数据的最大数量 (可选，默认值为 100)\n",
        "#  *   `offset`: 查询数据的偏移量 (可选，用于分页)\n",
        "#  *   `trace_id`: 查询特定的 Trace ID\n",
        "#\n",
        "#  `langfuse.traces()` 函数返回一个包含所有 Trace 数据的列表，每个 Trace 数据都是一个字典。\n",
        "\n",
        "# 获取最近 24 小时的 Trace 信息\n",
        "now = datetime.utcnow()\n",
        "start_time = now - timedelta(days=1)\n",
        "\n",
        "traces = langfuse.traces(\n",
        "    start_time=start_time.isoformat()\n",
        ").data\n",
        "\n",
        "traces_df = pd.DataFrame(traces)\n",
        "print(\"最近24小时的Traces信息：\")\n",
        "print(traces_df[[\"id\", \"name\", \"startTime\", \"endTime\"]])\n",
        "\n",
        "# In[3]:\n",
        "# ### 3.1.3 使用 `langfuse.spans()` 查询 Span 数据\n",
        "#\n",
        "# 使用 `langfuse.spans()` 可以查询 Span 数据。\n",
        "#\n",
        "# `langfuse.spans()` 函数的参数与 `langfuse.traces()` 函数类似，但多了 `trace_id` 参数，用于查询特定 Trace 中的 Span 数据。\n",
        "\n",
        "# 获取最近 24 小时的所有 Span 信息\n",
        "spans = langfuse.spans(\n",
        "    start_time = start_time.isoformat()\n",
        ").data\n",
        "\n",
        "spans_df = pd.DataFrame(spans)\n",
        "print(\"\\n最近24小时的Spans信息：\")\n",
        "print(spans_df[[\"id\", \"name\", \"traceId\", \"startTime\",\"endTime\"]])\n",
        "\n",
        "# In[4]:\n",
        "# ### 3.1.4 使用 `langfuse.observations()` 查询 Observation 数据\n",
        "#\n",
        "#  使用 `langfuse.observations()` 可以查询 Observation 数据。\n",
        "#  `langfuse.observations()` 函数的参数与 `langfuse.traces()` 和 `langfuse.spans()` 函数类似。\n",
        "\n",
        "# 获取最近 24 小时的所有 Observation 信息\n",
        "observations = langfuse.observations(\n",
        "    start_time = start_time.isoformat()\n",
        ").data\n",
        "\n",
        "observations_df = pd.DataFrame(observations)\n",
        "print(\"\\n最近24小时的Observations信息:\")\n",
        "print(observations_df[[\"id\", \"name\", \"spanId\", \"value\",\"time\"]])\n",
        "\n",
        "# In[5]:\n",
        "# ### 3.1.5 使用 Python 库进行数据分析\n",
        "#\n",
        "#   为了方便数据分析，我们可以使用 `pandas` 库将 Langfuse API 返回的数据转换为 `DataFrame`，并进行进一步的数据分析。\n",
        "#  例如，我们可以计算平均延迟。\n",
        "\n",
        "def calculate_average_latency(start_time, end_time):\n",
        "  spans = langfuse.spans(\n",
        "    start_time = start_time.isoformat(),\n",
        "      end_time = end_time.isoformat()\n",
        "  ).data\n",
        "  spans_df = pd.DataFrame(spans)\n",
        "\n",
        "  observations = langfuse.observations(\n",
        "        start_time = start_time.isoformat(),\n",
        "        end_time = end_time.isoformat()\n",
        "    ).data\n",
        "\n",
        "  observations_df = pd.DataFrame(observations)\n",
        "  latency_observations = observations_df[observations_df[\"name\"] == \"latency\"]\n",
        "\n",
        "  merged_df = pd.merge(spans_df, latency_observations, left_on = \"id\", right_on = \"spanId\", suffixes=('_span', '_observation'))\n",
        "\n",
        "  if not merged_df.empty:\n",
        "    average_latency = merged_df[\"value\"].mean()\n",
        "    print(f\"从 {start_time} 到 {end_time} 的平均延迟为: {average_latency:.4f} 秒\")\n",
        "  else:\n",
        "    print(\"当前时间段内没有延迟数据\")\n",
        "\n",
        "today = datetime.utcnow()\n",
        "last_week = today - timedelta(days = 7)\n",
        "calculate_average_latency(last_week, today)\n",
        "# In[6]:\n",
        "# ### 3.1.6 更多 API 用法\n",
        "#  你还可以使用 Langfuse API 进行更复杂的操作，例如：\n",
        "# *  根据 Trace ID 或 Span ID 查询特定数据。\n",
        "# *  根据自定义的过滤器查询数据。\n",
        "# *  使用 API 导出数据。\n",
        "# *  使用 API 实现自动化数据分析。\n",
        "#\n",
        "# 你可以参考 Langfuse 的官方文档，了解更多 API 的使用方法。\n",
        "#\n",
        "#  **下一步：**\n",
        "#  *  运行上面的代码，查看 API 返回的数据。\n",
        "#  *  尝试修改代码，使用不同的查询条件，分析不同的数据。\n",
        "#  *  尝试使用 pandas 或其他数据分析工具，对 Langfuse 数据进行更深入的分析。\n",
        "\n",
        "# In[7]:\n",
        "# ## 3.2  与 LLM 工具集成\n",
        "#\n",
        "# **学习目标:** 掌握 Langfuse 如何与其他 LLM 工具集成，例如向量数据库、评估框架等。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   集成向量数据库 (例如记录 LLM 调用中使用的向量数据)\n",
        "# *   集成评估框架 (例如 `Ragas`, `LLMEval`)\n",
        "# *   记录评估指标，实现自动化模型评估\n",
        "# *   演示与常用工具集成的代码示例\n",
        "#\n",
        "# ### 3.2.1  与评估框架 `Ragas` 集成\n",
        "#\n",
        "#  为了演示 Langfuse 与评估框架的集成，我们选择 `Ragas` 评估框架。\n",
        "#  `Ragas` 提供了一系列的指标，例如 `faithfulness` (忠实度), `answer_relevancy` (答案相关性),\n",
        "#  `context_recall` (上下文召回率) 和 `context_precision` (上下文精度) 等指标，用于评估 LLM 生成结果的质量。\n",
        "#\n",
        "#  **注意:** 你需要提前安装 `ragas` 库：\n",
        "#  ```bash\n",
        "#  !pip install ragas\n",
        "#  ```\n",
        "\n",
        "# In[8]:\n",
        "!pip install ragas\n",
        "\n",
        "# In[9]:\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
        "\n",
        "def ragas_evaluate(question, answer, context):\n",
        "  eval_result = evaluate(\n",
        "    [\n",
        "        {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"contexts\": [context]\n",
        "        }\n",
        "    ],\n",
        "      metrics = [faithfulness, answer_relevancy, context_recall, context_precision]\n",
        "  )\n",
        "  return eval_result\n",
        "\n",
        "def ask_openai_with_ragas_and_langfuse(question, context):\n",
        "    trace = langfuse.trace(name=\"ragas 评估的问答机器人\")\n",
        "    with trace.span(name=\"openai_call\") as span:\n",
        "        span.update_event(name = \"user_question\", input = question)\n",
        "        response, latency = ask_openai(question)\n",
        "        span.update_event(name=\"llm_response\", output = response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        span.log_observation(name=\"latency\", value = latency)\n",
        "\n",
        "        eval_result = ragas_evaluate(question, response[\"choices\"][0][\"message\"][\"content\"], context)\n",
        "\n",
        "        for metric_name, metric_value in eval_result.items():\n",
        "            span.score(name = metric_name, value = metric_value)\n",
        "\n",
        "    trace.end()\n",
        "    return  response[\"choices\"][0][\"message\"][\"content\"], eval_result\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"The capital of France is Paris.\"\n",
        "answer, eval_result = ask_openai_with_ragas_and_langfuse(question,context)\n",
        "\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI回答: {answer}\")\n",
        "print(f\"Ragas评估结果: {eval_result}\")\n",
        "# In[10]:\n",
        "# ### 3.2.2 查看评估结果\n",
        "#\n",
        "# 运行上面的代码，并在 Langfuse 仪表板中查看结果。\n",
        "#  你可以在 Span 的详细信息中找到 Ragas 评估的指标和评分。\n",
        "#\n",
        "#   **下一步：**\n",
        "#  *  运行上面的代码，查看 Langfuse 仪表板中的结果。\n",
        "#  *  尝试修改代码，集成其他评估框架或工具。\n",
        "\n",
        "# In[11]:\n",
        "# ## 3.3  异步操作\n",
        "#\n",
        "# **学习目标:** 掌握如何使用 Langfuse 进行异步操作，避免阻塞主线程\n",
        "#\n",
        "# **重点内容:**\n",
        "#  *   异步 `trace()` ,`span()`, `event()` 等相关方法\n",
        "#  *    异步执行 `langfuse.flush()`\n",
        "#  *   使用 `asyncio` 模块实现异步编程\n",
        "#\n",
        "# ### 3.3.1 为什么需要异步操作\n",
        "# 在实际应用中，LLM 调用可能会比较耗时，如果使用同步操作，会导致主线程阻塞，降低程序的响应速度。\n",
        "# Langfuse 提供了异步操作的支持，可以让你在不阻塞主线程的情况下记录跟踪数据。\n",
        "#\n",
        "#  ### 3.3.2  异步操作方法\n",
        "#  Langfuse 提供了异步版本的 API，例如 `langfuse.trace_async()`, `trace.span_async()`, `span.update_event_async()`, 以及 `langfuse.flush_async()` 等方法。\n",
        "#  这些方法的使用方式与同步方法类似，但需要使用 `asyncio` 模块进行异步编程。\n",
        "#  *   `async`：用于定义一个异步函数，这个函数可以被 `await` 调用。\n",
        "#  *   `await`：用于等待一个异步函数执行完成。\n",
        "#\n",
        "# ### 3.3.3 异步操作示例\n",
        "\n",
        "# In[12]:\n",
        "import asyncio\n",
        "\n",
        "async def ask_openai_async(question):\n",
        "    response = await openai.ChatCompletion.acreate(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "async def ask_openai_with_langfuse_async(question):\n",
        "    trace = langfuse.trace(name=\"异步问答机器人\")\n",
        "    async with trace.span(name=\"openai_call\") as span:\n",
        "      await span.update_event_async(name=\"user_question\", input = question)\n",
        "      response = await ask_openai_async(question)\n",
        "      await span.update_event_async(name = \"llm_response\", output=response)\n",
        "    await trace.end_async()\n",
        "    await langfuse.flush_async()\n",
        "    return response\n",
        "\n",
        "async def main():\n",
        "  question = \"你好，异步操作怎么样？\"\n",
        "  answer = await ask_openai_with_langfuse_async(question)\n",
        "  print(f\"用户提问: {question}\")\n",
        "  print(f\"AI回答: {answer}\")\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "# ### 3.3.4 注意\n",
        "#  *  使用异步 API 时，需要使用 `await` 关键字等待异步操作完成。\n",
        "#  *  需要在 `asyncio.run()` 中执行异步函数。\n",
        "#  *  需要调用 `langfuse.flush_async()` 确保所有数据都被正确提交。\n",
        "#\n",
        "#  **下一步:**\n",
        "#  *   运行上面的代码，查看 Langfuse 仪表板中的结果。\n",
        "#  *   尝试修改代码，使用异步方法记录更多信息。\n",
        "\n",
        "# In[14]:\n",
        "# ## 3.4  Langfuse 配置进阶\n",
        "#\n",
        "# **学习目标:** 掌握Langfuse的配置项，优化Langfuse使用体验\n",
        "#\n",
        "# **重点内容:**\n",
        "#  *  使用 `debug`, `timeout`, `batch_size` 等参数优化性能\n",
        "#  *  使用 `client_options` 配置HTTP请求头等信息\n",
        "#  *   自定义 Langfuse SDK 的行为和输出\n",
        "#\n",
        "# ### 3.4.1 配置参数\n",
        "# 在初始化 Langfuse SDK 时，你可以设置一些参数来优化性能。\n",
        "# *   `debug`： 开启调试模式，输出详细日志信息\n",
        "# *   `timeout`： 设置 HTTP 请求超时时间，单位秒\n",
        "# *   `batch_size`：设置批量提交数据的数量，可以提升性能\n",
        "#\n",
        "# ```python\n",
        "# langfuse = Langfuse(\n",
        "#    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "#    host=\"YOUR_LANGFUSE_HOST\",\n",
        "#    debug=True,\n",
        "#    timeout=10,\n",
        "#    batch_size=100\n",
        "#  )\n",
        "# ```\n",
        "# ### 3.4.2 client_options 配置HTTP请求\n",
        "# `client_options` 可以帮助你自定义HTTP请求头，例如添加 `Authorization` 等\n",
        "#\n",
        "# ```python\n",
        "# langfuse = Langfuse(\n",
        "#    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "#    host=\"YOUR_LANGFUSE_HOST\",\n",
        "#    client_options = {\"headers\": {\"Authorization\": \"Bearer XXXX\"}}\n",
        "#  )\n",
        "# ```\n",
        "# ### 3.4.3 自定义 Langfuse SDK 行为\n",
        "# 你可以通过继承 Langfuse 的 `BaseLangfuse` 类，自定义 Langfuse SDK 的行为。\n",
        "#\n",
        "#  例如，你可以修改 SDK 提交数据的方式、调整日志输出、自定义错误处理等。\n",
        "\n",
        "#  **下一步:**\n",
        "#  *  修改初始化代码，尝试配置不同的参数。\n",
        "#  *  阅读 Langfuse 文档，了解更多配置项。\n",
        "\n",
        "# In[15]:\n",
        "# ### 3.4.4 小结\n",
        "#\n",
        "# 你已经成功完成了模块三的学习，你掌握了 Langfuse 的高级功能，包括：\n",
        "# * 使用 Langfuse API 查询分析数据\n",
        "# * 集成 Ragas 等评估框架\n",
        "# * 使用异步操作\n",
        "# * Langfuse 配置进阶\n",
        "#  你已经具备了使用 Langfuse 分析复杂的 LLM 应用，并与其他工具集成。\n",
        "#\n",
        "# **下一步:**\n",
        "# * 巩固本模块所学的知识。\n",
        "# * 准备进入下一个模块的学习，掌握 Langfuse 的实战技巧。"
      ],
      "metadata": {
        "id": "2BrZzL186tjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "复杂 LLM 应用监控: 演示了如何使用 Langfuse 监控复杂的 LLM 应用，并结合实际代码演示了如何记录多个 Span 和 Event，使用 Observation 监控性能指标，使用评分功能评估 LLM 输出。\n",
        "\n",
        "优化 LLM 应用: 详细介绍了如何使用 Langfuse 分析数据，并基于分析结果优化 LLM 应用，包括 LLM 模型参数、Prompt Engineering等。\n",
        "\n",
        "构建可观测的 LLM 代理: 演示了如何使用 Langfuse 构建可观测的 LLM 代理，并记录代理的决策过程、执行步骤、性能指标等。\n",
        "\n",
        "最佳实践和进阶技巧: 总结了使用 Langfuse 的最佳实践，包括如何清晰命名 Span 和 Event、如何合理使用 Metadata、如何有效利用 Langfuse 仪表板、如何与其他工具集成 Langfuse 等。\n",
        "\n",
        "注释: 代码中包含了详细的注释，解释每个步骤的意义和目的。"
      ],
      "metadata": {
        "id": "6QxUvrUs6xbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# 模块四：Langfuse 实战 (Real-World Applications)\n",
        "\n",
        "这个模块将引导你如何使用 Langfuse 来监控复杂的 LLM 应用，进行性能优化，构建可观测的 LLM 代理，并总结 Langfuse 的最佳实践。\n",
        "\"\"\"\n",
        "\n",
        "# ## 4.1  监控复杂的 LLM 应用\n",
        "#\n",
        "# **学习目标:** 能够使用 Langfuse 监控复杂的 LLM 应用，并找出潜在问题。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   设计合适的 Span 和 Event 结构，跟踪复杂调用链\n",
        "# *   使用 Observation 监控性能指标，并找出瓶颈\n",
        "# *   使用评分功能评估 LLM 输出质量，并找出问题\n",
        "# *   利用 Langfuse API 分析数据，找出潜在的性能或质量问题\n",
        "#\n",
        "# ### 4.1.1 复杂 LLM 应用的特点\n",
        "#\n",
        "# 复杂的 LLM 应用通常具有以下特点：\n",
        "# *   多个步骤： 需要调用多个 API 或执行多个操作。\n",
        "# *   多个 LLM 模型： 可能需要调用多个 LLM 模型。\n",
        "# *   复杂的逻辑：  需要进行数据处理、条件判断、逻辑控制等。\n",
        "# *   异步操作：  需要处理异步任务，例如后台处理、消息队列等。\n",
        "#\n",
        "# ### 4.1.2 设计合适的 Span 和 Event 结构\n",
        "#\n",
        "# 对于复杂的 LLM 应用，我们需要设计合适的 Span 和 Event 结构，清晰地记录应用的执行流程。\n",
        "# *   **Span 的粒度：** Span 的粒度应该根据实际情况进行调整，不要过于粗略，也不要过于细致。一般来说，一个 Span 应该代表一个有意义的操作单元。\n",
        "# *   **Event 的信息：** Event 应该记录足够多的信息，例如输入、输出、参数、状态、错误信息等，方便后续分析。\n",
        "# *   **Span 的层级结构：** 合理利用 Span 的层级结构，清晰地记录调用链，方便理解应用的执行流程。\n",
        "#\n",
        "# ### 4.1.3 使用 Observation 监控性能指标\n",
        "#\n",
        "# 对于复杂的 LLM 应用，我们需要关注一些关键的性能指标，例如：\n",
        "# *   总延迟：  整个 Trace 的执行时间。\n",
        "# *   每个 Span 的延迟：  每个 Span 的执行时间。\n",
        "# *   LLM 模型的延迟： LLM 模型的调用时间。\n",
        "# *   API 调用的延迟： API 接口的调用时间。\n",
        "# *   数据库查询的延迟： 数据库查询的时间。\n",
        "# *   Token 数：  LLM 模型的输入和输出 Token 数。\n",
        "#\n",
        "#  通过监控这些性能指标，我们可以找出潜在的性能瓶颈，例如慢的 API 调用、耗时的数据库查询等。\n",
        "#\n",
        "# ### 4.1.4 使用评分功能评估 LLM 输出质量\n",
        "#\n",
        "#  对于复杂的 LLM 应用，我们需要关注 LLM 输出的质量，例如：\n",
        "# *   相关性：  LLM 输出是否与输入相关。\n",
        "# *   准确性：  LLM 输出是否准确。\n",
        "# *   完整性：  LLM 输出是否完整。\n",
        "# *   逻辑性：  LLM 输出是否逻辑清晰。\n",
        "# *   安全性： LLM 输出是否安全。\n",
        "#\n",
        "# 通过评分功能，我们可以评估 LLM 输出的质量，并找出潜在的问题。\n",
        "#\n",
        "# ### 4.1.5 使用 Langfuse API 分析数据\n",
        "#\n",
        "#  对于复杂的 LLM 应用，我们需要利用 Langfuse API 分析数据，找出潜在的问题。\n",
        "#  例如，我们可以：\n",
        "# *   统计每个 Span 的平均延迟。\n",
        "# *   统计每个 Event 的发生次数。\n",
        "# *   分析 LLM 输出的评分分布。\n",
        "# *   找出执行时间最长的 Trace。\n",
        "# *   找出评分最低的 Trace。\n",
        "#\n",
        "# 通过分析数据，我们可以找出潜在的性能或质量问题，为优化提供依据。\n",
        "\n",
        "# In[1]:\n",
        "# 导入 Langfuse SDK\n",
        "from langfuse import Langfuse\n",
        "import openai\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import asyncio\n",
        "\n",
        "# 初始化 Langfuse SDK\n",
        "# 将 \"YOUR_LANGFUSE_PUBLIC_KEY\" 替换为你的 Langfuse API Key\n",
        "# 将 \"YOUR_LANGFUSE_HOST\" 替换为你的 Langfuse Host URL， 一般是http://localhost:3000\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"YOUR_LANGFUSE_PUBLIC_KEY\",\n",
        "    host=\"YOUR_LANGFUSE_HOST\"\n",
        ")\n",
        "\n",
        "# 将 \"YOUR_OPENAI_API_KEY\" 替换为你的 OpenAI API Key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "def ask_openai(question):\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "\n",
        "def call_api(url):\n",
        "    start_time = time.time()\n",
        "    response = requests.get(url)\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "# In[2]:\n",
        "# ### 4.1.6 复杂的LLM应用示例\n",
        "\n",
        "def complex_llm_app(question):\n",
        "    trace = langfuse.trace(name=\"复杂LLM应用\")\n",
        "    with trace.span(name=\"preprocess\") as preprocess_span:\n",
        "        preprocess_span.update_event(name=\"start_preprocess\", input = question)\n",
        "        preprocessed_question = question.upper()\n",
        "        preprocess_span.update_event(name=\"end_preprocess\", output = preprocessed_question)\n",
        "\n",
        "    with trace.span(name=\"openai_call\") as openai_span:\n",
        "        openai_span.update_event(name=\"user_question\", input = preprocessed_question)\n",
        "        response, latency = ask_openai(preprocessed_question)\n",
        "        openai_span.update_event(name=\"llm_response\", output = response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        openai_span.log_observation(name=\"latency\", value = latency)\n",
        "        openai_span.log_observation(name=\"input_tokens\", value = response[\"usage\"][\"prompt_tokens\"])\n",
        "        openai_span.log_observation(name = \"output_tokens\", value = response[\"usage\"][\"completion_tokens\"])\n",
        "\n",
        "    with trace.span(name=\"api_call\") as api_span:\n",
        "        api_span.update_event(name=\"api_request\", input=\"https://dummyjson.com/products/1\")\n",
        "        api_response, api_latency = call_api(\"https://dummyjson.com/products/1\")\n",
        "        api_span.update_event(name=\"api_response\", output = api_response.json())\n",
        "        api_span.log_observation(name = \"latency\", value = api_latency)\n",
        "\n",
        "    with trace.span(name=\"postprocess\") as postprocess_span:\n",
        "         postprocess_span.update_event(name=\"start_postprocess\",input= response[\"choices\"][0][\"message\"][\"content\"])\n",
        "         # 模拟一些后处理\n",
        "         postprocessed_response = response[\"choices\"][0][\"message\"][\"content\"].lower()\n",
        "         postprocess_span.update_event(name=\"end_postprocess\",output=postprocessed_response)\n",
        "\n",
        "    score = input(\"请对 AI 的回答进行评分 (1-5，5分最高): \")\n",
        "    try:\n",
        "      trace.score(name=\"response_quality\", value=int(score))\n",
        "    except:\n",
        "      print(\"评分格式错误，请输入数字 1-5\")\n",
        "\n",
        "    trace.end()\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"], api_response.json()\n",
        "\n",
        "\n",
        "question = \"请问我有什么产品，并且用小写的形式给我？\"\n",
        "answer, api_response = complex_llm_app(question)\n",
        "print(f\"用户提问: {question}\")\n",
        "print(f\"AI 回答: {answer}\")\n",
        "print(f\"API 响应：{api_response}\")\n",
        "\n",
        "# In[3]:\n",
        "# ### 4.1.7 使用 Langfuse API 分析数据\n",
        "# 获取最近 24 小时的所有 Span 信息\n",
        "now = datetime.utcnow()\n",
        "start_time = now - timedelta(days=1)\n",
        "spans = langfuse.spans(\n",
        "    start_time = start_time.isoformat()\n",
        ").data\n",
        "\n",
        "spans_df = pd.DataFrame(spans)\n",
        "print(\"\\n最近24小时的Spans信息：\")\n",
        "print(spans_df[[\"id\", \"name\", \"traceId\", \"startTime\",\"endTime\"]])\n",
        "\n",
        "# 计算每个 Span 的平均延迟\n",
        "\n",
        "def calculate_average_latency_for_spans(start_time, end_time):\n",
        "  spans = langfuse.spans(\n",
        "    start_time = start_time.isoformat(),\n",
        "      end_time = end_time.isoformat()\n",
        "  ).data\n",
        "  spans_df = pd.DataFrame(spans)\n",
        "\n",
        "  observations = langfuse.observations(\n",
        "        start_time = start_time.isoformat(),\n",
        "        end_time = end_time.isoformat()\n",
        "    ).data\n",
        "\n",
        "  observations_df = pd.DataFrame(observations)\n",
        "  latency_observations = observations_df[observations_df[\"name\"] == \"latency\"]\n",
        "\n",
        "  merged_df = pd.merge(spans_df, latency_observations, left_on = \"id\", right_on = \"spanId\", suffixes=('_span', '_observation'))\n",
        "\n",
        "  if not merged_df.empty:\n",
        "    average_latency = merged_df.groupby(\"name_span\")[\"value\"].mean().reset_index()\n",
        "    print(f\"从 {start_time} 到 {end_time} 的每个 Span 的平均延迟为:\")\n",
        "    print(average_latency)\n",
        "  else:\n",
        "    print(\"当前时间段内没有延迟数据\")\n",
        "\n",
        "\n",
        "today = datetime.utcnow()\n",
        "last_week = today - timedelta(days = 7)\n",
        "calculate_average_latency_for_spans(last_week, today)\n",
        "\n",
        "#  **下一步：**\n",
        "#  *  运行上面的代码，查看 Langfuse 仪表板中的结果。\n",
        "#  *  尝试修改代码，记录更多的信息。\n",
        "#  *  尝试使用 Langfuse API 分析数据，找出潜在的性能或质量问题。\n",
        "\n",
        "# In[4]:\n",
        "# ## 4.2  优化 LLM 应用\n",
        "#\n",
        "# **学习目标:** 能够使用 Langfuse 来指导 LLM 应用的优化过程。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   根据性能指标和评估结果，优化 LLM 模型或参数\n",
        "# *   调整 Prompt Engineering, 提升LLM输出质量\n",
        "# *   使用 Langfuse 监控优化效果，并进行迭代改进\n",
        "#\n",
        "# ### 4.2.1  基于 Langfuse 的优化流程\n",
        "#  使用 Langfuse 优化 LLM 应用的流程通常如下：\n",
        "#  1.  **监控：** 使用 Langfuse 监控你的 LLM 应用，记录 Trace, Span, Event, Observation 和 Score。\n",
        "#  2.  **分析：** 使用 Langfuse 仪表板或 API 分析数据，找出潜在的性能或质量问题。\n",
        "#  3.  **优化：** 根据分析结果，优化你的 LLM 应用，例如修改 Prompt, 修改模型参数，调整调用逻辑。\n",
        "#  4.  **测试：** 使用新的代码重新测试你的 LLM 应用，并使用 Langfuse 记录新的数据。\n",
        "#  5.  **迭代：** 分析新的数据，判断优化效果，重复优化步骤。\n",
        "#\n",
        "# ### 4.2.2  根据性能指标优化\n",
        "#\n",
        "#   * **平均延迟：** 如果平均延迟过高，则需要分析是哪个 Span 耗时最长，可能是 API 调用、模型计算、数据库查询等。\n",
        "#       *  优化：可以优化慢的 API， 使用更快的 LLM 模型，或使用数据库缓存。\n",
        "#   * **LLM 延迟：** 如果 LLM 延迟过高，则需要尝试使用更快的 LLM 模型，或减少请求的 Token 数。\n",
        "#       *  优化： 使用更快的 LLM 模型，或者使用 Prompt Engineering 减少 token 数量。\n",
        "#   * **Token 数：** 如果 Token 数过高，则会增加成本，可以考虑优化 Prompt, 减少 Token 数。\n",
        "#      *  优化： 可以使用更简短的 Prompt，或者使用更适合的 LLM 模型。\n",
        "#\n",
        "# ### 4.2.3  根据评估结果优化\n",
        "#  * **相关性：** 如果 LLM 的输出相关性过低，可以尝试调整 Prompt, 提升 LLM 的理解能力。\n",
        "#      * 优化： 可以使用更清晰的 Prompt，或者使用多轮对话来引导 LLM 输出。\n",
        "#  * **准确性：** 如果 LLM 的输出准确性过低，可以尝试使用更专业的 LLM 模型，或者优化模型训练数据。\n",
        "#      * 优化： 可以使用更专业的 LLM 模型，或增加 Few Shot 示例。\n",
        "#  *  **安全性：** 如果 LLM 的输出安全性过低，可以尝试使用安全策略，避免产生有害或不安全的内容。\n",
        "#      * 优化： 可以使用 Prompt Engineering，或使用安全策略来过滤LLM输出。\n",
        "#\n",
        "# ### 4.2.4  使用 Langfuse 监控优化效果\n",
        "#\n",
        "#  在优化之后，你需要使用 Langfuse 监控你的 LLM 应用，并判断优化效果。\n",
        "#  *   使用 Langfuse 仪表板查看优化后的性能指标和评估结果。\n",
        "#  *   使用 Langfuse API 分析优化后的数据，并与优化前的数据进行对比。\n",
        "#  *  进行多次迭代优化，直到满足你的要求。\n",
        "#\n",
        "# **下一步:**\n",
        "#  *  根据你的实际情况，使用 Langfuse 分析你的 LLM 应用，并进行优化。\n",
        "#  *  使用 Langfuse 监控优化效果，并进行迭代改进。\n",
        "\n",
        "# In[5]:\n",
        "# ## 4.3  构建可观测的 LLM 代理\n",
        "#\n",
        "# **学习目标:** 掌握如何使用 Langfuse 来构建可观测的 LLM 代理。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   使用 Langfuse 跟踪 LLM 代理的执行过程\n",
        "# *   记录代理的决策过程，便于调试和分析\n",
        "# *   使用 Langfuse 评估代理的性能和效果\n",
        "# *   监控代理的调用次数、耗时等关键指标\n",
        "#\n",
        "# ### 4.3.1  LLM 代理的定义\n",
        "#\n",
        "#   LLM 代理是指能够自主执行任务的 LLM 程序，它通常具有以下特点：\n",
        "#  *  具有一定的记忆能力。\n",
        "#  *  能够自主进行决策。\n",
        "#  *  能够调用外部工具或 API。\n",
        "#  *  能够与用户进行多轮交互。\n",
        "#\n",
        "# ### 4.3.2 使用 Langfuse 跟踪 LLM 代理\n",
        "#\n",
        "#   使用 Langfuse 跟踪 LLM 代理的执行过程，需要记录以下信息：\n",
        "#  *   代理的执行步骤 (使用 Span 来记录)。\n",
        "#  *   代理的决策过程 (使用 Event 记录)。\n",
        "#  *   代理调用的工具或 API (使用 Event 记录)。\n",
        "#  *   代理的输入和输出 (使用 Event 记录)。\n",
        "#  *   代理的执行耗时 (使用 Observation 记录)。\n",
        "#\n",
        "# ### 4.3.3 记录代理的决策过程\n",
        "#  LLM 代理的决策过程通常是比较复杂的，需要在多个步骤进行决策。\n",
        "#  可以使用 Langfuse 的 Event 来记录代理的决策过程，例如：\n",
        "#  *   代理选择使用的工具。\n",
        "#  *   代理选择执行的操作。\n",
        "#  *   代理的中间状态。\n",
        "#  通过记录这些信息，可以帮助你了解代理的决策逻辑，并进行调试和分析。\n",
        "#\n",
        "#  ### 4.3.4 使用 Langfuse 评估代理的性能\n",
        "#\n",
        "#  使用 Langfuse 评估代理的性能，需要记录以下指标：\n",
        "#  *  任务完成率： 代理成功完成任务的比例。\n",
        "#  *  任务完成时间： 代理完成任务的时间。\n",
        "#  *  资源消耗：  代理的 CPU, 内存等资源消耗。\n",
        "#  *  成本：  代理的调用成本。\n",
        "#\n",
        "# ### 4.3.5 LLM 代理示例\n",
        "#  假设我们有一个 LLM 代理，它的任务是使用 OpenAI 和一个API获取相关信息，并输出最终结果。\n",
        "\n",
        "# In[6]:\n",
        "async def ask_openai_async(question):\n",
        "    response = await openai.ChatCompletion.acreate(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "    return response, time.time()\n",
        "\n",
        "\n",
        "async def call_api_async(url):\n",
        "    start_time = time.time()\n",
        "    response = requests.get(url)\n",
        "    end_time = time.time()\n",
        "    return response, end_time - start_time\n",
        "\n",
        "async def complex_agent(question):\n",
        "    trace = langfuse.trace(name=\"LLM 代理\")\n",
        "\n",
        "    async with trace.span(name=\"agent_decision\") as decision_span:\n",
        "        decision_span.update_event(name=\"user_question\", input=question)\n",
        "        decision_span.update_event(name=\"agent_decide_step\", output=\"需要调用 openai 获取相关信息\")\n",
        "        openai_response, openai_time = await ask_openai_async(question)\n",
        "        await decision_span.update_event_async(name=\"openai_response\", output=openai_response[\"choices\"][0][\"message\"][\"content\"])\n",
        "        decision_span.log_observation(name=\"openai_latency\", value=openai_time - time.time() )\n",
        "        await decision_span.update_event_async(name=\"agent_decide_step\", output=\"需要调用 API 获取产品信息\")\n",
        "\n",
        "    async with trace.span(name=\"api_call\") as api_span:\n",
        "        api_span.update_event(name=\"api_request\", input=\"https://dummyjson.com/products/1\")\n",
        "        api_response, api_latency = await call_api_async(\"https://dummyjson.com/products/1\")\n",
        "        await api_span.update_event_async(name=\"api_response\", output=api_response.json())\n",
        "        api_span.log_observation(name=\"latency\", value = api_latency)\n",
        "\n",
        "\n",
        "    await trace.end_async()\n",
        "    await langfuse.flush_async()\n",
        "\n",
        "    return openai_response[\"choices\"][0][\"message\"][\"content\"], api_response.json()\n",
        "\n",
        "async def main():\n",
        "  question = \"请问我有什么产品？\"\n",
        "  openai_answer, api_response = await complex_agent(question)\n",
        "  print(f\"用户提问: {question}\")\n",
        "  print(f\"AI 回答: {openai_answer}\")\n",
        "  print(f\"API 响应：{api_response}\")\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "# In[7]:\n",
        "# ### 4.3.6 查看结果\n",
        "#  运行上面的代码，并在 Langfuse 仪表板中查看结果。 你可以看到 LLM 代理的执行过程、决策过程、调用的工具、输入输出、性能指标等等。\n",
        "#\n",
        "#   **下一步：**\n",
        "#  *  运行上面的代码，查看 Langfuse 仪表板中的结果。\n",
        "#  *  尝试修改代码，记录更多的代理信息。\n",
        "\n",
        "# In[8]:\n",
        "# ## 4.4  最佳实践和进阶技巧\n",
        "#\n",
        "# **学习目标:** 掌握 Langfuse 的最佳实践，提高使用效率和效果。\n",
        "#\n",
        "# **重点内容:**\n",
        "# *   如何清晰命名 Span 和 Event\n",
        "# *   如何合理使用 metadata\n",
        "# *   如何有效利用 Langfuse 仪表板\n",
        "# *   如何与其他工具集成 Langfuse\n",
        "# *   如何解决使用 Langfuse 过程中的常见问题\n",
        "#\n",
        "# ### 4.4.1 如何清晰命名 Span 和 Event\n",
        "#\n",
        "#  清晰命名 Span 和 Event 是非常重要的，它可以让你更方便地理解和分析数据。\n",
        "#  *   Span 的命名： 应该能够清晰地表达 Span 的功能，例如 \"openai_call\", \"api_call\", \"preprocess_data\" 等。\n",
        "#  *   Event 的命名： 应该能够清晰地表达 Event 的类型，例如 \"user_question\", \"llm_response\", \"api_request\" 等。\n",
        "#  *   避免使用模糊或不清晰的命名，例如 \"step1\", \"step2\" 等。\n",
        "#\n",
        "# ### 4.4.2 如何合理使用 Metadata\n",
        "#\n",
        "#   Metadata 可以记录额外的属性信息，例如用户 ID, 模型版本，请求参数，响应头等。\n",
        "#  *   合理使用 Metadata 可以帮助你更全面地了解数据，并进行更深入的分析。\n",
        "#  *   避免在 Metadata 中记录过多的敏感信息，例如用户密码、个人隐私等。\n",
        "#  *   Metadata 的数据类型应该尽量保持一致。\n",
        "#\n",
        "# ### 4.4.3 如何有效利用 Langfuse 仪表板\n",
        "#\n",
        "#  Langfuse 仪表板提供了可视化的界面，可以方便地查看和分析数据。\n",
        "#  *  使用仪表板中的过滤器，可以根据不同的条件过滤数据。\n",
        "#  * 使用仪表板中的图表，可以直观地了解数据的分布和趋势。\n",
        "#  * 使用仪表板中的搜索功能，可以快速找到你想要的数据。\n",
        "#  *  自定义仪表板，根据自己的需求配置展示的数据。\n",
        "#\n",
        "# ### 4.4.4 如何与其他工具集成 Langfuse\n",
        "#\n",
        "#  Langfuse 可以与许多其他的工具集成，例如：\n",
        "#  *   向量数据库 (例如记录 LLM 调用中使用的向量数据)\n",
        "#  *   评估框架 (例如 `Ragas`, `LLMEval`)\n",
        "#  *   日志系统 (例如将 Langfuse 的数据同步到你的日志系统中)\n",
        "#  *   监控系统 (例如将 Langfuse 的数据同步到你的监控系统中)\n",
        "#\n",
        "#  通过与其他工具集成，可以提高你的开发效率，并使你的 LLM 应用更加强大。\n",
        "#\n",
        "# ### 4.4.5 如何解决使用 Langfuse 过程中的常见问题\n",
        "#\n",
        "#  *  仔细查看 Langfuse 的官方文档，解决大部分问题。\n",
        "#  *  检查你的 API Key 和 Host URL 是否配置正确。\n",
        "#  *  查看你的网络连接是否正常。\n",
        "#  *  尝试使用 `debug` 模式，查看详细的日志输出。\n",
        "#  *  如果问题仍然无法解决，请向 Langfuse 社区寻求帮助。\n",
        "\n",
        "# In[8]:\n",
        "# ### 4.4.6 小结\n",
        "#\n",
        "# 你已经成功完成了模块四的学习，你掌握了 Langfuse 的实战技巧，包括：\n",
        "#  *   监控复杂的 LLM 应用\n",
        "#  *   优化 LLM 应用\n",
        "#  *   构建可观测的 LLM 代理\n",
        "#  *   Langfuse 最佳实践\n",
        "#\n",
        "# 你已经具备了使用 Langfuse 开发和优化 LLM 应用的能力。\n",
        "#\n",
        "#  **下一步：**\n",
        "#  *  巩固本模块所学的知识。\n",
        "#  *  在你的实际项目中使用 Langfuse，并不断改进和优化。\n",
        "#  *  参与 Langfuse 社区，贡献你的代码和经验。"
      ],
      "metadata": {
        "id": "kKyWjNmW7BAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFEae642Ps5s/2VNJ/+L+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw9B1aQ65HeY"
      },
      "source": [
        "### **一个简化版本**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe1b9kQp5EnT"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as Data\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 文本预处理\n",
        "sentences = [\"jack like dog\", \"jack like cat\", \"jack like animal\",\n",
        "  \"dog cat animal\", \"banana apple cat dog like\", \"dog fish milk like\",\n",
        "  \"dog cat animal like\", \"jack like apple\", \"apple like\", \"jack like banana\",\n",
        "  \"apple banana jack movie book music like\", \"cat dog hate\", \"cat dog like\"]\n",
        "\n",
        "word_sequence = \" \".join(sentences).split() # ['jack', 'like', 'dog', 'jack', 'like', 'cat', 'animal',...]\n",
        "vocab = list(set(word_sequence)) # build words vocabulary\n",
        "word2idx = {w: i for i, w in enumerate(vocab)} # {'jack':0, 'like':1,...}\n",
        "\n",
        "# Word2Vec Parameters\n",
        "batch_size = 8\n",
        "embedding_size = 2  # 2 dim vector represent one word\n",
        "C = 2 # window size\n",
        "voc_size = len(vocab)\n",
        "\n",
        "# 1.\n",
        "skip_grams = []\n",
        "for idx in range(C, len(word_sequence) - C):\n",
        "  center = word2idx[word_sequence[idx]] # center word\n",
        "  context_idx = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1)) # context word idx\n",
        "  context = [word2idx[word_sequence[i]] for i in context_idx]\n",
        "  for w in context:\n",
        "    skip_grams.append([center, w])\n",
        "\n",
        "# 2.\n",
        "'''\n",
        "由于 Word2Vec 的输入是 one-hot 表示，所以我们先构建一个对角全 1 的矩阵，\n",
        "利用 np.eye(rows) 方法，其中的参数 rows 表示全 1 矩阵的行数，\n",
        "对于这个问题来说，语料库中总共有多少个单词，就有多少行。\n",
        "然后根据 skip_grams 每行第一列的值，取出相应全 1 矩阵的行。\n",
        "将这些取出的行，append 到一个 list 中去，最终的这个 list 就是所有的样本 X。\n",
        "标签不需要 one-hot 表示，只需要类别值，所以只用把 skip_grams 中每行的第二列取出来存起来即可\n",
        "'''\n",
        "\n",
        "def make_data(skip_grams):\n",
        "  input_data = []\n",
        "  output_data = []\n",
        "  for i in range(len(skip_grams)):\n",
        "    input_data.append(np.eye(voc_size)[skip_grams[i][0]])\n",
        "    output_data.append(skip_grams[i][1])\n",
        "  return input_data, output_data\n",
        "\n",
        "# 3.\n",
        "input_data, output_data = make_data(skip_grams)\n",
        "input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)\n",
        "dataset = Data.TensorDataset(input_data, output_data)\n",
        "loader = Data.DataLoader(dataset, batch_size, True)\n",
        "\n",
        "# Model\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Word2Vec, self).__init__()\n",
        "\n",
        "    # W and V is not Traspose relationship\n",
        "    self.W = nn.Parameter(torch.randn(voc_size, embedding_size).type(dtype))\n",
        "    self.V = nn.Parameter(torch.randn(embedding_size, voc_size).type(dtype))\n",
        "\n",
        "  def forward(self, X):\n",
        "    # X : [batch_size, voc_size] one-hot\n",
        "    # torch.mm only for 2 dim matrix, but torch.matmul can use to any dim\n",
        "    hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
        "    output_layer = torch.matmul(hidden_layer, self.V) # output_layer : [batch_size, voc_size]\n",
        "    return output_layer\n",
        "\n",
        "model = Word2Vec().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training\n",
        "for epoch in range(2000):\n",
        "  for i, (batch_x, batch_y) in enumerate(loader):\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "    pred = model(batch_x)\n",
        "    loss = criterion(pred, batch_y)\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "      print(epoch + 1, i, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "for i, label in enumerate(vocab):\n",
        "  W, WT = model.parameters()\n",
        "  x,y = float(W[i][0]), float(W[i][1])\n",
        "  plt.scatter(x, y)\n",
        "  plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEdGLsXFDjhw"
      },
      "source": [
        "### **完整版**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OjXV8cRDmBV"
      },
      "source": [
        "# https://wmathor.com/index.php/archives/1435/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as tud\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import scipy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "C = 3 # context window\n",
        "K = 15 # number of negative samples\n",
        "epochs = 2\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "# 词向量的维度大概怎么选择：EMBEDDING_SIZE > 8 * log(MAX_VOCAB_SIZE)\n",
        "EMBEDDING_SIZE = 100\n",
        "batch_size = 32\n",
        "lr = 0.2\n",
        "\n",
        "with open('text8.train.txt') as f:\n",
        "    text = f.read() # 得到文本内容\n",
        "\n",
        "text = text.lower().split() #　分割成单词列表\n",
        "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 得到单词字典表，key是单词，value是次数\n",
        "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # 把不常用的单词都编码为\"<UNK>\"\n",
        "word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
        "idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
        "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
        "word_freqs = word_counts / np.sum(word_counts)\n",
        "word_freqs = word_freqs ** (3./4.)\n",
        "\n",
        "class WordEmbeddingDataset(tud.Dataset):\n",
        "    def __init__(self, text, word2idx, word_freqs):\n",
        "        ''' text: a list of words, all text from the training dataset\n",
        "            word2idx: the dictionary from word to index\n",
        "            word_freqs: the frequency of each word\n",
        "        '''\n",
        "        super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\n",
        "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text] # 把单词数字化表示。如果不在词典中，也表示为unk\n",
        "        self.text_encoded = torch.LongTensor(self.text_encoded) # nn.Embedding需要传入LongTensor类型\n",
        "        self.word2idx = word2idx\n",
        "        self.word_freqs = torch.Tensor(word_freqs)\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        ''' 这个function返回以下数据用于训练\n",
        "            - 中心词\n",
        "            - 这个单词附近的positive word\n",
        "            - 随机采样的K个单词作为negative word\n",
        "        '''\n",
        "        center_words = self.text_encoded[idx] # 取得中心词\n",
        "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1)) # 先取得中心左右各C个词的索引\n",
        "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 为了避免索引越界，所以进行取余处理\n",
        "        pos_words = self.text_encoded[pos_indices] # tensor(list)\n",
        "        \n",
        "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
        "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\n",
        "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
        "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
        "        \n",
        "        # while 循环是为了保证 neg_words中不能包含背景词\n",
        "        while len(set(pos_indices) & set(neg_words)) > 0:\n",
        "            neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
        "\n",
        "        return center_words, pos_words, neg_words\n",
        "\n",
        "dataset = WordEmbeddingDataset(text, word2idx, word_freqs)\n",
        "dataloader = tud.DataLoader(dataset, batch_size, shuffle=True)\n",
        "\n",
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(EmbeddingModel, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        \n",
        "    def forward(self, input_labels, pos_labels, neg_labels):\n",
        "        ''' input_labels: center words, [batch_size]\n",
        "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
        "            neg_labels：negative words, [batch_size, (window_size * 2 * K)]\n",
        "            \n",
        "            return: loss, [batch_size]\n",
        "        '''\n",
        "        input_embedding = self.in_embed(input_labels) # [batch_size, embed_size]\n",
        "        pos_embedding = self.out_embed(pos_labels)# [batch_size, (window * 2), embed_size]\n",
        "        neg_embedding = self.out_embed(neg_labels) # [batch_size, (window * 2 * K), embed_size]\n",
        "        \n",
        "        input_embedding = input_embedding.unsqueeze(2) # [batch_size, embed_size, 1]\n",
        "        \n",
        "        pos_dot = torch.bmm(pos_embedding, input_embedding) # [batch_size, (window * 2), 1]\n",
        "        pos_dot = pos_dot.squeeze(2) # [batch_size, (window * 2)]\n",
        "        \n",
        "        neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]\n",
        "        neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\n",
        "        \n",
        "        log_pos = F.logsigmoid(pos_dot).sum(1) # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
        "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
        "        \n",
        "        loss = log_pos + log_neg\n",
        "        \n",
        "        return -loss\n",
        "    \n",
        "    def input_embedding(self):\n",
        "        return self.in_embed.weight.detach().numpy()\n",
        "\n",
        "model = EmbeddingModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for e in range(1):\n",
        "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
        "        input_labels = input_labels.long()\n",
        "        pos_labels = pos_labels.long()\n",
        "        neg_labels = neg_labels.long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('epoch', e, 'iteration', i, loss.item())\n",
        "            torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))\n",
        "\n",
        "embedding_weights = model.input_embedding()\n",
        "\n",
        "\n",
        "def find_nearest(word):\n",
        "    index = word2idx[word]\n",
        "    embedding = embedding_weights[index]\n",
        "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "    return [idx2word[i] for i in cos_dis.argsort()[:10]]\n",
        "\n",
        "for word in [\"two\", \"america\", \"computer\"]:\n",
        "    print(word, find_nearest(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WokgXR-oDmki"
      },
      "source": [
        "### **使用预训练词向量**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "dD2hrb-zDrXV",
        "outputId": "0c1c4921-152e-42e8-fa9f-6680d6328e69"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "embeds = nn.Embedding(2, 5)\n",
        "embeds.weight\n",
        "\n",
        "# 使用预训练好的词向量，则采用\n",
        "pretrained_weight = np.array(pretrained_weight)\n",
        "embeds.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
        "embeds.weight\n",
        "\n",
        "# 查看某个词的词向量，需要传入这个词在词典中的 index，并且这个 index 得是 LongTensor 型的\n",
        "embeds = nn.Embedding(100, 10)\n",
        "embeds(torch.LongTensor([50]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-40b5b54ce161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 使用预训练好的词向量，则采用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpretrained_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pretrained_weight' is not defined"
          ]
        }
      ]
    }
  ]
}
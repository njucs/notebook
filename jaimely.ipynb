{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jaimely.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/notebook/blob/master/jaimely.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_HEsreWqr_KM",
        "colab_type": "code",
        "outputId": "1736b6ed-e092-42e3-a4f7-aea22a5dd9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "cell_type": "code",
      "source": [
        "'''Trains a simple convnet on the MNIST dataset.\n",
        "Gets to 99.25% test accuracy after 12 epochs\n",
        "(there is still a lot of margin for parameter tuning).\n",
        "16 seconds per epoch on a GRID K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.2597 - acc: 0.9204 - val_loss: 0.0576 - val_acc: 0.9806\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0863 - acc: 0.9745 - val_loss: 0.0399 - val_acc: 0.9868\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0660 - acc: 0.9813 - val_loss: 0.0372 - val_acc: 0.9875\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0536 - acc: 0.9843 - val_loss: 0.0301 - val_acc: 0.9898\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0477 - acc: 0.9859 - val_loss: 0.0297 - val_acc: 0.9901\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0404 - acc: 0.9883 - val_loss: 0.0339 - val_acc: 0.9885\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0373 - acc: 0.9886 - val_loss: 0.0309 - val_acc: 0.9905\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0335 - acc: 0.9895 - val_loss: 0.0291 - val_acc: 0.9907\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0303 - acc: 0.9906 - val_loss: 0.0288 - val_acc: 0.9908\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0287 - acc: 0.9913 - val_loss: 0.0326 - val_acc: 0.9899\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0267 - acc: 0.9919 - val_loss: 0.0283 - val_acc: 0.9909\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0252 - acc: 0.9915 - val_loss: 0.0281 - val_acc: 0.9909\n",
            "Test loss: 0.028115917088295826\n",
            "Test accuracy: 0.9909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4EHCL5t-DyZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4a0592a7-08e7-4fe6-ae99-b4bba7d8191b"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R17Vi_ewD_Ty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4aab1301-cda0-4d35-be11-2b0b984b5f76"
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/\"\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   中心词驱动的汉语统计句法分析模型的改进\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v23Q8fPQER3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5ff3b08-2d20-4052-bfdd-edab19f421da"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "O13zaRi8ETQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "f91bb985-785e-40eb-9a56-0395a0ccec55"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 18404796507869937822, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 2267349821794222475\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 6013125150936320073\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11276946637\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 10389984540060627047\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "e_UZiDeJGO6F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 如何使用TensorBoard"
      ]
    },
    {
      "metadata": {
        "id": "1INZEIxhGF8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d2aa51c2-74b0-4195-bf8b-1e34a32750aa"
      },
      "cell_type": "code",
      "source": [
        "# 你可以更改目录名\n",
        "LOG_DIR = 'tb_logs'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-28 06:59:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.173.32.212, 34.232.181.106, 52.22.236.254, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.173.32.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  14.7MB/s    in 1.0s    \n",
            "\n",
            "2019-03-28 06:59:32 (14.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://6084be3b.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DBd8OqRJGGJs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4d76bcb8-e22c-4e7d-9724-39610ea61396"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# 输入图像维度\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# 将类别向量转换成二分类矩阵\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "tbCallBack = TensorBoard(log_dir=LOG_DIR, \n",
        "                         histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=batch_size,\n",
        "                         write_images=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[tbCallBack])\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.2639 - acc: 0.9192 - val_loss: 0.0581 - val_acc: 0.9814\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0877 - acc: 0.9734 - val_loss: 0.0389 - val_acc: 0.9867\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0657 - acc: 0.9801 - val_loss: 0.0341 - val_acc: 0.9884\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0536 - acc: 0.9845 - val_loss: 0.0320 - val_acc: 0.9882\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0467 - acc: 0.9858 - val_loss: 0.0327 - val_acc: 0.9890\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0411 - acc: 0.9874 - val_loss: 0.0289 - val_acc: 0.9899\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0370 - acc: 0.9887 - val_loss: 0.0266 - val_acc: 0.9912\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0342 - acc: 0.9899 - val_loss: 0.0289 - val_acc: 0.9903\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0319 - acc: 0.9902 - val_loss: 0.0272 - val_acc: 0.9907\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0291 - acc: 0.9908 - val_loss: 0.0261 - val_acc: 0.9911\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0283 - acc: 0.9913 - val_loss: 0.0279 - val_acc: 0.9913\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0264 - acc: 0.9922 - val_loss: 0.0254 - val_acc: 0.9921\n",
            "Test loss: 0.025357878165416514\n",
            "Test accuracy: 0.9921\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}